{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arplab/project/paradeigma/multi_modal/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu117\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, Data2VecAudioModel\n",
    "from datasets import load_dataset, Dataset, Audio, Features\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Sess01_script01_User002M_001\n",
       "1      Sess01_script01_User002M_002\n",
       "2      Sess01_script01_User002M_003\n",
       "3      Sess01_script01_User002M_004\n",
       "4      Sess01_script01_User001F_001\n",
       "                   ...             \n",
       "306    Sess01_script06_User001F_016\n",
       "307    Sess01_script06_User002M_041\n",
       "308    Sess01_script06_User002M_042\n",
       "309    Sess01_script06_User002M_043\n",
       "310    Sess01_script06_User001F_017\n",
       "Name:  .1, Length: 311, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation_list = sorted(os.listdir('/home/arplab/project/paradeigma/multi_modal/org_KEMDy20/annotation'))\n",
    "file_name = annotation_list[0]\n",
    "df_annotation = pd.read_csv('org_KEMDy20/annotation/' + file_name, skiprows=1)\n",
    "file_lst = df_annotation[' .1']\n",
    "file_lst"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./org_KEMDy20/Session01/Sess01_script01_User002M_001.wav',\n",
       " './org_KEMDy20/Session01/Sess01_script01_User002M_002.wav',\n",
       " './org_KEMDy20/Session01/Sess01_script01_User002M_003.wav']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav_file_lst = []\n",
    "for i in file_lst:\n",
    "    wav_file_lst.append('./org_KEMDy20/Session01/' + i + '.wav')\n",
    "    \n",
    "wav_file_lst[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': './org_KEMDy20/Session01/Sess01_script01_User002M_001.wav',\n",
       " 'array': array([ 0.0526123 ,  0.05267334,  0.05197144, ..., -0.00387573,\n",
       "        -0.00244141, -0.00137329], dtype=float32),\n",
       " 'sampling_rate': 16000}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "dataset = Dataset.from_dict({\"wav\": wav_file_lst}).cast_column('wav', Audio())\n",
    "dataset[0]['wav']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/data2vec-audio-base-960h were not used when initializing Data2VecAudioModel: ['lm_head.bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing Data2VecAudioModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Data2VecAudioModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "check_point_wav_model = 'facebook/data2vec-audio-base-960h'\n",
    "processor = AutoProcessor.from_pretrained(check_point_wav_model)\n",
    "model_wav = Data2VecAudioModel.from_pretrained(check_point_wav_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_rate = dataset['wav'][0]['sampling_rate']\n",
    "sampling_rate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### audio embedding method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wav_embedding_func(wav_file_lst, processor, model):\n",
    "    \n",
    "    padding = 'max_length'\n",
    "    max_length = 16000\n",
    "    truncation = True\n",
    "    \n",
    "    dataset = Dataset.from_dict({'wav': wav_file_lst}).cast_column('wav', Audio())\n",
    "    wav_embeddings = []\n",
    "    for i in range(len(dataset)):\n",
    "        inputs = processor(\n",
    "            dataset[i]['wav']['array'], \n",
    "            sampling_rate = sampling_rate, return_tensors = 'pt', return_attention_mask = True,\n",
    "            padding = padding, max_length = max_length, truncation = truncation)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model_wav(**inputs)\n",
    "            \n",
    "        wav_embeddings.append(output['last_hidden_state'])\n",
    "    return wav_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_embedding_func(wav_file_lst, processor, model_wav)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer, BartModel\n",
    "from transformers import AutoTokenizer, Data2VecTextModel\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./org_KEMDy20/Session01/Sess01_script01_User002M_001.txt',\n",
       " './org_KEMDy20/Session01/Sess01_script01_User002M_002.txt',\n",
       " './org_KEMDy20/Session01/Sess01_script01_User002M_003.txt']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_file_lst = []\n",
    "for i in file_lst:\n",
    "    txt_file_lst.append('./org_KEMDy20/Session01/' + i + '.txt')\n",
    "txt_file_lst[:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 왜 다른 모델을 사용하고 있는건가요? \n",
    "의도 설명 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/data2vec-text-base were not used when initializing Data2VecTextModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing Data2VecTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Data2VecTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Data2VecTextModel were not initialized from the model checkpoint at facebook/data2vec-text-base and are newly initialized: ['data2vec_text.pooler.dense.bias', 'data2vec_text.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at facebook/data2vec-text-base were not used when initializing Data2VecTextModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing Data2VecTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Data2VecTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Data2VecTextModel were not initialized from the model checkpoint at facebook/data2vec-text-base and are newly initialized: ['data2vec_text.pooler.dense.bias', 'data2vec_text.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50265, 768, padding_idx=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = \"facebook/data2vec-text-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model_txt = Data2VecTextModel.from_pretrained(checkpoint)\n",
    "model_txt_cuda = Data2VecTextModel.from_pretrained(checkpoint).cuda()\n",
    "\n",
    "model_txt.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_embeddings_func(txt_file_lst, tokenizer, model):\n",
    "    sentences = []\n",
    "    padding = 'max_length'\n",
    "    max_length = 80\n",
    "    truncation = True\n",
    "    \n",
    "    for sentence in txt_file_lst:\n",
    "        f = open(sentence,'r')\n",
    "        line = f.readline()\n",
    "        line = re.sub('\\n', '', line)\n",
    "        line = re.sub('  ', ' ', line)\n",
    "        line = line.rstrip().lstrip()\n",
    "        sentences.append(line)\n",
    "        f.close()\n",
    "        \n",
    "    inputs = tokenizer(\n",
    "        sentences, padding = padding, max_length = max_length, truncation = truncation, return_tensors='pt'\n",
    "        )\n",
    "    txt_embeddings, _ = model(**inputs, return_dict=False)\n",
    "    return txt_embeddings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1883, -0.0291, -0.0117,  ..., -0.0931, -0.0109,  0.1308],\n",
       "         [-0.0740, -0.0787, -0.0656,  ..., -0.0323,  0.0825,  0.0826],\n",
       "         [-0.0908,  0.1073, -0.1467,  ..., -0.0191, -0.0405,  0.0479],\n",
       "         ...,\n",
       "         [-0.1111,  0.1112, -0.1535,  ..., -0.0108, -0.0418,  0.0736],\n",
       "         [-0.0947,  0.0485, -0.1209,  ...,  0.0095,  0.0013,  0.1210],\n",
       "         [ 0.2380, -0.0621,  0.0350,  ...,  0.0772, -0.0580,  0.0579]],\n",
       "\n",
       "        [[ 0.1796, -0.0431, -0.0118,  ..., -0.0794, -0.0236,  0.1116],\n",
       "         [-0.0791, -0.0831, -0.0669,  ..., -0.0359,  0.0919,  0.0774],\n",
       "         [-0.1199,  0.0889, -0.1398,  ..., -0.0478, -0.0554,  0.0616],\n",
       "         ...,\n",
       "         [-0.0865,  0.0340, -0.1327,  ..., -0.0668, -0.0287,  0.1769],\n",
       "         [ 0.0054,  0.1212, -0.0730,  ...,  0.0018,  0.0233,  0.1023],\n",
       "         [ 0.2306, -0.0696,  0.0456,  ...,  0.0123, -0.0796,  0.0533]],\n",
       "\n",
       "        [[ 0.1986, -0.0062,  0.0027,  ..., -0.1102, -0.0632,  0.1459],\n",
       "         [-0.1368, -0.0599, -0.1114,  ..., -0.0047,  0.1534, -0.0438],\n",
       "         [-0.1007,  0.0865, -0.0711,  ..., -0.0440, -0.0332,  0.0430],\n",
       "         ...,\n",
       "         [-0.1175, -0.0353, -0.0812,  ...,  0.0270,  0.0794,  0.0586],\n",
       "         [-0.0331,  0.0339, -0.1358,  ..., -0.0711, -0.0504,  0.1099],\n",
       "         [ 0.2426, -0.0632,  0.0424,  ...,  0.0901, -0.0711,  0.0406]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.1902, -0.0698,  0.0056,  ..., -0.0377, -0.0791,  0.0792],\n",
       "         [-0.0907, -0.1122, -0.0899,  ..., -0.0243,  0.0981,  0.0872],\n",
       "         [-0.0954, -0.1323, -0.1068,  ..., -0.0269,  0.1024,  0.0622],\n",
       "         ...,\n",
       "         [-0.1057, -0.0198, -0.0957,  ..., -0.0585,  0.0647,  0.0149],\n",
       "         [-0.1057, -0.0198, -0.0957,  ..., -0.0585,  0.0647,  0.0149],\n",
       "         [-0.1057, -0.0198, -0.0957,  ..., -0.0585,  0.0647,  0.0149]],\n",
       "\n",
       "        [[ 0.1983, -0.0762,  0.0090,  ..., -0.0575, -0.1033,  0.1217],\n",
       "         [-0.0845, -0.1327, -0.0956,  ..., -0.0186,  0.0898,  0.0780],\n",
       "         [-0.0881, -0.1403, -0.0994,  ..., -0.0206,  0.0964,  0.0686],\n",
       "         ...,\n",
       "         [-0.0749, -0.0242, -0.0803,  ..., -0.1001,  0.0918, -0.0233],\n",
       "         [-0.0749, -0.0242, -0.0803,  ..., -0.1001,  0.0918, -0.0233],\n",
       "         [-0.0749, -0.0242, -0.0803,  ..., -0.1001,  0.0918, -0.0233]],\n",
       "\n",
       "        [[ 0.1875, -0.0401,  0.0123,  ..., -0.0961, -0.0166,  0.1334],\n",
       "         [-0.0794, -0.0802, -0.0765,  ..., -0.0354,  0.0934,  0.0799],\n",
       "         [-0.1360,  0.1000, -0.1619,  ..., -0.0434, -0.0778,  0.0470],\n",
       "         ...,\n",
       "         [-0.0495,  0.0254, -0.1813,  ..., -0.0578,  0.0109,  0.0583],\n",
       "         [-0.0869,  0.0632, -0.1337,  ...,  0.0244, -0.0392,  0.0939],\n",
       "         [ 0.2412, -0.0578,  0.0452,  ...,  0.0650, -0.0619,  0.0572]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embeddings_func(txt_file_lst, tokenizer, model_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_dataset(\n",
    "#     file_name_lst, \n",
    "#     text_embeddings, wav_embeddings, \n",
    "#     label_emotion, label_arousal, label_valence, \n",
    "#     session_name = 'Session01'\n",
    "#     ):\n",
    "    \n",
    "#     dataset = {'file_names': file_name_lst, \n",
    "#          'text_embeddings': text_embeddings, \n",
    "#          'wav_embeddings':wav_embeddings,\n",
    "#          'Emotion':label_emotion,\n",
    "#          'Arousal':label_arousal,\n",
    "#          'Valence':label_valence}\n",
    "    \n",
    "def build_dataset(\n",
    "    file_name_lst, \n",
    "    text_embeddings, wav_embeddings,\n",
    "    eda, temp ,\n",
    "    label_emotion, label_arousal, label_valence, \n",
    "    session_name = 'Session01'\n",
    "    ):\n",
    "    \n",
    "    dataset = {'file_names': file_name_lst, \n",
    "         'text_embeddings': text_embeddings, \n",
    "         'wav_embeddings':wav_embeddings,\n",
    "         'EDA':eda,\n",
    "         'Temp':temp,\n",
    "         'Emotion':label_emotion,\n",
    "         'Arousal':label_arousal,\n",
    "         'Valence':label_valence\n",
    "         }\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Session36', 'Session37', 'Session38', 'Session39', 'Session40'],\n",
       " ['Sess36_eval.csv',\n",
       "  'Sess37_eval.csv',\n",
       "  'Sess38_eval.csv',\n",
       "  'Sess39_eval.csv',\n",
       "  'Sess40_eval.csv'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session_num_lst = []\n",
    "annot_num_lst = []\n",
    "num_sessions = 40\n",
    "for i in range(num_sessions):\n",
    "    if i <= 8:\n",
    "        session_num_lst.append('Session0' + str(i+1))\n",
    "        annot_num_lst.append('Sess0' + str(i+1) + '_eval.csv')\n",
    "        \n",
    "    else:\n",
    "        session_num_lst.append('Session' + str(i+1))\n",
    "        annot_num_lst.append('Sess' + str(i+1) + '_eval.csv')\n",
    "        \n",
    "session_num_lst[-5:], annot_num_lst[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./model/data/paradeigma_ts_data_preprocessing.pkl', 'rb') as f:\n",
    "    target_data_frames = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>segment_id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>valence</th>\n",
       "      <th>arousal</th>\n",
       "      <th>eda</th>\n",
       "      <th>temp</th>\n",
       "      <th>ibi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sess01_script01_User002M_001</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>[-0.06791500000000017, -0.110201, -0.025627999...</td>\n",
       "      <td>[34.81, 34.81, 34.81, 34.79, 34.79, 34.79, 34....</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sess01_script01_User002M_002</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3.1</td>\n",
       "      <td>2.9</td>\n",
       "      <td>[0.006407000000000274, -0.021784000000000248, ...</td>\n",
       "      <td>[34.79, 34.79, 34.79, 34.77, 34.77, 34.77, 34....</td>\n",
       "      <td>[0.65625, 0.609375, 0.609375]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sess01_script01_User002M_003</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.07944700000000005, 0.05125599999999997, 0.0...</td>\n",
       "      <td>[34.75, 34.75, 34.75, 34.75, 34.79, 34.79, 34....</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sess01_script01_User002M_004</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>[0.0038439999999999586, -0.03331699999999982, ...</td>\n",
       "      <td>[34.77, 34.77, 34.77, 34.77, 34.77, 34.77, 34....</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sess01_script01_User001F_001</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3.8</td>\n",
       "      <td>2.8</td>\n",
       "      <td>[0.1241910000000006, 0.09474499999999964, 0.03...</td>\n",
       "      <td>[34.55, 34.55, 34.55, 34.55, 34.55, 34.55, 34....</td>\n",
       "      <td>[0.640625, 0.640625, 0.578125]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>Sess01_script06_User001F_016</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>[0.006400999999999435, -0.010241999999999862, ...</td>\n",
       "      <td>[34.43, 34.43, 34.43, 34.43, 34.43, 34.41, 34....</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>Sess01_script06_User002M_041</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.1</td>\n",
       "      <td>[0.0038450000000000983, -0.007689000000000057,...</td>\n",
       "      <td>[36.05, 36.05, 36.05, 36.05, 36.03, 36.03, 36....</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>Sess01_script06_User002M_042</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3</td>\n",
       "      <td>2.9</td>\n",
       "      <td>[0.06279299999999965, 0.019222000000000072, -0...</td>\n",
       "      <td>[36.07, 36.07, 36.07, 36.07, 36.05, 36.05, 36....</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>Sess01_script06_User002M_043</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>[-0.010251999999999928, -0.0038450000000000983...</td>\n",
       "      <td>[36.07, 36.05, 36.05, 36.05, 36.05]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>Sess01_script06_User001F_017</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>[-0.01408300000000029, -0.025605999999999796, ...</td>\n",
       "      <td>[34.41, 34.43, 34.43, 34.43, 34.43, 34.45, 34....</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>311 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       segment_id  emotion valence arousal  \\\n",
       "0    Sess01_script01_User002M_001  neutral     3.4     2.9   \n",
       "1    Sess01_script01_User002M_002  neutral     3.1     2.9   \n",
       "2    Sess01_script01_User002M_003  neutral     3.1       3   \n",
       "3    Sess01_script01_User002M_004  neutral     3.7     3.1   \n",
       "4    Sess01_script01_User001F_001  neutral     3.8     2.8   \n",
       "..                            ...      ...     ...     ...   \n",
       "306  Sess01_script06_User001F_016  neutral     3.3     3.5   \n",
       "307  Sess01_script06_User002M_041  neutral     3.3     3.1   \n",
       "308  Sess01_script06_User002M_042  neutral       3     2.9   \n",
       "309  Sess01_script06_User002M_043  neutral       3     2.7   \n",
       "310  Sess01_script06_User001F_017  neutral       3     3.4   \n",
       "\n",
       "                                                   eda  \\\n",
       "0    [-0.06791500000000017, -0.110201, -0.025627999...   \n",
       "1    [0.006407000000000274, -0.021784000000000248, ...   \n",
       "2    [0.07944700000000005, 0.05125599999999997, 0.0...   \n",
       "3    [0.0038439999999999586, -0.03331699999999982, ...   \n",
       "4    [0.1241910000000006, 0.09474499999999964, 0.03...   \n",
       "..                                                 ...   \n",
       "306  [0.006400999999999435, -0.010241999999999862, ...   \n",
       "307  [0.0038450000000000983, -0.007689000000000057,...   \n",
       "308  [0.06279299999999965, 0.019222000000000072, -0...   \n",
       "309  [-0.010251999999999928, -0.0038450000000000983...   \n",
       "310  [-0.01408300000000029, -0.025605999999999796, ...   \n",
       "\n",
       "                                                  temp  \\\n",
       "0    [34.81, 34.81, 34.81, 34.79, 34.79, 34.79, 34....   \n",
       "1    [34.79, 34.79, 34.79, 34.77, 34.77, 34.77, 34....   \n",
       "2    [34.75, 34.75, 34.75, 34.75, 34.79, 34.79, 34....   \n",
       "3    [34.77, 34.77, 34.77, 34.77, 34.77, 34.77, 34....   \n",
       "4    [34.55, 34.55, 34.55, 34.55, 34.55, 34.55, 34....   \n",
       "..                                                 ...   \n",
       "306  [34.43, 34.43, 34.43, 34.43, 34.43, 34.41, 34....   \n",
       "307  [36.05, 36.05, 36.05, 36.05, 36.03, 36.03, 36....   \n",
       "308  [36.07, 36.07, 36.07, 36.07, 36.05, 36.05, 36....   \n",
       "309                [36.07, 36.05, 36.05, 36.05, 36.05]   \n",
       "310  [34.41, 34.43, 34.43, 34.43, 34.43, 34.45, 34....   \n",
       "\n",
       "                                ibi  \n",
       "0                               NaN  \n",
       "1     [0.65625, 0.609375, 0.609375]  \n",
       "2                               NaN  \n",
       "3                               NaN  \n",
       "4    [0.640625, 0.640625, 0.578125]  \n",
       "..                              ...  \n",
       "306                             NaN  \n",
       "307                             NaN  \n",
       "308                             NaN  \n",
       "309                             NaN  \n",
       "310                             NaN  \n",
       "\n",
       "[311 rows x 7 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_data_frames[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_num_lst = []\n",
    "temp_num_lst = []\n",
    "\n",
    "for session in range(num_sessions):\n",
    "    eda_num_lst.append(target_data_frames[session+1]['eda'])\n",
    "    temp_num_lst.append(target_data_frames[session+1]['temp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0      [-0.06791500000000017, -0.110201, -0.025627999...\n",
       " 1      [0.006407000000000274, -0.021784000000000248, ...\n",
       " 2      [0.07944700000000005, 0.05125599999999997, 0.0...\n",
       " 3      [0.0038439999999999586, -0.03331699999999982, ...\n",
       " 4      [0.1241910000000006, 0.09474499999999964, 0.03...\n",
       "                              ...                        \n",
       " 306    [0.006400999999999435, -0.010241999999999862, ...\n",
       " 307    [0.0038450000000000983, -0.007689000000000057,...\n",
       " 308    [0.06279299999999965, 0.019222000000000072, -0...\n",
       " 309    [-0.010251999999999928, -0.0038450000000000983...\n",
       " 310    [-0.01408300000000029, -0.025605999999999796, ...\n",
       " Name: eda, Length: 311, dtype: object,\n",
       " 0      [-0.23577899999999907, -0.17427100000000095, -...\n",
       " 1      [-0.0012810000000000044, 0.0, -0.0012810000000...\n",
       " 2      [0.37749000000000166, 0.14736199999999933, 0.0...\n",
       " 3      [0.034598000000000795, -0.006406000000000134, ...\n",
       " 4      [0.00512600000000063, -0.012814000000000547, -...\n",
       "                              ...                        \n",
       " 260    [-0.033313999999999844, -0.04228399999999999, ...\n",
       " 261    [0.0358780000000003, -0.0410029999999999, -0.0...\n",
       " 262    [-0.02050200000000002, 0.023064000000000195, 0...\n",
       " 263    [-0.07560300000000097, -0.017939000000001926, ...\n",
       " 264    [0.36582799999999693, 0.08201000000000036, -0....\n",
       " Name: eda, Length: 265, dtype: object]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eda_num_lst[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]2023-04-02 19:21:45.108869: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-02 19:21:46.430201: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11/include:/usr/local/cuda-11/lib64::/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-04-02 19:21:46.430298: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11/include:/usr/local/cuda-11/lib64::/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-04-02 19:21:46.430305: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "1it [00:42, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session01번째 작업이 완료되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [01:21, 40.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session02번째 작업이 완료되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = {}\n",
    "for annot, session, eda, temp in tqdm(zip(annot_num_lst[:2], session_num_lst[:2], eda_num_lst[:2], temp_num_lst[:2])):\n",
    "\n",
    "    df_annotation = pd.read_csv('./org_KEMDy20/annotation/' + annot, skiprows=1)\n",
    "    file_name_lst = df_annotation[' .1']\n",
    "    \n",
    "    txt_file_lst = []\n",
    "    wav_file_lst = []\n",
    "    for file_name in file_name_lst:\n",
    "        txt_file_lst.append('./org_KEMDy20/' + session + '/' + file_name + '.txt')\n",
    "        wav_file_lst.append('./org_KEMDy20/' + session + '/' + file_name + '.wav')\n",
    "        \n",
    "    txt_embeddings = text_embeddings_func(txt_file_lst, tokenizer, model_txt)\n",
    "    wav_embeddings = wav_embedding_func(wav_file_lst, processor, model_wav)\n",
    "    \n",
    "    dataset[session] = build_dataset(file_name_lst, \n",
    "                txt_embeddings, \n",
    "                wav_embeddings,\n",
    "                eda,\n",
    "                temp,  \n",
    "                df_annotation.Emotion, \n",
    "                df_annotation.Arousal, \n",
    "                df_annotation.Valence)\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    print(f'{session}번째 작업이 완료되었습니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Session01', 'Session02'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['file_names', 'text_embeddings', 'wav_embeddings', 'EDA', 'Temp', 'Emotion', 'Arousal', 'Valence'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Session01'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# 1_3 = eda / temp 없음\n",
    "# with open('./model/data/paradeigma_dataset_1_3.pkl', 'wb') as f:\n",
    "#     pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('./model/data/paradeigma_dataset_1_4.pkl', 'wb') as f:\n",
    "    pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['file_names', 'text_embeddings', 'wav_embeddings', 'Emotion', 'Arousal', 'Valence'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./model/data/paradeigma_dataset_1_3.pkl', 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "dataset['Session01'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arplab/project/paradeigma/multi_modal/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['file_names', 'text_embeddings', 'wav_embeddings', 'EDA', 'Temp', 'Emotion', 'Arousal', 'Valence'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./model/data/paradeigma_dataset_1_4.pkl', 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "dataset['Session01'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20807e8adcbd42114165e2664a1949a43ef9aa8599fa0cbc30f37819eb44d659"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
