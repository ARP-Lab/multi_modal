{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# working model for tensorfusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arplab/project/paradeigma/multi_modal/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu117\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from torchmetrics import F1Score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('/home/arplab/project/paradeigma/multi_modal/model/data/paradeigma_KEMDY20_annotation_nonmissing.pkl', 'rb') as f:\n",
    "    annotation_20_nonmissing = pickle.load(f)\n",
    "\n",
    "with open('/home/arplab/project/paradeigma/multi_modal/model/data/paradeigma_KEMDY19_annotation_nonmissing.pkl', 'rb') as f:\n",
    "    annotation_19_nonmissing = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/home/arplab/project/paradeigma/multi_modal/model/data/paradeigma_KEMDY20_embedding_for_dataset.pkl', 'rb') as f:\n",
    "    embedding_20_dataset = pickle.load(f)\n",
    "\n",
    "with open('/home/arplab/project/paradeigma/multi_modal/model/data/paradeigma_KEMDY19_embedding_for_dataset.pkl', 'rb') as f:\n",
    "    embedding_19_dataset = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'neutral': 0,\n",
       "  'happy': 1,\n",
       "  'surprise': 2,\n",
       "  'disgust': 3,\n",
       "  'angry': 4,\n",
       "  'sad': 5,\n",
       "  'fear': 6,\n",
       "  'surprise;neutral': 7,\n",
       "  'neutral;sad': 8,\n",
       "  'happy;neutral': 9,\n",
       "  'angry;neutral': 10,\n",
       "  'neutral;disqust': 11,\n",
       "  'neutral;fear': 12,\n",
       "  'happy;surprise': 13,\n",
       "  'happy;angry;neutral': 14,\n",
       "  'angry;disqust': 15,\n",
       "  'happy;surprise;neutral': 16,\n",
       "  'happy;fear': 17,\n",
       "  'happy;neutral;fear': 18,\n",
       "  'angry;neutral;disqust': 19,\n",
       "  'neutral;disqust;sad': 20,\n",
       "  'angry;neutral;disqust;fear;sad': 21,\n",
       "  'happy;sad': 22,\n",
       "  'happy;neutral;disqust': 23},\n",
       " {0: 'neutral',\n",
       "  1: 'happy',\n",
       "  2: 'surprise',\n",
       "  3: 'disgust',\n",
       "  4: 'angry',\n",
       "  5: 'sad',\n",
       "  6: 'fear',\n",
       "  7: 'surprise;neutral',\n",
       "  8: 'neutral;sad',\n",
       "  9: 'happy;neutral',\n",
       "  10: 'angry;neutral',\n",
       "  11: 'neutral;disqust',\n",
       "  12: 'neutral;fear',\n",
       "  13: 'happy;surprise',\n",
       "  14: 'happy;angry;neutral',\n",
       "  15: 'angry;disqust',\n",
       "  16: 'happy;surprise;neutral',\n",
       "  17: 'happy;fear',\n",
       "  18: 'happy;neutral;fear',\n",
       "  19: 'angry;neutral;disqust',\n",
       "  20: 'neutral;disqust;sad',\n",
       "  21: 'angry;neutral;disqust;fear;sad',\n",
       "  22: 'happy;sad',\n",
       "  23: 'happy;neutral;disqust'})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoding Emotion for whole data\n",
    "# 사전에 실제로 encoding한 끝 수가 마지막 linear layer의 끝자리랑 맞아야 합니다. 아니면 CUDA error: CUBLAS_STATUS_EXECUTION_FAILED가 나는 것 같아요.\n",
    "# 예를 들어, label이 0~9, 11,13이렇게 12개가 되었어도, 0~13은 14개니까 마지막 레이어에서 14개 unit을 받아야 multiclass classification이 에러없이 진행됩니다!\n",
    "# 데이터에서 정답 라벨 인코딩: ['neutral', 'happy', 'surprise', 'disgust', 'angry', 'sad', 'fear']\n",
    "# 이 순서를 지켜서 라벨링을 해야함\n",
    "encode_dict = {'neutral':0, 'happy':1, 'surprise':2, 'disgust':3, 'angry':4, 'sad':5, 'fear':6,\n",
    "               'surprise;neutral': 7, 'neutral;sad': 8, 'happy;neutral': 9, 'angry;neutral': 10, \n",
    "               'neutral;disqust': 11, 'neutral;fear': 12, 'happy;surprise': 13, 'happy;angry;neutral': 14,\n",
    "               'angry;disqust': 15, 'happy;surprise;neutral': 16, 'happy;fear': 17,'happy;neutral;fear': 18,\n",
    "               'angry;neutral;disqust': 19, 'neutral;disqust;sad': 20, 'angry;neutral;disqust;fear;sad': 21,\n",
    "               'happy;sad': 22, 'happy;neutral;disqust': 23}\n",
    "decode_dict = {b:i for i, b in encode_dict.items()}\n",
    "encode_dict, decode_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9811/666273214.py:3: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  raw_dataset = raw_dataset.append(annotation_19_nonmissing, ignore_index = True)\n",
      "/tmp/ipykernel_9811/666273214.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  raw_dataset = raw_dataset.append(annotation_20_nonmissing, ignore_index = True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "raw_dataset = pd.DataFrame()\n",
    "raw_dataset = raw_dataset.append(annotation_19_nonmissing, ignore_index = True)\n",
    "raw_dataset = raw_dataset.append(annotation_20_nonmissing, ignore_index = True)\n",
    "merged_dataset = raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_label = list(merged_dataset['Emotion'].unique())\n",
    "\n",
    "def encode_emotion(x):\n",
    "    return emotion_label.index(x)\n",
    "\n",
    "merged_dataset['Emotion'] = merged_dataset['Emotion'].apply(encode_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_embeddings = {'wav':[], 'txt':[]}\n",
    "for session, embeddings in embedding_19_dataset[0].items():\n",
    "    merged_embeddings['wav'].append(embeddings)\n",
    "    \n",
    "for session, embeddings in embedding_19_dataset[1].items():\n",
    "    merged_embeddings['txt'].append(embeddings)\n",
    "    \n",
    "for session, embeddings in embedding_20_dataset[0].items():\n",
    "    merged_embeddings['wav'].append(embeddings)\n",
    "    \n",
    "for session, embeddings in embedding_20_dataset[1].items():\n",
    "    merged_embeddings['txt'].append(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "merged_embeddings['wav'] = torch.concat(merged_embeddings['wav'])\n",
    "merged_embeddings['txt'] = torch.concat(merged_embeddings['txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([21723, 149, 1024]), torch.Size([21723, 80, 768]), 21723)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_embeddings['wav'].shape,merged_embeddings['txt'].shape,len(merged_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_padding(ts_list, padding_length = 50, mode = 'constant'):\n",
    "    \n",
    "    padding_value=0\n",
    "    \n",
    "    if (type(ts_list) != type([])) :\n",
    "        ts_list = [padding_value] * padding_length\n",
    "    \n",
    "    elif len(ts_list) >= padding_length :\n",
    "        ts_list = ts_list[0:padding_length]\n",
    "    \n",
    "    elif mode == 'constant':\n",
    "        length = padding_length - len(ts_list)\n",
    "        extend_list = [padding_value] * length\n",
    "        ts_list = ts_list + extend_list    \n",
    "        \n",
    "    elif mode == 'replicate':\n",
    "        \n",
    "        quotient = padding_length // len(ts_list)\n",
    "        remainder = padding_length % len(ts_list)\n",
    "        result = ts_list * quotient\n",
    "        result += ts_list[:remainder]\n",
    "        ts_list = result\n",
    "        \n",
    "    return ts_list            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset['Scaled EDA'] = merged_dataset['Scaled EDA'].apply(sequence_padding)\n",
    "merged_dataset['Scaled TEMP'] = merged_dataset['Scaled TEMP'].apply(sequence_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stats.stackexchange.com/questions/107874/how-to-deal-with-a-skewed-class-in-binary-classification-having-many-features\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch dataset 만들기\n",
    "- 참고: https://tutorials.pytorch.kr/beginner/basics/data_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EtriDataset(Dataset):\n",
    "    def __init__(self, file_names, \n",
    "                 text_embeddings, \n",
    "                 wav_embeddings, \n",
    "                 Temp,\n",
    "                 EDA,\n",
    "                 Emotion,\n",
    "                 Emotion_ext, \n",
    "                 Arousal, \n",
    "                 Valence):\n",
    "        self.file_names = file_names\n",
    "        self.text_embeddings = text_embeddings\n",
    "        self.wav_embeddings = wav_embeddings\n",
    "        self.temp = Temp\n",
    "        self.eda = EDA\n",
    "        self.label_emotion = Emotion\n",
    "        self.label_emotion_ext = Emotion_ext\n",
    "        self.label_arousal = Arousal\n",
    "        self.label_valence = Valence\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text_embeddings = self.text_embeddings[idx]\n",
    "        wav_embeddings = self.wav_embeddings[idx]\n",
    "        temp = self.temp[idx]\n",
    "        eda = self.eda[idx]\n",
    "        label_emotion = self.label_emotion[idx]\n",
    "        label_emotion_ext = self.label_emotion_ext[idx]\n",
    "        label_arousal = self.label_arousal[idx]\n",
    "        label_valence = self.label_valence[idx]\n",
    "        return text_embeddings, wav_embeddings, temp, eda, label_emotion, label_emotion_ext, label_arousal, label_valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Segment ID', 'Emotion', 'Valence', 'Arousal', 'emotion_vector',\n",
       "       'valence_vector', 'arousal_vector', 'EDA', 'TEMP', 'EDA length',\n",
       "       'TEMP length', 'Scaled EDA', 'Scaled TEMP'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data load 및 나누기: https://076923.github.io/posts/Python-pytorch-11/\n",
    "\n",
    "# session을 통합시킨 데이터 셋을 만들었을 때\n",
    "dataset = EtriDataset(file_names = merged_dataset['Segment ID'],\n",
    "                      text_embeddings = merged_embeddings['txt'],\n",
    "                      wav_embeddings = merged_embeddings['wav'],\n",
    "                      Emotion = merged_dataset['Emotion'],\n",
    "                      Arousal = merged_dataset['Arousal'],\n",
    "                      Valence = merged_dataset['Valence'],\n",
    "                      EDA = torch.concat(list(merged_dataset['Scaled EDA'].apply(lambda x: torch.tensor(x).view(1,-1)))), ## 일부 세션 데이터만 사용하므로, 길이를 맞춰주기 위해 일부 slicing함\n",
    "                      Temp = torch.concat(list(merged_dataset['Scaled EDA'].apply(lambda x: torch.tensor(x).view(1,-1)))), ## 일부 세션 데이터만 사용하므로, 길이를 맞춰주기 위해 일부 slicing함\n",
    "                      Emotion_ext = torch.concat(list(merged_dataset['Scaled EDA'].apply(lambda x: torch.tensor(x).view(1,-1))))) ## 일부 세션 데이터만 사용하므로, 길이를 맞춰주기 위해 일부 slicing함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15206 3259 3258\n",
      "Training Data Size : 15206\n",
      "Validation Data Size : 3258\n",
      "Testing Data Size : 3259\n"
     ]
    }
   ],
   "source": [
    "dataset_size = len(dataset)\n",
    "train_size = int(dataset_size * 0.7)\n",
    "validation_size = int(dataset_size * 0.15)\n",
    "test_size = dataset_size - train_size - validation_size\n",
    "\n",
    "train_dataset, validation_dataset, test_dataset = random_split(dataset, [train_size, validation_size, test_size])\n",
    "\n",
    "print(train_size, test_size, validation_size)\n",
    "print(f\"Training Data Size : {len(train_dataset)}\")\n",
    "print(f\"Validation Data Size : {len(validation_dataset)}\")\n",
    "print(f\"Testing Data Size : {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([149, 1024]), torch.Size([33]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data size\n",
    "merged_embeddings['wav'][0].shape , torch.Tensor(merged_dataset['EDA'][0]).shape\n",
    "# raw_dataset[session]['wav_embeddings'][0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True, drop_last=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True, drop_last=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NetWork 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNetwork_pre(nn.Module):\n",
    "    def __init__(self, input_length, input_width):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_length*input_width, 768)\n",
    "        self.gelu1 = nn.GELU()\n",
    "        self.bn1 = nn.BatchNorm1d(768)\n",
    "        self.fc2 = nn.Linear(768, 512)\n",
    "        self.gelu2 = nn.GELU()\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.fc3 = nn.Linear(512, 32)\n",
    "        self.gelu3 = nn.GELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.gelu1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.gelu2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.fc3(x)\n",
    "        output = self.gelu3(x)\n",
    "        return output\n",
    "    \n",
    "class ConvNetwork_pre(nn.Module):\n",
    "    def __init__(self, input_channel):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels = input_channel, out_channels= 32, kernel_size = 3, padding = 1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(in_channels = 32, out_channels = 10, kernel_size = 3, padding = 1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        output = self.relu2(x)\n",
    "        return output\n",
    "\n",
    "class ConvNetwork_final(nn.Module):\n",
    "    def __init__(self, input_channel):\n",
    "        super().__init__()\n",
    "        self.conv2d_1 = nn.Conv2d(in_channels = input_channel, out_channels = 64, kernel_size=2)\n",
    "        self.leakyrelu_1 = nn.LeakyReLU()\n",
    "        self.maxpool2d_1 = nn.MaxPool2d(2)\n",
    "        self.conv2d_2 = nn.Conv2d(in_channels = 64, out_channels = 32, kernel_size=2)\n",
    "        self.leakyrelu_2 = nn.LeakyReLU()\n",
    "        self.maxpool2d_2 = nn.MaxPool2d(2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(224, 64)\n",
    "        self.leakyrelu_3 = nn.LeakyReLU()\n",
    "        self.batchnorm = nn.BatchNorm1d(64)\n",
    "        self.drop = nn.Dropout(p=0.25)\n",
    "        self.fc2 = nn.Linear(64, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv2d_1(x)\n",
    "        x = self.leakyrelu_1(x)\n",
    "        x = self.maxpool2d_1(x)\n",
    "        x = self.conv2d_2(x)\n",
    "        x = self.leakyrelu_2(x)\n",
    "        x = self.maxpool2d_2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.leakyrelu_3(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.drop(x)\n",
    "        output = self.fc2(x)  \n",
    "        return output\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorFusionMixer(nn.Module):\n",
    "    def __init__(self, ModelA, ModelB, ModelC, ModelD, ModelE):\n",
    "        super().__init__()\n",
    "        self.ModelA = ModelA\n",
    "        self.ModelB = ModelB\n",
    "        self.ModelC = ModelC\n",
    "        self.ModelD = ModelD\n",
    "        self.Model_cnn_final = ModelE\n",
    "        # self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def tensor_fusion(self, batch_arr1, batch_arr2, batch_arr3):\n",
    "        fusion_matrix_lst = []\n",
    "        for i, (arr1, arr2, arr3) in enumerate(zip(batch_arr1, batch_arr2, batch_arr3)):\n",
    "            arr1 = arr1.unsqueeze(-1).unsqueeze(-1)\n",
    "            arr2 = arr2.unsqueeze(0).unsqueeze(-1)\n",
    "            arr3 = arr3.squeeze().unsqueeze(0).unsqueeze(0)\n",
    "            \n",
    "            # outer_matrix = torch.einsum('i,j,kp->ijk', arr1, arr2, arr3)\n",
    "            kron_matrix = torch.kron(torch.kron(arr1,arr2), arr3)\n",
    "            l, w, d = kron_matrix.shape\n",
    "            \n",
    "            kron_matrix = kron_matrix.view(-1, l, w, d)\n",
    "            fusion_matrix_lst.append(kron_matrix)\n",
    "            \n",
    "        fusion_matrix = torch.concat(fusion_matrix_lst)\n",
    "        # fusion_matrix = fusion_matrix.unsqueeze(-1)\n",
    "        \n",
    "        return fusion_matrix\n",
    "        \n",
    "    def forward(self, x1, x2, x3, x4):\n",
    "        x1 = self.ModelA(x1)\n",
    "        x2 = self.ModelB(x2)\n",
    "        x3 = self.ModelC(x3)\n",
    "        x4 = self.ModelD(x4)\n",
    "        \n",
    "        x5 = torch.cat([x3,x4], dim=0)\n",
    "        fusion_matrix = self.tensor_fusion(x1, x2, x5)\n",
    "        \n",
    "        output = self.Model_cnn_final(fusion_matrix) \n",
    "        return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 4 GPUs!\n",
      "DataParallel(\n",
      "  (module): TensorFusionMixer(\n",
      "    (ModelA): MLPNetwork_pre(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=61440, out_features=768, bias=True)\n",
      "      (gelu1): GELU(approximate='none')\n",
      "      (bn1): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc2): Linear(in_features=768, out_features=512, bias=True)\n",
      "      (gelu2): GELU(approximate='none')\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc3): Linear(in_features=512, out_features=32, bias=True)\n",
      "      (gelu3): GELU(approximate='none')\n",
      "    )\n",
      "    (ModelB): MLPNetwork_pre(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=152576, out_features=768, bias=True)\n",
      "      (gelu1): GELU(approximate='none')\n",
      "      (bn1): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc2): Linear(in_features=768, out_features=512, bias=True)\n",
      "      (gelu2): GELU(approximate='none')\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc3): Linear(in_features=512, out_features=32, bias=True)\n",
      "      (gelu3): GELU(approximate='none')\n",
      "    )\n",
      "    (ModelC): ConvNetwork_pre(\n",
      "      (conv1): Conv1d(50, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (relu1): ReLU()\n",
      "      (conv2): Conv1d(32, 10, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (relu2): ReLU()\n",
      "    )\n",
      "    (ModelD): ConvNetwork_pre(\n",
      "      (conv1): Conv1d(50, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (relu1): ReLU()\n",
      "      (conv2): Conv1d(32, 10, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (relu2): ReLU()\n",
      "    )\n",
      "    (Model_cnn_final): ConvNetwork_final(\n",
      "      (conv2d_1): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "      (leakyrelu_1): LeakyReLU(negative_slope=0.01)\n",
      "      (maxpool2d_1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (conv2d_2): Conv2d(64, 32, kernel_size=(2, 2), stride=(1, 1))\n",
      "      (leakyrelu_2): LeakyReLU(negative_slope=0.01)\n",
      "      (maxpool2d_2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=224, out_features=64, bias=True)\n",
      "      (leakyrelu_3): LeakyReLU(negative_slope=0.01)\n",
      "      (batchnorm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop): Dropout(p=0.25, inplace=False)\n",
      "      (fc2): Linear(in_features=64, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# txt_input_length, txt_input_width = raw_dataset[session]['text_embeddings'][0].shape | 마지막엔 지울 것\n",
    "# _, wav_input_length, wav_input_width = raw_dataset[session]['wav_embeddings'][0].shape\n",
    "txt_input_length, txt_input_width = dataset.text_embeddings[0].shape\n",
    "wav_input_length, wav_input_width = dataset.wav_embeddings[0].shape\n",
    "temp_input_length = dataset.temp[0].shape[0]\n",
    "eda_input_length = dataset.eda[0].shape[0]\n",
    "\n",
    "# tf_mixer에 들어갈 wav mlp, txt mlp 선언\n",
    "model_mlp_txt = MLPNetwork_pre(txt_input_length,txt_input_width).to(device)\n",
    "model_mlp_wav = MLPNetwork_pre(wav_input_length,wav_input_width).to(device)\n",
    "model_conv_temp = ConvNetwork_pre(temp_input_length).to(device)\n",
    "model_conv_eda = ConvNetwork_pre(eda_input_length).to(device)\n",
    "\n",
    "model_cnn_final = ConvNetwork_final(32).to(device)\n",
    "\n",
    "# 최종 모델 선언\n",
    "model_tf_cnn_mixer = TensorFusionMixer(ModelA = model_mlp_txt, \n",
    "                                   ModelB = model_mlp_wav,\n",
    "                                   ModelC = model_conv_temp,\n",
    "                                   ModelD = model_conv_eda,\n",
    "                                   ModelE = model_cnn_final).to(device)\n",
    "\n",
    "# model 병렬 학습 처리\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model_mlp_txt = nn.DataParallel(model_mlp_txt).to(device)\n",
    "    model_mlp_wav = nn.DataParallel(model_mlp_wav).to(device)\n",
    "    model_conv_temp = nn.DataParallel(model_conv_temp).to(device)\n",
    "    model_conv_eda = nn.DataParallel(model_conv_eda).to(device)\n",
    "    model_tf_cnn_mixer = nn.DataParallel(model_tf_cnn_mixer).to(device)\n",
    "print(model_tf_cnn_mixer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CCC, self).__init__()\n",
    "        self.mean = torch.mean\n",
    "        self.var = torch.var\n",
    "        self.sum = torch.sum\n",
    "        self.sqrt = torch.sqrt\n",
    "        self.std = torch.std\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        mean_gt = self.mean (target, 0)\n",
    "        mean_pred = self.mean (pred, 0)\n",
    "        var_gt = self.var (target, 0)\n",
    "        var_pred = self.var (pred, 0)\n",
    "        v_pred = pred - mean_pred\n",
    "        v_gt = target - mean_gt\n",
    "        cor = self.sum (v_pred * v_gt) / (self.sqrt(self.sum(v_pred ** 2)) * self.sqrt(self.sum(v_gt ** 2)))\n",
    "        sd_gt = self.std(target)\n",
    "        sd_pred = self.std(pred)\n",
    "        numerator = 2 * cor * sd_gt * sd_pred\n",
    "        denominator = var_gt + var_pred + (mean_gt-mean_pred) ** 2\n",
    "        ccc = numerator / denominator\n",
    "        return ccc\n",
    "    \n",
    "ccc = CCC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class weighted_MSELoss(nn.Module):\n",
    "    def __init__(self, weight):\n",
    "        super().__init__()\n",
    "        self.weight = weight.to(device)\n",
    "    def forward(self,inputs,targets):\n",
    "        inputs[]\n",
    "        return ((inputs - targets)**2) * self.weight"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습을 위한 train, test method 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)    \n",
    "    # data 순서: text_embeddings, wav_embeddings, temp, eda, label_emotion, label_emotion_ext, label_arousal, label_valence\n",
    "\n",
    "    for batch, (X_txt, X_wav, X_temp, X_eda, \n",
    "                    label_emotion, label_emotion_ext, label_arousal, label_valence) in enumerate(dataloader): \n",
    "\n",
    "        # 예측 오류 계산 \n",
    "        X_txt, X_wav, X_temp, X_eda, y_v, y_a= X_txt.to(device), X_wav.to(device), X_temp.to(device), X_eda.to(device),label_valence.type(torch.float32).to(device), label_arousal.type(torch.float32).to(device)\n",
    "        \n",
    "        X_temp = X_temp.unsqueeze(dim=-1)\n",
    "        X_eda = X_eda.unsqueeze(dim=-1)\n",
    "        y_v = y_v.unsqueeze(dim=-1)\n",
    "        y_a = y_a.unsqueeze(dim=-1)\n",
    "\n",
    "        \n",
    "        pred = model(X_txt, X_wav, X_temp, X_eda)\n",
    "    \n",
    "        pred_v = pred[:,0].unsqueeze(dim=-1)\n",
    "        pred_a = pred[:,1].unsqueeze(dim=-1)\n",
    "        \n",
    "        y = torch.concat([y_v, y_a], dim=-1)\n",
    "        \n",
    "        # loss_a = loss_fn(pred_a, y_a)\n",
    "        # loss_v = loss_fn(pred_v, y_v)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        pred_ccc_v= ccc(pred_v, y_v)\n",
    "        pred_ccc_a = ccc(pred_a, y_a)\n",
    "\n",
    "        \n",
    "        ccc_mean = (pred_ccc_a + pred_ccc_v) / 2\n",
    "\n",
    "        # 역전파\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # loss_a.backward(retain_graph = True)\n",
    "        # loss_v.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X_txt)\n",
    "            # loss_a, loss_v, current = loss_a.item(), loss_v.item(), batch * len(X_txt)\n",
    "            # print(f\"loss_a: {loss_a:>7f}, loss_v: {loss_v:>7f}\")\n",
    "            print(f\"loss: {loss}\")\n",
    "            print(f\"ccc_mean : {ccc_mean.item():>9f}, Arousal_ccc : {pred_ccc_a.item():>9f}, Valence_ccc : {pred_ccc_v.item():>9f}\".format())\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn, mode = 'test'):\n",
    "    \n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    # test_loss_a = 0\n",
    "    # test_loss_v = 0\n",
    "    ccc_mean = 0\n",
    "    ccc_a = 0\n",
    "    ccc_v = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # data 순서: text_embeddings, wav_embeddings, temp, eda, label_emotion, label_emotion_ext, label_arousal, label_valence\n",
    "        for batch, (X_txt, X_wav, X_temp, X_eda, \n",
    "                    label_emotion, label_emotion_ext, label_arousal, label_valence) in enumerate(dataloader): \n",
    "\n",
    "            # 예측 오류 계산 \n",
    "            X_txt, X_wav, X_temp, X_eda, y_v, y_a= X_txt.to(device), X_wav.to(device), X_temp.to(device), X_eda.to(device),label_valence.type(torch.float32).to(device), label_arousal.type(torch.float32).to(device)\n",
    "            \n",
    "            X_temp = X_temp.unsqueeze(dim=-1)\n",
    "            X_eda = X_eda.unsqueeze(dim=-1)\n",
    "            y_v = y_v.unsqueeze(dim=-1)\n",
    "            y_a = y_a.unsqueeze(dim=-1)\n",
    "\n",
    "            pred = model(X_txt, X_wav, X_temp, X_eda)\n",
    "            \n",
    "            pred_v = pred[:,0].unsqueeze(dim=-1)\n",
    "            pred_a = pred[:,1].unsqueeze(dim=-1)\n",
    "            y = torch.concat([y_v, y_a], dim=-1)\n",
    "\n",
    "            \n",
    "            loss = loss_fn(pred, y)\n",
    "            test_loss += loss\n",
    "            \n",
    "            # loss_a = loss_fn(pred_a, y_a)\n",
    "            # loss_v = loss_fn(pred_v, y_v)\n",
    "\n",
    "            # test_loss_a += loss_a\n",
    "            # test_loss_v += loss_v\n",
    "\n",
    "            pred_ccc_a = ccc(pred_a, y_a)\n",
    "            pred_ccc_v = ccc(pred_v, y_v)\n",
    "            \n",
    "            ccc_a += pred_ccc_a\n",
    "            ccc_v += pred_ccc_v\n",
    "      \n",
    "      \n",
    "    test_loss /= num_batches  \n",
    "    # test_loss_v /= num_batches    \n",
    "    # test_loss_a /= num_batches\n",
    "    \n",
    "    ccc_v /= num_batches\n",
    "    ccc_a /= num_batches\n",
    "    ccc_mean = (ccc_a + ccc_v) / 2\n",
    "\n",
    "    \n",
    "    if mode == 'test':\n",
    "        # print(f\"loss_a: {loss_a:>7f}, loss_v: {loss_v:>7f}\")\n",
    "        print(f\"loss: {loss:>7f}\")\n",
    "        print(f\"Test ccc_mean : {ccc_mean.item():>9f}, Arousal_ccc : {pred_ccc_a.item():>9f}, Valence_ccc : {pred_ccc_v.item():>9f}\")\n",
    "    elif mode == 'val':\n",
    "        # print(f\"loss_a: {loss_a:>7f}, loss_v: {loss_v:>7f}\")\n",
    "        print(f\"loss: {loss:>7f}\")\n",
    "        print(f\"Val ccc_mean : {ccc_mean.item():>9f}, Arousal_ccc : {pred_ccc_a.item():>9f}, Valence_ccc : {pred_ccc_v.item():>9f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 지난 저장한 모델이 있다면\n",
    "# PATH = './data/test_model.pkl'\n",
    "# model_tf_mixer = torch.load(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = nn.CrossEntropyLoss(weight=weight_for_class).to(device)\n",
    "# loss_fn = nn.CrossEntropyLoss().to(device) # weigth를 주기위해 위의 loss로 임시 변경\n",
    "loss_fn = weighted_MSELoss(weight = weight_for_class).to(device) # multi target regression(감정별로 count 한 타겟)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "\n",
    "# optimizer = optim.SGD(model_tf_mixer.parameters(), lr=lr) # classification\n",
    "optimizer = optim.Adagrad(model_tf_cnn_mixer.parameters(), lr=lr) # regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start training mlp fusion mixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Training Parameters\n",
    "lr = 1e-3\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adagrad(model_tf_cnn_mixer.parameters(), lr=lr) # regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Epoch 1----------------\n",
      "Training...............\n",
      "loss: 0.12521889805793762\n",
      "ccc_mean :  0.729150, Arousal_ccc :  0.634851, Valence_ccc :  0.823448\n",
      "Validation...............\n",
      "loss: 0.525226\n",
      "Val ccc_mean :  0.255874, Arousal_ccc :  0.421532, Valence_ccc :  0.191950\n",
      "---------------Epoch 2----------------\n",
      "Training...............\n",
      "loss: 0.09230402112007141\n",
      "ccc_mean :  0.837825, Arousal_ccc :  0.803565, Valence_ccc :  0.872086\n",
      "Validation...............\n",
      "loss: 0.488434\n",
      "Val ccc_mean :  0.239326, Arousal_ccc :  0.242905, Valence_ccc :  0.196564\n",
      "---------------Epoch 3----------------\n",
      "Training...............\n",
      "loss: 0.11502836644649506\n",
      "ccc_mean :  0.812366, Arousal_ccc :  0.771519, Valence_ccc :  0.853213\n",
      "Validation...............\n",
      "loss: 0.404094\n",
      "Val ccc_mean :  0.254568, Arousal_ccc :  0.236322, Valence_ccc :  0.387527\n",
      "---------------Epoch 4----------------\n",
      "Training...............\n",
      "loss: 0.09919228404760361\n",
      "ccc_mean :  0.828372, Arousal_ccc :  0.756645, Valence_ccc :  0.900098\n",
      "Validation...............\n",
      "loss: 0.506235\n",
      "Val ccc_mean :  0.235475, Arousal_ccc :  0.320670, Valence_ccc :  0.288959\n",
      "---------------Epoch 5----------------\n",
      "Training...............\n",
      "loss: 0.08847508579492569\n",
      "ccc_mean :  0.840015, Arousal_ccc :  0.808115, Valence_ccc :  0.871914\n",
      "Validation...............\n",
      "loss: 0.596528\n",
      "Val ccc_mean :  0.262487, Arousal_ccc :  0.261213, Valence_ccc :  0.124001\n",
      "---------------Epoch 6----------------\n",
      "Training...............\n",
      "loss: 0.11783716082572937\n",
      "ccc_mean :  0.813165, Arousal_ccc :  0.743221, Valence_ccc :  0.883108\n",
      "Validation...............\n",
      "loss: 0.485470\n",
      "Val ccc_mean :  0.265169, Arousal_ccc :  0.171017, Valence_ccc :  0.318165\n",
      "---------------Epoch 7----------------\n",
      "Training...............\n",
      "loss: 0.07944667339324951\n",
      "ccc_mean :  0.831449, Arousal_ccc :  0.767747, Valence_ccc :  0.895151\n",
      "Validation...............\n",
      "loss: 0.533725\n",
      "Val ccc_mean :  0.211570, Arousal_ccc :  0.150982, Valence_ccc :  0.098319\n",
      "---------------Epoch 8----------------\n",
      "Training...............\n",
      "loss: 0.14903277158737183\n",
      "ccc_mean :  0.779981, Arousal_ccc :  0.782500, Valence_ccc :  0.777463\n",
      "Validation...............\n",
      "loss: 0.384383\n",
      "Val ccc_mean :  0.239298, Arousal_ccc :  0.181558, Valence_ccc :  0.221975\n",
      "---------------Epoch 9----------------\n",
      "Training...............\n",
      "loss: 0.096148282289505\n",
      "ccc_mean :  0.811654, Arousal_ccc :  0.736972, Valence_ccc :  0.886337\n",
      "Validation...............\n",
      "loss: 0.466144\n",
      "Val ccc_mean :  0.254247, Arousal_ccc :  0.387401, Valence_ccc :  0.293917\n",
      "---------------Epoch 10----------------\n",
      "Training...............\n",
      "loss: 0.09368651360273361\n",
      "ccc_mean :  0.822464, Arousal_ccc :  0.756679, Valence_ccc :  0.888249\n",
      "Validation...............\n",
      "loss: 0.366339\n",
      "Val ccc_mean :  0.244267, Arousal_ccc :  0.113862, Valence_ccc :  0.280613\n",
      "---------------Epoch 11----------------\n",
      "Training...............\n",
      "loss: 0.09638065099716187\n",
      "ccc_mean :  0.817330, Arousal_ccc :  0.755285, Valence_ccc :  0.879374\n",
      "Validation...............\n",
      "loss: 0.370021\n",
      "Val ccc_mean :  0.202188, Arousal_ccc :  0.431388, Valence_ccc :  0.202223\n",
      "---------------Epoch 12----------------\n",
      "Training...............\n",
      "loss: 0.13409698009490967\n",
      "ccc_mean :  0.749664, Arousal_ccc :  0.694035, Valence_ccc :  0.805293\n",
      "Validation...............\n",
      "loss: 0.508095\n",
      "Val ccc_mean :  0.264863, Arousal_ccc :  0.343528, Valence_ccc :  0.175353\n",
      "---------------Epoch 13----------------\n",
      "Training...............\n",
      "loss: 0.08004529774188995\n",
      "ccc_mean :  0.860308, Arousal_ccc :  0.830048, Valence_ccc :  0.890567\n",
      "Validation...............\n",
      "loss: 0.493180\n",
      "Val ccc_mean :  0.261691, Arousal_ccc :  0.316713, Valence_ccc :  0.280515\n",
      "---------------Epoch 14----------------\n",
      "Training...............\n",
      "loss: 0.091944620013237\n",
      "ccc_mean :  0.854729, Arousal_ccc :  0.826883, Valence_ccc :  0.882576\n",
      "Validation...............\n",
      "loss: 0.294652\n",
      "Val ccc_mean :  0.237499, Arousal_ccc :  0.511625, Valence_ccc :  0.357789\n",
      "---------------Epoch 15----------------\n",
      "Training...............\n",
      "loss: 0.1047503799200058\n",
      "ccc_mean :  0.798615, Arousal_ccc :  0.748166, Valence_ccc :  0.849065\n",
      "Validation...............\n",
      "loss: 0.397856\n",
      "Val ccc_mean :  0.233881, Arousal_ccc :  0.184014, Valence_ccc :  0.312860\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"---------------Epoch {epoch+1}----------------\")\n",
    "    print(\"Training...............\")\n",
    "    train(train_dataloader, model_tf_cnn_mixer, criterion, optimizer)\n",
    "    print(\"Validation...............\")\n",
    "    test(validation_dataloader, model_tf_cnn_mixer, criterion, mode = 'val')\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실험을 위해 모델 저장\n",
    "PATH = '/home/arplab/project/paradeigma/multi_modal/model/data/paradeigma_test_model_multilabel_CNN.pkl'\n",
    "torch.save(model_tf_cnn_mixer, PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic TensorFusionNet 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.510035\n",
      "Test ccc_mean :  0.231434, Arousal_ccc :  0.393748, Valence_ccc :  0.167040\n"
     ]
    }
   ],
   "source": [
    "test(test_dataloader, model_tf_cnn_mixer, criterion, mode = 'test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bdc1fd12ca460d5768d71e9df3d9063ef832ce64a62e55a1a523c8c99752868e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
