{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkhk172216\u001b[0m (\u001b[33mtoez\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# working model for tensorfusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu117\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from torchmetrics import F1Score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/40toez/project/kyungho/multi_modal\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./model/data/paradeigma_KEMDY20_annotation_nonmissing.pkl', 'rb') as f:\n",
    "    annotation_20_nonmissing = pickle.load(f)\n",
    "\n",
    "with open('./model/data/paradeigma_KEMDY19_annotation_nonmissing.pkl', 'rb') as f:\n",
    "    annotation_19_nonmissing = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./model/data/paradeigma_KEMDY20_embedding_for_dataset.pkl', 'rb') as f:\n",
    "    embedding_20_dataset = pickle.load(f)\n",
    "\n",
    "with open('./model/data/paradeigma_KEMDY19_embedding_for_dataset.pkl', 'rb') as f:\n",
    "    embedding_19_dataset = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1454891/666273214.py:3: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  raw_dataset = raw_dataset.append(annotation_19_nonmissing, ignore_index = True)\n",
      "/tmp/ipykernel_1454891/666273214.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  raw_dataset = raw_dataset.append(annotation_20_nonmissing, ignore_index = True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "raw_dataset = pd.DataFrame()\n",
    "raw_dataset = raw_dataset.append(annotation_19_nonmissing, ignore_index = True)\n",
    "raw_dataset = raw_dataset.append(annotation_20_nonmissing, ignore_index = True)\n",
    "merged_dataset = raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_label = list(merged_dataset['Emotion'].unique())\n",
    "\n",
    "def encode_emotion(x):\n",
    "    return emotion_label.index(x)\n",
    "\n",
    "merged_dataset['Emotion'] = merged_dataset['Emotion'].apply(encode_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_embeddings = {'wav':[], 'txt':[]}\n",
    "for session, embeddings in embedding_19_dataset[0].items():\n",
    "    merged_embeddings['wav'].append(embeddings)\n",
    "    \n",
    "for session, embeddings in embedding_19_dataset[1].items():\n",
    "    merged_embeddings['txt'].append(embeddings)\n",
    "    \n",
    "for session, embeddings in embedding_20_dataset[0].items():\n",
    "    merged_embeddings['wav'].append(embeddings)\n",
    "    \n",
    "for session, embeddings in embedding_20_dataset[1].items():\n",
    "    merged_embeddings['txt'].append(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "merged_embeddings['wav'] = torch.concat(merged_embeddings['wav'])\n",
    "merged_embeddings['txt'] = torch.concat(merged_embeddings['txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([21723, 149, 1024]), torch.Size([21723, 80, 768]), 21723)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_embeddings['wav'].shape,merged_embeddings['txt'].shape,len(merged_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_padding(ts_list, padding_length = 50, mode = 'constant'):\n",
    "    \n",
    "    padding_value=0\n",
    "    \n",
    "    if (type(ts_list) != type([])) :\n",
    "        ts_list = [padding_value] * padding_length\n",
    "    \n",
    "    elif len(ts_list) >= padding_length :\n",
    "        ts_list = ts_list[0:padding_length]\n",
    "    \n",
    "    elif mode == 'constant':\n",
    "        length = padding_length - len(ts_list)\n",
    "        extend_list = [padding_value] * length\n",
    "        ts_list = ts_list + extend_list    \n",
    "\n",
    "    elif mode == 'replicate':\n",
    "        \n",
    "        quotient = padding_length // len(ts_list)\n",
    "        remainder = padding_length % len(ts_list)\n",
    "        result = ts_list * quotient\n",
    "        result += ts_list[:remainder]\n",
    "        ts_list = result\n",
    "\n",
    "    return ts_list            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset['Scaled EDA'] = merged_dataset['Scaled EDA'].apply(sequence_padding)\n",
    "merged_dataset['Scaled TEMP'] = merged_dataset['Scaled TEMP'].apply(sequence_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stats.stackexchange.com/questions/107874/how-to-deal-with-a-skewed-class-in-binary-classification-having-many-features\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch dataset 만들기\n",
    "- 참고: https://tutorials.pytorch.kr/beginner/basics/data_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EtriDataset(Dataset):\n",
    "    def __init__(self, file_names, \n",
    "                 text_embeddings, \n",
    "                 wav_embeddings, \n",
    "                 Temp,\n",
    "                 EDA,\n",
    "                 Emotion,\n",
    "                 Emotion_ext, \n",
    "                 Arousal, \n",
    "                 Valence):\n",
    "        self.file_names = file_names\n",
    "        self.text_embeddings = text_embeddings\n",
    "        self.wav_embeddings = wav_embeddings\n",
    "        self.temp = Temp\n",
    "        self.eda = EDA\n",
    "        self.label_emotion = Emotion\n",
    "        self.label_emotion_ext = Emotion_ext\n",
    "        self.label_arousal = Arousal\n",
    "        self.label_valence = Valence\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text_embeddings = self.text_embeddings[idx]\n",
    "        wav_embeddings = self.wav_embeddings[idx]\n",
    "        temp = self.temp[idx]\n",
    "        eda = self.eda[idx]\n",
    "        label_emotion = self.label_emotion[idx]\n",
    "        label_emotion_ext = self.label_emotion_ext[idx]\n",
    "        label_arousal = self.label_arousal[idx]\n",
    "        label_valence = self.label_valence[idx]\n",
    "        return text_embeddings, wav_embeddings, temp, eda, label_emotion, label_emotion_ext, label_arousal, label_valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Segment ID', 'Emotion', 'Valence', 'Arousal', 'emotion_vector',\n",
       "       'valence_vector', 'arousal_vector', 'EDA', 'TEMP', 'EDA length',\n",
       "       'TEMP length', 'Scaled EDA', 'Scaled TEMP'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data load 및 나누기: https://076923.github.io/posts/Python-pytorch-11/\n",
    "\n",
    "# session을 통합시킨 데이터 셋을 만들었을 때\n",
    "dataset = EtriDataset(file_names = merged_dataset['Segment ID'],\n",
    "                      text_embeddings = merged_embeddings['txt'],\n",
    "                      wav_embeddings = merged_embeddings['wav'],\n",
    "                      Emotion = merged_dataset['Emotion'],\n",
    "                      Arousal = merged_dataset['Arousal'],\n",
    "                      Valence = merged_dataset['Valence'],\n",
    "                      EDA = torch.concat(list(merged_dataset['Scaled EDA'].apply(lambda x: torch.tensor(x).view(1,-1)))), ## 일부 세션 데이터만 사용하므로, 길이를 맞춰주기 위해 일부 slicing함\n",
    "                      Temp = torch.concat(list(merged_dataset['Scaled EDA'].apply(lambda x: torch.tensor(x).view(1,-1)))), ## 일부 세션 데이터만 사용하므로, 길이를 맞춰주기 위해 일부 slicing함\n",
    "                      Emotion_ext = torch.concat(list(merged_dataset['Scaled EDA'].apply(lambda x: torch.tensor(x).view(1,-1))))) ## 일부 세션 데이터만 사용하므로, 길이를 맞춰주기 위해 일부 slicing함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15206 3259 3258\n",
      "Training Data Size : 15206\n",
      "Validation Data Size : 3258\n",
      "Testing Data Size : 3259\n"
     ]
    }
   ],
   "source": [
    "dataset_size = len(dataset)\n",
    "train_size = int(dataset_size * 0.7)\n",
    "validation_size = int(dataset_size * 0.15)\n",
    "test_size = dataset_size - train_size - validation_size\n",
    "\n",
    "train_dataset, validation_dataset, test_dataset = random_split(dataset, [train_size, validation_size, test_size])\n",
    "\n",
    "print(train_size, test_size, validation_size)\n",
    "print(f\"Training Data Size : {len(train_dataset)}\")\n",
    "print(f\"Validation Data Size : {len(validation_dataset)}\")\n",
    "print(f\"Testing Data Size : {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([149, 1024]), torch.Size([33]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data size\n",
    "merged_embeddings['wav'][0].shape , torch.Tensor(merged_dataset['EDA'][0]).shape\n",
    "# raw_dataset[session]['wav_embeddings'][0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True, drop_last=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=256, shuffle=True, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=256, shuffle=True, drop_last=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NetWork 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/40toez/project/kyungho/multi_modal/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py:88: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNetwork_pre(nn.Module):\n",
    "    def __init__(self, input_length, input_width):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_length*input_width, 768)\n",
    "        self.gelu1 = nn.GELU()\n",
    "        self.bn1 = nn.BatchNorm1d(768)\n",
    "        self.fc2 = nn.Linear(768, 512)\n",
    "        self.gelu2 = nn.GELU()\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.fc3 = nn.Linear(512, 32)\n",
    "        self.gelu3 = nn.GELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.gelu1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.gelu2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.fc3(x)\n",
    "        output = self.gelu3(x)\n",
    "        return output\n",
    "    \n",
    "class ConvNetwork_pre(nn.Module):\n",
    "    def __init__(self, input_channel):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels = input_channel, out_channels= 32, kernel_size = 3, padding = 1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(in_channels = 32, out_channels = 10, kernel_size = 3, padding = 1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        output = self.relu2(x)\n",
    "        return output\n",
    "\n",
    "class ConvNetwork_final(nn.Module):\n",
    "    def __init__(self, input_channel):\n",
    "        super().__init__()\n",
    "        self.conv2d_1 = nn.Conv2d(in_channels = input_channel, out_channels = 64, kernel_size=2)\n",
    "        self.leakyrelu_1 = nn.LeakyReLU()\n",
    "        self.maxpool2d_1 = nn.MaxPool2d(2)\n",
    "        self.conv2d_2 = nn.Conv2d(in_channels = 64, out_channels = 32, kernel_size=2)\n",
    "        self.leakyrelu_2 = nn.LeakyReLU()\n",
    "        self.maxpool2d_2 = nn.MaxPool2d(2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(224, 64)\n",
    "        self.leakyrelu_3 = nn.LeakyReLU()\n",
    "        self.batchnorm = nn.BatchNorm1d(64)\n",
    "        self.drop = nn.Dropout(p=0.25)\n",
    "        self.fc2 = nn.Linear(64, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv2d_1(x)\n",
    "        x = self.leakyrelu_1(x)\n",
    "        x = self.maxpool2d_1(x)\n",
    "        x = self.conv2d_2(x)\n",
    "        x = self.leakyrelu_2(x)\n",
    "        x = self.maxpool2d_2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.leakyrelu_3(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.drop(x)\n",
    "        output = self.fc2(x)  \n",
    "        return output\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorFusionMixer(nn.Module):\n",
    "    def __init__(self, ModelA, ModelB, ModelC, ModelD, ModelE):\n",
    "        super().__init__()\n",
    "        self.ModelA = ModelA\n",
    "        self.ModelB = ModelB\n",
    "        self.ModelC = ModelC\n",
    "        self.ModelD = ModelD\n",
    "        self.Model_cnn_final = ModelE\n",
    "        # self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def tensor_fusion(self, batch_arr1, batch_arr2, batch_arr3):\n",
    "        fusion_matrix_lst = []\n",
    "        for i, (arr1, arr2, arr3) in enumerate(zip(batch_arr1, batch_arr2, batch_arr3)):\n",
    "            arr1 = arr1.unsqueeze(-1).unsqueeze(-1)\n",
    "            arr2 = arr2.unsqueeze(0).unsqueeze(-1)\n",
    "            arr3 = arr3.squeeze().unsqueeze(0).unsqueeze(0)\n",
    "            \n",
    "            # outer_matrix = torch.einsum('i,j,kp->ijk', arr1, arr2, arr3)\n",
    "            kron_matrix = torch.kron(torch.kron(arr1,arr2), arr3)\n",
    "            l, w, d = kron_matrix.shape\n",
    "            \n",
    "            kron_matrix = kron_matrix.view(-1, l, w, d)\n",
    "            fusion_matrix_lst.append(kron_matrix)\n",
    "            \n",
    "        fusion_matrix = torch.concat(fusion_matrix_lst)\n",
    "        # fusion_matrix = fusion_matrix.unsqueeze(-1)\n",
    "        \n",
    "        return fusion_matrix\n",
    "        \n",
    "    def forward(self, x1, x2, x3, x4):\n",
    "        x1 = self.ModelA(x1)\n",
    "        x2 = self.ModelB(x2)\n",
    "        x3 = self.ModelC(x3)\n",
    "        x4 = self.ModelD(x4)\n",
    "        \n",
    "        x5 = torch.cat([x3,x4], dim=0)\n",
    "        fusion_matrix = self.tensor_fusion(x1, x2, x5)\n",
    "        \n",
    "        output = self.Model_cnn_final(fusion_matrix) \n",
    "        return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFusionMixer(\n",
      "  (ModelA): MLPNetwork_pre(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc1): Linear(in_features=61440, out_features=768, bias=True)\n",
      "    (gelu1): GELU(approximate='none')\n",
      "    (bn1): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (fc2): Linear(in_features=768, out_features=512, bias=True)\n",
      "    (gelu2): GELU(approximate='none')\n",
      "    (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (fc3): Linear(in_features=512, out_features=32, bias=True)\n",
      "    (gelu3): GELU(approximate='none')\n",
      "  )\n",
      "  (ModelB): MLPNetwork_pre(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc1): Linear(in_features=152576, out_features=768, bias=True)\n",
      "    (gelu1): GELU(approximate='none')\n",
      "    (bn1): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (fc2): Linear(in_features=768, out_features=512, bias=True)\n",
      "    (gelu2): GELU(approximate='none')\n",
      "    (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (fc3): Linear(in_features=512, out_features=32, bias=True)\n",
      "    (gelu3): GELU(approximate='none')\n",
      "  )\n",
      "  (ModelC): ConvNetwork_pre(\n",
      "    (conv1): Conv1d(50, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (relu1): ReLU()\n",
      "    (conv2): Conv1d(32, 10, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (relu2): ReLU()\n",
      "  )\n",
      "  (ModelD): ConvNetwork_pre(\n",
      "    (conv1): Conv1d(50, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (relu1): ReLU()\n",
      "    (conv2): Conv1d(32, 10, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (relu2): ReLU()\n",
      "  )\n",
      "  (Model_cnn_final): ConvNetwork_final(\n",
      "    (conv2d_1): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (leakyrelu_1): LeakyReLU(negative_slope=0.01)\n",
      "    (maxpool2d_1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv2d_2): Conv2d(64, 32, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (leakyrelu_2): LeakyReLU(negative_slope=0.01)\n",
      "    (maxpool2d_2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc1): Linear(in_features=224, out_features=64, bias=True)\n",
      "    (leakyrelu_3): LeakyReLU(negative_slope=0.01)\n",
      "    (batchnorm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (drop): Dropout(p=0.25, inplace=False)\n",
      "    (fc2): Linear(in_features=64, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/40toez/project/kyungho/multi_modal/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "# txt_input_length, txt_input_width = raw_dataset[session]['text_embeddings'][0].shape | 마지막엔 지울 것\n",
    "# _, wav_input_length, wav_input_width = raw_dataset[session]['wav_embeddings'][0].shape\n",
    "txt_input_length, txt_input_width = dataset.text_embeddings[0].shape\n",
    "wav_input_length, wav_input_width = dataset.wav_embeddings[0].shape\n",
    "temp_input_length = dataset.temp[0].shape[0]\n",
    "eda_input_length = dataset.eda[0].shape[0]\n",
    "\n",
    "# tf_mixer에 들어갈 wav mlp, txt mlp 선언\n",
    "model_mlp_txt = MLPNetwork_pre(txt_input_length,txt_input_width).to(device)\n",
    "model_mlp_wav = MLPNetwork_pre(wav_input_length,wav_input_width).to(device)\n",
    "model_conv_temp = ConvNetwork_pre(temp_input_length).to(device)\n",
    "model_conv_eda = ConvNetwork_pre(eda_input_length).to(device)\n",
    "\n",
    "model_cnn_final = ConvNetwork_final(32).to(device)\n",
    "\n",
    "# 최종 모델 선언\n",
    "model_tf_cnn_mixer = TensorFusionMixer(ModelA = model_mlp_txt, \n",
    "                                   ModelB = model_mlp_wav,\n",
    "                                   ModelC = model_conv_temp,\n",
    "                                   ModelD = model_conv_eda,\n",
    "                                   ModelE = model_cnn_final).to(device)\n",
    "\n",
    "# model 병렬 학습 처리\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model_mlp_txt = nn.DataParallel(model_mlp_txt).to(device)\n",
    "    model_mlp_wav = nn.DataParallel(model_mlp_wav).to(device)\n",
    "    model_conv_temp = nn.DataParallel(model_conv_temp).to(device)\n",
    "    model_conv_eda = nn.DataParallel(model_conv_eda).to(device)\n",
    "    model_tf_cnn_mixer = nn.DataParallel(model_tf_cnn_mixer).to(device)\n",
    "    \n",
    "print(model_tf_cnn_mixer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CCC, self).__init__()\n",
    "        self.mean = torch.mean\n",
    "        self.var = torch.var\n",
    "        self.sum = torch.sum\n",
    "        self.sqrt = torch.sqrt\n",
    "        self.std = torch.std\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        mean_gt = self.mean (target, 0)\n",
    "        mean_pred = self.mean (pred, 0)\n",
    "        var_gt = self.var (target, 0)\n",
    "        var_pred = self.var (pred, 0)\n",
    "        v_pred = pred - mean_pred\n",
    "        v_gt = target - mean_gt\n",
    "        cor = self.sum (v_pred * v_gt) / (self.sqrt(self.sum(v_pred ** 2)) * self.sqrt(self.sum(v_gt ** 2)))\n",
    "        sd_gt = self.std(target)\n",
    "        sd_pred = self.std(pred)\n",
    "        numerator = 2 * cor * sd_gt * sd_pred\n",
    "        denominator = var_gt + var_pred + (mean_gt-mean_pred) ** 2\n",
    "        ccc = numerator / denominator\n",
    "        return ccc\n",
    "    \n",
    "ccc = CCC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCCLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self,inputs,targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        # pred_v = inputs[:,0]\n",
    "        # pred_a = inputs[:,1]\n",
    "        # y_v  = targets[:,0]\n",
    "        # y_a  = targets[:,1]\n",
    "\n",
    "        return 1 - ccc(self.inputs, self.targets )\n",
    "    \n",
    "cccl = CCCLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0013, -0.0014, -0.0013, -0.0013, -0.0013, -0.0014, -0.0013, -0.0012,\n",
       "        -0.0013, -0.0014, -0.0013, -0.0013, -0.0013, -0.0012, -0.0012, -0.0012,\n",
       "        -0.0014, -0.0012, -0.0013, -0.0013])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (256,20)\n",
    "p = torch.rand(shape)\n",
    "q = torch.rand(shape)\n",
    "\n",
    "b = ccc(p, q)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0599)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (256,2)\n",
    "p = torch.rand(shape)\n",
    "q = torch.rand(shape)\n",
    "\n",
    "a = cccl(p,q)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m shape \u001b[39m=\u001b[39m (\u001b[39m256\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m a \u001b[39m=\u001b[39m cccl(torch\u001b[39m.\u001b[39;49mrand(shape), torch\u001b[39m.\u001b[39;49mrand(shape))\n\u001b[1;32m      3\u001b[0m a\u001b[39m.\u001b[39mshape,a\n",
      "File \u001b[0;32m~/project/kyungho/multi_modal/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[90], line 7\u001b[0m, in \u001b[0;36mCCCLoss.forward\u001b[0;34m(self, inputs, targets)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,inputs,targets):\n\u001b[0;32m----> 7\u001b[0m     pred_v \u001b[39m=\u001b[39m inputs[:,\u001b[39m0\u001b[39;49m]\n\u001b[1;32m      8\u001b[0m     pred_a \u001b[39m=\u001b[39m inputs[:,\u001b[39m1\u001b[39m]\n\u001b[1;32m      9\u001b[0m     y_v  \u001b[39m=\u001b[39m targets[:,\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "shape = (256)\n",
    "a = cccl(torch.rand(shape), torch.rand(shape))\n",
    "a.shape,a"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습을 위한 train, test method 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)    \n",
    "    # data 순서: text_embeddings, wav_embeddings, temp, eda, label_emotion, label_emotion_ext, label_arousal, label_valence\n",
    "\n",
    "    for batch, (X_txt, X_wav, X_temp, X_eda, \n",
    "                    label_emotion, label_emotion_ext, label_arousal, label_valence) in enumerate(dataloader): \n",
    "\n",
    "        # 예측 오류 계산 \n",
    "        X_txt, X_wav, X_temp, X_eda, y_v, y_a= X_txt.to(device), X_wav.to(device), X_temp.to(device), X_eda.to(device),label_valence.type(torch.float32).to(device), label_arousal.type(torch.float32).to(device)\n",
    "        \n",
    "        X_temp = X_temp.unsqueeze(dim=-1)\n",
    "        X_eda = X_eda.unsqueeze(dim=-1)\n",
    "        y_v = y_v.unsqueeze(dim=-1)\n",
    "        y_a = y_a.unsqueeze(dim=-1)\n",
    "\n",
    "        pred = model(X_txt, X_wav, X_temp, X_eda)\n",
    "    \n",
    "        # pred_v = pred[:,0].unsqueeze(dim=-1)\n",
    "        # pred_a = pred[:,1].unsqueeze(dim=-1)\n",
    "        \n",
    "        y = torch.concat([y_v, y_a], dim=-1)\n",
    "\n",
    "        \n",
    "        # # loss_a = loss_fn(pred_a, y_a)\n",
    "        # # loss_v = loss_fn(pred_v, y_v)\n",
    "        # loss = loss_fn(pred, y)\n",
    "        \n",
    "        # pred_ccc_v= ccc(pred_v, y_v)\n",
    "        # pred_ccc_a = ccc(pred_a, y_a)\n",
    "        \n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        \n",
    "        # ccc_mean = (pred_ccc_a + pred_ccc_v) / 2\n",
    "\n",
    "        # 역전파\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # loss_a.backward(retain_graph = True)\n",
    "        # loss_v.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X_txt)\n",
    "            # loss_a, loss_v, current = loss_a.item(), loss_v.item(), batch * len(X_txt)\n",
    "            # print(f\"loss_a: {loss_a:>7f}, loss_v: {loss_v:>7f}\")\n",
    "            print(f\"loss: {loss}\")\n",
    "            # print(f\"ccc_mean : {ccc_mean.item():>9f}, Arousal_ccc : {pred_ccc_a.item():>9f}, Valence_ccc : {pred_ccc_v.item():>9f}\".format())\n",
    "        \n",
    "    # wandb.log({'loss':loss,'ccc_mean':ccc_mean.item(),'ccc_valence' : pred_ccc_v.item(),'ccc_arousal' : pred_ccc_a.item()})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn, mode = 'test'):\n",
    "    \n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    # test_loss_a = 0\n",
    "    # test_loss_v = 0\n",
    "    ccc_mean = 0\n",
    "    ccc_a = 0\n",
    "    ccc_v = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # data 순서: text_embeddings, wav_embeddings, temp, eda, label_emotion, label_emotion_ext, label_arousal, label_valence\n",
    "        for batch, (X_txt, X_wav, X_temp, X_eda, \n",
    "                    label_emotion, label_emotion_ext, label_arousal, label_valence) in enumerate(dataloader): \n",
    "\n",
    "            # 예측 오류 계산 \n",
    "            X_txt, X_wav, X_temp, X_eda, y_v, y_a= X_txt.to(device), X_wav.to(device), X_temp.to(device), X_eda.to(device),label_valence.type(torch.float32).to(device), label_arousal.type(torch.float32).to(device)\n",
    "            \n",
    "            X_temp = X_temp.unsqueeze(dim=-1)\n",
    "            X_eda = X_eda.unsqueeze(dim=-1)\n",
    "            y_v = y_v.unsqueeze(dim=-1)\n",
    "            y_a = y_a.unsqueeze(dim=-1)\n",
    "\n",
    "            pred = model(X_txt, X_wav, X_temp, X_eda)\n",
    "            \n",
    "            # pred_v = pred[:,0].unsqueeze(dim=-1)\n",
    "            # pred_a = pred[:,1].unsqueeze(dim=-1)\n",
    "            y = torch.concat([y_v, y_a], dim=-1)\n",
    "\n",
    "            \n",
    "            loss = loss_fn(pred, y)\n",
    "            test_loss += loss\n",
    "            \n",
    "            # loss_a = loss_fn(pred_a, y_a)\n",
    "            # loss_v = loss_fn(pred_v, y_v)\n",
    "\n",
    "            # test_loss_a += loss_a\n",
    "            # test_loss_v += loss_v\n",
    "\n",
    "            # pred_ccc_a = ccc(pred_a, y_a)\n",
    "            # pred_ccc_v = ccc(pred_v, y_v)\n",
    "            \n",
    "            # ccc_a += pred_ccc_a\n",
    "            # ccc_v += pred_ccc_v\n",
    "      \n",
    "      \n",
    "    test_loss /= num_batches  \n",
    "    # test_loss_v /= num_batches    \n",
    "    # test_loss_a /= num_batches\n",
    "    \n",
    "    # ccc_v /= num_batches\n",
    "    # ccc_a /= num_batches\n",
    "    # ccc_mean = (ccc_a + ccc_v) / 2\n",
    "\n",
    "    \n",
    "    if mode == 'test':\n",
    "        # print(f\"loss_a: {loss_a:>7f}, loss_v: {loss_v:>7f}\")\n",
    "        print(f\"loss: {loss.item():>7f}\")\n",
    "        # print(f\"Test ccc_mean : {ccc_mean.item():>9f}, Arousal_ccc : {pred_ccc_a.item():>9f}, Valence_ccc : {pred_ccc_v.item():>9f}\")\n",
    "    elif mode == 'val':\n",
    "        # print(f\"loss_a: {loss_a:>7f}, loss_v: {loss_v:>7f}\")\n",
    "        print(f\"loss: {loss.item():>7f}\")\n",
    "        # print(f\"Val ccc_mean : {ccc_mean.item():>9f}, Arousal_ccc : {pred_ccc_a.item():>9f}, Valence_ccc : {pred_ccc_v.item():>9f}\")\n",
    "        # wandb.log({'loss':loss,'ccc_mean':ccc_mean.item(),'ccc_valence' : pred_ccc_v.item(),'ccc_arousal' : pred_ccc_a.item()})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 지난 저장한 모델이 있다면\n",
    "# PATH = './data/test_model.pkl'\n",
    "# model_tf_mixer = torch.load(PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start training mlp fusion mixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Training Parameters\n",
    "lr = 1e-3\n",
    "# criterion = nn.MSELoss()\n",
    "criterion = cccl\n",
    "optimizer = optim.Adagrad(model_tf_cnn_mixer.parameters(), lr=lr) # regression\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:zhi64fo6) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>ccc_arousal</td><td>▁▂▃▃▄▃▃▂▄▄▄▄▅▄▅▄▆▄▆▄▄▅▃▆▃▆▄▇▄▇▄▆▄▇▄▇▄█▄▄</td></tr><tr><td>ccc_mean</td><td>▁▂▃▂▃▃▃▃▃▃▄▃▅▃▅▃▅▄▆▄▃▆▄▆▄▇▄▆▄▇▄▇▄▇▄█▄█▄▄</td></tr><tr><td>ccc_valence</td><td>▁▃▂▂▃▂▃▂▃▂▄▂▅▃▅▄▅▃▆▄▃▆▄▆▃▇▃▆▃▇▃█▄▆▄█▄█▅▃</td></tr><tr><td>loss</td><td>█▆▅▄▄▅▄▅▄▄▄▅▃▄▂▃▃▄▂▄▅▂▄▃▆▁▄▂▅▂▅▂▄▃▄▂▄▁▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>ccc_arousal</td><td>0.21151</td></tr><tr><td>ccc_mean</td><td>0.20469</td></tr><tr><td>ccc_valence</td><td>0.11404</td></tr><tr><td>loss</td><td>0.45808</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lunar-galaxy-2</strong> at: <a href='https://wandb.ai/toez/ETRI_regression/runs/zhi64fo6' target=\"_blank\">https://wandb.ai/toez/ETRI_regression/runs/zhi64fo6</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230410_144120-zhi64fo6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:zhi64fo6). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/40toez/project/kyungho/multi_modal/wandb/run-20230410_172806-8yfi4zbg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/toez/ETRI_regression/runs/8yfi4zbg' target=\"_blank\">light-shape-3</a></strong> to <a href='https://wandb.ai/toez/ETRI_regression' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/toez/ETRI_regression' target=\"_blank\">https://wandb.ai/toez/ETRI_regression</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/toez/ETRI_regression/runs/8yfi4zbg' target=\"_blank\">https://wandb.ai/toez/ETRI_regression/runs/8yfi4zbg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"ETRI_regression\",\n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": lr,\n",
    "        \"epochs\": epochs,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Epoch 1----------------\n",
      "Training...............\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m---------------Epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m----------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTraining...............\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m train(train_dataloader, model_tf_cnn_mixer, criterion, optimizer)\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mValidation.............\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m test(validation_dataloader, model_tf_cnn_mixer, criterion, mode \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[28], line 38\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39m# ccc_mean = (pred_ccc_a + pred_ccc_v) / 2\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \n\u001b[1;32m     36\u001b[0m \u001b[39m# 역전파\u001b[39;00m\n\u001b[1;32m     37\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 38\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     39\u001b[0m \u001b[39m# loss_a.backward(retain_graph = True)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39m# loss_v.backward()\u001b[39;00m\n\u001b[1;32m     41\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/project/kyungho/multi_modal/.venv/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/project/kyungho/multi_modal/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:190\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    186\u001b[0m inputs \u001b[39m=\u001b[39m (inputs,) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(inputs, torch\u001b[39m.\u001b[39mTensor) \u001b[39melse\u001b[39;00m \\\n\u001b[1;32m    187\u001b[0m     \u001b[39mtuple\u001b[39m(inputs) \u001b[39mif\u001b[39;00m inputs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mtuple\u001b[39m()\n\u001b[1;32m    189\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[39mlen\u001b[39m(tensors))\n\u001b[0;32m--> 190\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _make_grads(tensors, grad_tensors_, is_grads_batched\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    191\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/project/kyungho/multi_modal/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:85\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mrequires_grad:\n\u001b[1;32m     84\u001b[0m     \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mnumel() \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 85\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m     new_grads\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mones_like(out, memory_format\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mpreserve_format))\n\u001b[1;32m     87\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(f\"---------------Epoch {epoch+1}----------------\")\n",
    "    print(\"Training...............\")\n",
    "    train(train_dataloader, model_tf_cnn_mixer, criterion, optimizer)\n",
    "    print(\"Validation.............\")\n",
    "    test(validation_dataloader, model_tf_cnn_mixer, criterion, mode = 'val')\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실험을 위해 모델 저장\n",
    "PATH = '.model/data/paradeigma_test_model_multilabel_CNN.pkl'\n",
    "torch.save(model_tf_cnn_mixer, PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic TensorFusionNet 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.510035\n",
      "Test ccc_mean :  0.231434, Arousal_ccc :  0.393748, Valence_ccc :  0.167040\n"
     ]
    }
   ],
   "source": [
    "test(test_dataloader, model_tf_cnn_mixer, criterion, mode = 'test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bdc1fd12ca460d5768d71e9df3d9063ef832ce64a62e55a1a523c8c99752868e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
