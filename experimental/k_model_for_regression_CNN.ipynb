{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# working model for tensorfusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu117\n"
     ]
    }
   ],
   "source": [
    "# 필요 라이브러리\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "from torchmetrics import F1Score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 80\n",
    "LR = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = \"/home/40toez/project/kyungho/multi_modal/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Annotation, embedded data 불러오기\n",
    "with open(ROOT + 'model/data/paradeigma_KEMDY20_annotation_nonmissing.pkl', 'rb') as f:\n",
    "    annotation_20_nonmissing = pickle.load(f)\n",
    "\n",
    "with open(ROOT + 'model/data/paradeigma_KEMDY19_annotation_nonmissing.pkl', 'rb') as f:\n",
    "    annotation_19_nonmissing = pickle.load(f)\n",
    "    \n",
    "with open(ROOT + 'model/data/paradeigma_KEMDY20_embedding_for_dataset.pkl', 'rb') as f:\n",
    "    embedding_20_dataset = pickle.load(f)\n",
    "\n",
    "with open(ROOT + 'model/data/paradeigma_KEMDY19_embedding_for_dataset.pkl', 'rb') as f:\n",
    "    embedding_19_dataset = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_71137/1231358673.py:3: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  raw_dataset = raw_dataset.append(annotation_19_nonmissing, ignore_index = True)\n",
      "/tmp/ipykernel_71137/1231358673.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  raw_dataset = raw_dataset.append(annotation_20_nonmissing, ignore_index = True)\n"
     ]
    }
   ],
   "source": [
    "# 19년도 20년도 병합\n",
    "raw_dataset = pd.DataFrame()\n",
    "raw_dataset = raw_dataset.append(annotation_19_nonmissing, ignore_index = True)\n",
    "raw_dataset = raw_dataset.append(annotation_20_nonmissing, ignore_index = True)\n",
    "merged_dataset = raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emotion 정수인코딩\n",
    "emotion_label = list(merged_dataset['Emotion'].unique())\n",
    "\n",
    "def encode_emotion(x):\n",
    "    return emotion_label.index(x)\n",
    "\n",
    "merged_dataset['Emotion'] = merged_dataset['Emotion'].apply(encode_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wav, txt data 분리\n",
    "merged_embeddings = {'wav':[], 'txt':[]}\n",
    "for session, embeddings in embedding_19_dataset[0].items():\n",
    "    merged_embeddings['wav'].append(embeddings)\n",
    "    \n",
    "for session, embeddings in embedding_19_dataset[1].items():\n",
    "    merged_embeddings['txt'].append(embeddings)\n",
    "    \n",
    "for session, embeddings in embedding_20_dataset[0].items():\n",
    "    merged_embeddings['wav'].append(embeddings)\n",
    "    \n",
    "for session, embeddings in embedding_20_dataset[1].items():\n",
    "    merged_embeddings['txt'].append(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wav txt 2020 + 2019\n",
    "\n",
    "merged_embeddings['wav'] = torch.concat(merged_embeddings['wav'])\n",
    "merged_embeddings['txt'] = torch.concat(merged_embeddings['txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([21723, 149, 1024]), torch.Size([21723, 80, 768]), 21723)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 확인\n",
    "merged_embeddings['wav'].shape,merged_embeddings['txt'].shape,len(merged_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding 함수\n",
    "def sequence_padding(ts_list, padding_length = 50, mode = 'constant'):\n",
    "    \n",
    "    padding_value=0\n",
    "    \n",
    "    if (type(ts_list) != type([])) :\n",
    "        ts_list = [padding_value] * padding_length\n",
    "    \n",
    "    elif len(ts_list) >= padding_length :\n",
    "        ts_list = ts_list[0:padding_length]\n",
    "    \n",
    "    elif mode == 'constant':\n",
    "        length = padding_length - len(ts_list)\n",
    "        extend_list = [padding_value] * length\n",
    "        ts_list = ts_list + extend_list    \n",
    "\n",
    "    elif mode == 'replicate':\n",
    "        \n",
    "        quotient = padding_length // len(ts_list)\n",
    "        remainder = padding_length % len(ts_list)\n",
    "        result = ts_list * quotient\n",
    "        result += ts_list[:remainder]\n",
    "        ts_list = result\n",
    "\n",
    "    return ts_list            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시계열 데이터 2020 + 2019\n",
    "merged_dataset['Scaled EDA'] = merged_dataset['Scaled EDA'].apply(sequence_padding)\n",
    "merged_dataset['Scaled TEMP'] = merged_dataset['Scaled TEMP'].apply(sequence_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stats.stackexchange.com/questions/107874/how-to-deal-with-a-skewed-class-in-binary-classification-having-many-features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch dataset 만들기\n",
    "- 참고: https://tutorials.pytorch.kr/beginner/basics/data_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EtriDataset(Dataset):\n",
    "    def __init__(self, file_names, \n",
    "                 text_embeddings, \n",
    "                 wav_embeddings, \n",
    "                 Temp,\n",
    "                 EDA,\n",
    "                 Emotion,\n",
    "                 Emotion_ext, \n",
    "                 Arousal, \n",
    "                 Valence):\n",
    "        self.file_names = file_names\n",
    "        self.text_embeddings = text_embeddings\n",
    "        self.wav_embeddings = wav_embeddings\n",
    "        self.temp = Temp\n",
    "        self.eda = EDA\n",
    "        self.label_emotion = Emotion\n",
    "        self.label_emotion_ext = Emotion_ext\n",
    "        self.label_arousal = Arousal\n",
    "        self.label_valence = Valence\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text_embeddings = self.text_embeddings[idx]\n",
    "        wav_embeddings = self.wav_embeddings[idx]\n",
    "        temp = self.temp[idx]\n",
    "        eda = self.eda[idx]\n",
    "        label_emotion = self.label_emotion[idx]\n",
    "        label_emotion_ext = self.label_emotion_ext[idx]\n",
    "        label_arousal = self.label_arousal[idx]\n",
    "        label_valence = self.label_valence[idx]\n",
    "        return text_embeddings, wav_embeddings, temp, eda, label_emotion, label_emotion_ext, label_arousal, label_valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Segment ID', 'Emotion', 'Valence', 'Arousal', 'emotion_vector',\n",
       "       'valence_vector', 'arousal_vector', 'EDA', 'TEMP', 'EDA length',\n",
       "       'TEMP length', 'Scaled EDA', 'Scaled TEMP'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data load 및 나누기: https://076923.github.io/posts/Python-pytorch-11/\n",
    "\n",
    "# session을 통합시킨 데이터 셋을 만들었을 때\n",
    "dataset = EtriDataset(file_names = merged_dataset['Segment ID'],\n",
    "                      text_embeddings = merged_embeddings['txt'],\n",
    "                      wav_embeddings = merged_embeddings['wav'],\n",
    "                      Emotion = merged_dataset['Emotion'],\n",
    "                      Arousal = merged_dataset['Arousal'],\n",
    "                      Valence = merged_dataset['Valence'],\n",
    "                      EDA = torch.concat(list(merged_dataset['Scaled EDA'].apply(lambda x: torch.tensor(x).view(1,-1)))), ## 일부 세션 데이터만 사용하므로, 길이를 맞춰주기 위해 일부 slicing함\n",
    "                      Temp = torch.concat(list(merged_dataset['Scaled TEMP'].apply(lambda x: torch.tensor(x).view(1,-1)))), ## 일부 세션 데이터만 사용하므로, 길이를 맞춰주기 위해 일부 slicing함\n",
    "                      Emotion_ext = torch.concat(list(merged_dataset['emotion_vector'].apply(lambda x: torch.tensor(x).view(1,-1))))) ## 일부 세션 데이터만 사용하므로, 길이를 맞춰주기 위해 일부 slicing함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15206 3259 3258\n",
      "Training Data Size : 15206\n",
      "Validation Data Size : 3258\n",
      "Testing Data Size : 3259\n"
     ]
    }
   ],
   "source": [
    "dataset_size = len(dataset)\n",
    "train_size = int(dataset_size * 0.7)\n",
    "validation_size = int(dataset_size * 0.15)\n",
    "test_size = dataset_size - train_size - validation_size\n",
    "\n",
    "train_dataset, validation_dataset, test_dataset = random_split(dataset, [train_size, validation_size, test_size])\n",
    "\n",
    "print(train_size, test_size, validation_size)\n",
    "print(f\"Training Data Size : {len(train_dataset)}\")\n",
    "print(f\"Validation Data Size : {len(validation_dataset)}\")\n",
    "print(f\"Testing Data Size : {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([149, 1024]), torch.Size([33]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data size\n",
    "merged_embeddings['wav'][0].shape , torch.Tensor(merged_dataset['EDA'][0]).shape\n",
    "# raw_dataset[session]['wav_embeddings'][0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NetWork 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# print(f\"Using {device} device\")\n",
    "\n",
    "# CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)\n",
    "# 위의 오류가 해결되기 전까진 일단 cpu를 가지고 모델을 돌리기로 한다. \n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNetwork_pre(nn.Module):\n",
    "    def __init__(self, input_length, input_width):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_length*input_width, 768)\n",
    "        self.gelu1 = nn.GELU()\n",
    "        self.bn1 = nn.BatchNorm1d(768)\n",
    "        self.fc2 = nn.Linear(768, 512)\n",
    "        self.gelu2 = nn.GELU()\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.fc3 = nn.Linear(512, 32)\n",
    "        self.gelu3 = nn.GELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.gelu1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.gelu2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.fc3(x)\n",
    "        output = self.gelu3(x)\n",
    "        return output\n",
    "    \n",
    "class ConvNetwork_pre(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels = 1, out_channels= 1, kernel_size = 16)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(in_channels = 1, out_channels = 1, kernel_size = 16)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.transpose(x, 1,2)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        output = self.relu2(x)\n",
    "        return output    \n",
    "    \n",
    "class ConvNetwork_middle(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=2, out_channels=1, kernel_size=11)\n",
    "        self.relu1 = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        output = self.relu1(x)    \n",
    "        return output.squeeze()\n",
    "    \n",
    "class ConvNetwork_final(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "        self.conv2d_1 = nn.Conv2d(in_channels = 10, out_channels = 64, kernel_size=2)\n",
    "        self.leakyrelu_1 = nn.LeakyReLU()\n",
    "        self.maxpool2d_1 = nn.MaxPool2d(2)\n",
    "        self.conv2d_2 = nn.Conv2d(in_channels = 64, out_channels = 32, kernel_size=2)\n",
    "        self.leakyrelu_2 = nn.LeakyReLU()\n",
    "        self.maxpool2d_2 = nn.MaxPool2d(2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(1568, 64)\n",
    "        self.leakyrelu_3 = nn.LeakyReLU()\n",
    "        self.batchnorm = nn.BatchNorm1d(64)\n",
    "        self.drop = nn.Dropout(p=0.25)\n",
    "        self.fc2 = nn.Linear(64, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv2d_1(x)\n",
    "        x = self.leakyrelu_1(x)\n",
    "        x = self.maxpool2d_1(x)\n",
    "        x = self.conv2d_2(x)\n",
    "        x = self.leakyrelu_2(x)\n",
    "        x = self.maxpool2d_2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.leakyrelu_3(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.drop(x)\n",
    "        output = self.fc2(x)  \n",
    "        return output\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorFusionMixer(nn.Module):\n",
    "    def __init__(self, ModelA, ModelB, ModelC, ModelD, ModelE, ModelF):\n",
    "        super().__init__()\n",
    "        self.ModelA = ModelA\n",
    "        self.ModelB = ModelB\n",
    "        self.ModelC = ModelC\n",
    "        self.ModelD = ModelD\n",
    "        self.ModelE = ModelE\n",
    "        self.Model_cnn_final = ModelF\n",
    "        # self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def tensor_fusion(self, batch_arr1, batch_arr2, batch_arr3):\n",
    "        fusion_matrix_lst = []\n",
    "        for i, (arr1, arr2, arr3) in enumerate(zip(batch_arr1, batch_arr2, batch_arr3)):\n",
    "            # arr1 = arr1.unsqueeze(-1).unsqueeze(-1)\n",
    "            # arr2 = arr2.unsqueeze(0).unsqueeze(-1)\n",
    "            # arr3 = arr3.squeeze().unsqueeze(0).unsqueeze(0)\n",
    "            \n",
    "            arr1 = arr1.unsqueeze(0).unsqueeze(0)\n",
    "            arr2 = arr2.unsqueeze(0).unsqueeze(-1)\n",
    "            arr3 = arr3.unsqueeze(-1).unsqueeze(-1)\n",
    "            \n",
    "            # outer_matrix = torch.einsum('i,j,kp->ijk', arr1, arr2, arr3)\n",
    "            kron_matrix = torch.kron(arr3, torch.kron(arr2, arr1,))\n",
    "            l, w, d = kron_matrix.shape\n",
    "            \n",
    "            kron_matrix = kron_matrix.view(-1, l, w, d)\n",
    "            fusion_matrix_lst.append(kron_matrix)\n",
    "            \n",
    "        fusion_matrix = torch.concat(fusion_matrix_lst)\n",
    "        # fusion_matrix = fusion_matrix.unsqueeze(-1)\n",
    "        \n",
    "        return fusion_matrix\n",
    "        \n",
    "    def forward(self, x1, x2, x3, x4):\n",
    "        x1 = self.ModelA(x1)\n",
    "        x2 = self.ModelB(x2)\n",
    "        x3 = self.ModelC(x3)\n",
    "        x4 = self.ModelD(x4)\n",
    "        x5 = torch.concat([x3, x4], dim=1)\n",
    "        x5 = self.ModelE(x5)\n",
    "        fusion_matrix = self.tensor_fusion(x1, x2, x5)\n",
    "        # x5 = torch.cat([x3,x4], dim=0)\n",
    "        output = self.Model_cnn_final(fusion_matrix) \n",
    "        return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFusionMixer(\n",
      "  (ModelA): MLPNetwork_pre(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc1): Linear(in_features=61440, out_features=768, bias=True)\n",
      "    (gelu1): GELU(approximate='none')\n",
      "    (bn1): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (fc2): Linear(in_features=768, out_features=512, bias=True)\n",
      "    (gelu2): GELU(approximate='none')\n",
      "    (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (fc3): Linear(in_features=512, out_features=32, bias=True)\n",
      "    (gelu3): GELU(approximate='none')\n",
      "  )\n",
      "  (ModelB): MLPNetwork_pre(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc1): Linear(in_features=152576, out_features=768, bias=True)\n",
      "    (gelu1): GELU(approximate='none')\n",
      "    (bn1): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (fc2): Linear(in_features=768, out_features=512, bias=True)\n",
      "    (gelu2): GELU(approximate='none')\n",
      "    (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (fc3): Linear(in_features=512, out_features=32, bias=True)\n",
      "    (gelu3): GELU(approximate='none')\n",
      "  )\n",
      "  (ModelC): ConvNetwork_pre(\n",
      "    (conv1): Conv1d(1, 1, kernel_size=(16,), stride=(1,))\n",
      "    (relu1): ReLU()\n",
      "    (conv2): Conv1d(1, 1, kernel_size=(16,), stride=(1,))\n",
      "    (relu2): ReLU()\n",
      "  )\n",
      "  (ModelD): ConvNetwork_pre(\n",
      "    (conv1): Conv1d(1, 1, kernel_size=(16,), stride=(1,))\n",
      "    (relu1): ReLU()\n",
      "    (conv2): Conv1d(1, 1, kernel_size=(16,), stride=(1,))\n",
      "    (relu2): ReLU()\n",
      "  )\n",
      "  (ModelE): ConvNetwork_middle(\n",
      "    (conv1): Conv1d(2, 1, kernel_size=(11,), stride=(1,))\n",
      "    (relu1): ReLU()\n",
      "  )\n",
      "  (Model_cnn_final): ConvNetwork_final(\n",
      "    (conv2d_1): Conv2d(10, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (leakyrelu_1): LeakyReLU(negative_slope=0.01)\n",
      "    (maxpool2d_1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv2d_2): Conv2d(64, 32, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (leakyrelu_2): LeakyReLU(negative_slope=0.01)\n",
      "    (maxpool2d_2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc1): Linear(in_features=1568, out_features=64, bias=True)\n",
      "    (leakyrelu_3): LeakyReLU(negative_slope=0.01)\n",
      "    (batchnorm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (drop): Dropout(p=0.25, inplace=False)\n",
      "    (fc2): Linear(in_features=64, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# txt_input_length, txt_input_width = raw_dataset[session]['text_embeddings'][0].shape | 마지막엔 지울 것\n",
    "# _, wav_input_length, wav_input_width = raw_dataset[session]['wav_embeddings'][0].shape\n",
    "txt_input_length, txt_input_width = dataset.text_embeddings[0].shape\n",
    "wav_input_length, wav_input_width = dataset.wav_embeddings[0].shape\n",
    "temp_input_length = dataset.temp[0].shape[0]\n",
    "eda_input_length = dataset.eda[0].shape[0]\n",
    "\n",
    "# tf_mixer에 들어갈 wav mlp, txt mlp 선언\n",
    "model_mlp_txt = MLPNetwork_pre(txt_input_length,txt_input_width).to(device)\n",
    "model_mlp_wav = MLPNetwork_pre(wav_input_length,wav_input_width).to(device)\n",
    "# model_conv_temp = ConvNetwork_pre(temp_input_length).to(device)\n",
    "model_conv_temp = ConvNetwork_pre().to(device)\n",
    "model_conv_eda = ConvNetwork_pre().to(device)\n",
    "# model_conv_eda = ConvNetwork_pre(eda_input_length).to(device)\n",
    "model_conv_middle = ConvNetwork_middle().to(device)\n",
    "# model_cnn_final = ConvNetwork_final(32).to(device)\n",
    "model_cnn_final = ConvNetwork_final().to(device)\n",
    "# 최종 모델 선언\n",
    "model_tf_cnn_mixer = TensorFusionMixer(ModelA = model_mlp_txt, \n",
    "                                   ModelB = model_mlp_wav,\n",
    "                                   ModelC = model_conv_temp,\n",
    "                                   ModelD = model_conv_eda,\n",
    "                                   ModelE = model_conv_middle,\n",
    "                                   ModelF = model_cnn_final\n",
    "                                   ).to(device)\n",
    "\n",
    "# model 병렬 학습 처리\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "#     model_mlp_txt = nn.DataParallel(model_mlp_txt).to(device)\n",
    "#     model_mlp_wav = nn.DataParallel(model_mlp_wav).to(device)\n",
    "#     model_conv_temp = nn.DataParallel(model_conv_temp).to(device)\n",
    "#     model_conv_eda = nn.DataParallel(model_conv_eda).to(device)\n",
    "#     model_tf_cnn_mixer = nn.DataParallel(model_tf_cnn_mixer).to(device)\n",
    "    \n",
    "print(model_tf_cnn_mixer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def covariance(x, y):\n",
    "    x_mean = torch.mean(x)\n",
    "    y_mean = torch.mean(y)\n",
    "    return torch.mean((x - x_mean) * (y - y_mean))\n",
    "\n",
    "def ccc(arousal_valence_A, arousal_valence_B):\n",
    "    # Calculate means\n",
    "    mean_A = torch.mean(arousal_valence_A, axis=0)\n",
    "    mean_B = torch.mean(arousal_valence_B, axis=0)\n",
    "\n",
    "    # Calculate variances\n",
    "    var_A = torch.var(arousal_valence_A, axis=0, unbiased=True)\n",
    "    var_B = torch.var(arousal_valence_B, axis=0, unbiased=True)\n",
    "\n",
    "    # Calculate covariance\n",
    "    cov_arousal = covariance(arousal_valence_A[:, 0], arousal_valence_B[:, 0])\n",
    "    cov_valence = covariance(arousal_valence_A[:, 1], arousal_valence_B[:, 1])\n",
    "\n",
    "    # Calculate CCC for arousal and valence components\n",
    "    ccc_arousal = (2 * cov_arousal) / (var_A[0] + var_B[0] + (mean_A[0] - mean_B[0]) ** 2)\n",
    "    ccc_valence = (2 * cov_valence) / (var_A[1] + var_B[1] + (mean_A[1] - mean_B[1]) ** 2)\n",
    "\n",
    "    # Average the CCCs for arousal and valence components\n",
    "    ccc = (ccc_arousal + ccc_valence) / 2\n",
    "\n",
    "    return ccc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCCLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self,inputs,targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        # pred_v = inputs[:,0]\n",
    "        # pred_a = inputs[:,1]\n",
    "        # y_v  = targets[:,0]\n",
    "        # y_a  = targets[:,1]\n",
    "\n",
    "        return 1 - ccc(self.inputs, self.targets )\n",
    "    \n",
    "cccl = CCCLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습을 위한 train, test method 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)    \n",
    "    # data 순서: text_embeddings, wav_embeddings, temp, eda, label_emotion, label_emotion_ext, label_arousal, label_valence\n",
    "    return_loss = 0\n",
    "    for batch, (X_txt, X_wav, X_temp, X_eda, \n",
    "                    label_emotion, label_emotion_ext, label_arousal, label_valence) in enumerate(dataloader): \n",
    "\n",
    "        # 예측 오류 계산 \n",
    "        X_txt, X_wav, X_temp, X_eda, y_v, y_a= X_txt.to(device), X_wav.to(device), X_temp.to(device), X_eda.to(device),label_valence.type(torch.float32).to(device), label_arousal.type(torch.float32).to(device)\n",
    "        \n",
    "        X_temp = X_temp.unsqueeze(dim=-1)\n",
    "        X_eda = X_eda.unsqueeze(dim=-1)\n",
    "        y_v = y_v.unsqueeze(dim=-1)\n",
    "        y_a = y_a.unsqueeze(dim=-1)\n",
    "        \n",
    "        pred = model(X_txt, X_wav, X_temp, X_eda)\n",
    "    \n",
    "        # pred_v = pred[:,0].unsqueeze(dim=-1)\n",
    "        # pred_a = pred[:,1].unsqueeze(dim=-1)\n",
    "        \n",
    "        y = torch.concat([y_v, y_a], dim=-1)\n",
    "\n",
    "        \n",
    "        # # loss_a = loss_fn(pred_a, y_a)\n",
    "        # # loss_v = loss_fn(pred_v, y_v)\n",
    "        # loss = loss_fn(pred, y)\n",
    "        \n",
    "        # pred_ccc_v= ccc(pred_v, y_v)\n",
    "        # pred_ccc_a = ccc(pred_a, y_a)\n",
    "        \n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # ccc_mean = (pred_ccc_a + pred_ccc_v) / 2\n",
    "        \n",
    "\n",
    "        # 역전파\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # loss_a.backward(retain_graph = True)\n",
    "        # loss_v.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X_txt)\n",
    "            # loss_a, loss_v, current = loss_a.item(), loss_v.item(), batch * len(X_txt)\n",
    "            # print(f\"loss_a: {loss_a:>7f}, loss_v: {loss_v:>7f}\")\n",
    "            print(f\"loss: {loss}\")\n",
    "            # print(f\"ccc_mean : {ccc_mean.item():>9f}, Arousal_ccc : {pred_ccc_a.item():>9f}, Valence_ccc : {pred_ccc_v.item():>9f}\".format())\n",
    "            return_loss += loss\n",
    "    # wandb.log({'loss':loss,'ccc_mean':ccc_mean.item(),'ccc_valence' : pred_ccc_v.item(),'ccc_arousal' : pred_ccc_a.item()})  \n",
    "    \n",
    "    return return_loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn, mode = 'test'):\n",
    "    \n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    # test_loss_a = 0\n",
    "    # test_loss_v = 0\n",
    "    # ccc_mean = 0\n",
    "    # ccc_a = 0\n",
    "    # ccc_v = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # data 순서: text_embeddings, wav_embeddings, temp, eda, label_emotion, label_emotion_ext, label_arousal, label_valence\n",
    "        for batch, (X_txt, X_wav, X_temp, X_eda, \n",
    "                    label_emotion, label_emotion_ext, label_arousal, label_valence) in enumerate(dataloader): \n",
    "\n",
    "            # 예측 오류 계산 \n",
    "            X_txt, X_wav, X_temp, X_eda, y_v, y_a= X_txt.to(device), X_wav.to(device), X_temp.to(device), X_eda.to(device),label_valence.type(torch.float32).to(device), label_arousal.type(torch.float32).to(device)\n",
    "            \n",
    "            X_temp = X_temp.unsqueeze(dim=-1)\n",
    "            X_eda = X_eda.unsqueeze(dim=-1)\n",
    "            y_v = y_v.unsqueeze(dim=-1)\n",
    "            y_a = y_a.unsqueeze(dim=-1)\n",
    "\n",
    "            pred = model(X_txt, X_wav, X_temp, X_eda)\n",
    "            \n",
    "            # pred_v = pred[:,0].unsqueeze(dim=-1)\n",
    "            # pred_a = pred[:,1].unsqueeze(dim=-1)\n",
    "            y = torch.concat([y_v, y_a], dim=-1)\n",
    "\n",
    "            \n",
    "            loss = loss_fn(pred, y)\n",
    "            test_loss += loss\n",
    "            \n",
    "            # loss_a = loss_fn(pred_a, y_a)\n",
    "            # loss_v = loss_fn(pred_v, y_v)\n",
    "\n",
    "            # test_loss_a += loss_a\n",
    "            # test_loss_v += loss_v\n",
    "\n",
    "            # pred_ccc_a = ccc(pred_a, y_a)\n",
    "            # pred_ccc_v = ccc(pred_v, y_v)\n",
    "            \n",
    "            # ccc_a += pred_ccc_a\n",
    "            # ccc_v += pred_ccc_v\n",
    "      \n",
    "      \n",
    "    test_loss /= num_batches  \n",
    "    # test_loss_v /= num_batches    \n",
    "    # test_loss_a /= num_batches\n",
    "    \n",
    "    # ccc_v /= num_batches\n",
    "    # ccc_a /= num_batches\n",
    "    # ccc_mean = (ccc_a + ccc_v) / 2\n",
    "\n",
    "    \n",
    "    if mode == 'test':\n",
    "        # print(f\"loss_a: {loss_a:>7f}, loss_v: {loss_v:>7f}\")\n",
    "        print(f\"loss: {test_loss:>7f}\")\n",
    "        # print(f\"Test ccc_mean : {ccc_mean.item():>9f}, Arousal_ccc : {pred_ccc_a.item():>9f}, Valence_ccc : {pred_ccc_v.item():>9f}\")\n",
    "    elif mode == 'val':\n",
    "        # print(f\"loss_a: {loss_a:>7f}, loss_v: {loss_v:>7f}\")\n",
    "        print(f\"loss: {test_loss:>7f}\")\n",
    "        # print(f\"Val ccc_mean : {ccc_mean.item():>9f}, Arousal_ccc : {pred_ccc_a.item():>9f}, Valence_ccc : {pred_ccc_v.item():>9f}\")\n",
    "        # wandb.log({'loss':loss,'ccc_mean':ccc_mean.item(),'ccc_valence' : pred_ccc_v.item(),'ccc_arousal' : pred_ccc_a.item()})\n",
    "    return test_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 지난 저장한 모델이 있다면\n",
    "# PATH = './data/test_model.pkl'\n",
    "# model_tf_mixer = torch.load(PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start training mlp fusion mixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = CCCLoss() # nn.MSELoss()\n",
    "optimizer = optim.Adagrad(model_tf_cnn_mixer.parameters(), lr=LR) # regression\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:j8ieulfn) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Experiment_10</strong> at: <a href='https://wandb.ai/toez/ETRI_kyungho_regression/runs/j8ieulfn' target=\"_blank\">https://wandb.ai/toez/ETRI_kyungho_regression/runs/j8ieulfn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230411_180043-j8ieulfn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:j8ieulfn). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/40toez/project/kyungho/multi_modal/experimental/wandb/run-20230411_180534-kaqhowtp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/toez/ETRI_kyungho_regression/runs/kaqhowtp' target=\"_blank\">Experiment_10</a></strong> to <a href='https://wandb.ai/toez/ETRI_kyungho_regression' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/toez/ETRI_kyungho_regression' target=\"_blank\">https://wandb.ai/toez/ETRI_kyungho_regression</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/toez/ETRI_kyungho_regression/runs/kaqhowtp' target=\"_blank\">https://wandb.ai/toez/ETRI_kyungho_regression/runs/kaqhowtp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/toez/ETRI_kyungho_regression/runs/kaqhowtp?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fdecef3c4f0>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now = 10\n",
    "wandb.init(\n",
    "    project=\"ETRI_kyungho_regression\",\n",
    "    name = f\"Experiment_{now}\",\n",
    "    config={\n",
    "    \"dataset\": 'KEMDy19, KEMDy20',\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"learning_rate\": LR,\n",
    "    \"architecture\": \"TensorFusion\",\n",
    "    \"optimizer\": optimizer.__class__.__name__,\n",
    "    \"criterion\": criterion.__class__.__name__,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Epoch 1----------------\n",
      "Training...............\n",
      "loss: 0.9999064803123474\n",
      "Validation.............\n",
      "loss: 0.949436\n",
      "---------------Epoch 2----------------\n",
      "Training...............\n",
      "loss: 0.9281802773475647\n",
      "Validation.............\n",
      "loss: 1.000008\n",
      "---------------Epoch 3----------------\n",
      "Training...............\n",
      "loss: 0.9999755620956421\n",
      "Validation.............\n",
      "loss: 1.000003\n",
      "---------------Epoch 4----------------\n",
      "Training...............\n",
      "loss: 0.9999707341194153\n",
      "Validation.............\n",
      "loss: 1.000000\n",
      "---------------Epoch 5----------------\n",
      "Training...............\n",
      "loss: 0.9999820590019226\n",
      "Validation.............\n",
      "loss: 1.000000\n",
      "---------------Epoch 6----------------\n",
      "Training...............\n",
      "loss: 1.0000073909759521\n",
      "Validation.............\n",
      "loss: 0.999997\n",
      "---------------Epoch 7----------------\n",
      "Training...............\n",
      "loss: 0.9999845623970032\n",
      "Validation.............\n",
      "loss: 0.999995\n",
      "---------------Epoch 8----------------\n",
      "Training...............\n",
      "loss: 1.000011920928955\n",
      "Validation.............\n",
      "loss: 0.999995\n",
      "---------------Epoch 9----------------\n",
      "Training...............\n",
      "loss: 0.9999968409538269\n",
      "Validation.............\n",
      "loss: 0.999995\n",
      "---------------Epoch 10----------------\n",
      "Training...............\n",
      "loss: 1.0000118017196655\n",
      "Validation.............\n",
      "loss: 0.999992\n",
      "---------------Epoch 11----------------\n",
      "Training...............\n",
      "loss: 1.000016212463379\n",
      "Validation.............\n",
      "loss: 0.999987\n",
      "---------------Epoch 12----------------\n",
      "Training...............\n",
      "loss: 0.9999900460243225\n",
      "Validation.............\n",
      "loss: 0.999929\n",
      "---------------Epoch 13----------------\n",
      "Training...............\n",
      "loss: 0.9998948574066162\n",
      "Validation.............\n",
      "loss: 0.999523\n",
      "---------------Epoch 14----------------\n",
      "Training...............\n",
      "loss: 1.000535249710083\n",
      "Validation.............\n",
      "loss: 0.997030\n",
      "---------------Epoch 15----------------\n",
      "Training...............\n",
      "loss: 0.9960489869117737\n",
      "Validation.............\n",
      "loss: 0.923915\n",
      "---------------Epoch 16----------------\n",
      "Training...............\n",
      "loss: 0.9957003593444824\n",
      "Validation.............\n",
      "loss: 0.900440\n",
      "---------------Epoch 17----------------\n",
      "Training...............\n",
      "loss: 0.9159466028213501\n",
      "Validation.............\n",
      "loss: 0.871109\n",
      "---------------Epoch 18----------------\n",
      "Training...............\n",
      "loss: 0.8009672164916992\n",
      "Validation.............\n",
      "loss: 0.826126\n",
      "---------------Epoch 19----------------\n",
      "Training...............\n",
      "loss: 0.757360577583313\n",
      "Validation.............\n",
      "loss: 0.811382\n",
      "---------------Epoch 20----------------\n",
      "Training...............\n",
      "loss: 0.7707643508911133\n",
      "Validation.............\n",
      "loss: 0.791792\n",
      "---------------Epoch 21----------------\n",
      "Training...............\n",
      "loss: 0.6977906227111816\n",
      "Validation.............\n",
      "loss: 0.741800\n",
      "---------------Epoch 22----------------\n",
      "Training...............\n",
      "loss: 0.5862866640090942\n",
      "Validation.............\n",
      "loss: 0.720335\n",
      "---------------Epoch 23----------------\n",
      "Training...............\n",
      "loss: 0.5213892459869385\n",
      "Validation.............\n",
      "loss: 0.716612\n",
      "---------------Epoch 24----------------\n",
      "Training...............\n",
      "loss: 0.5382022857666016\n",
      "Validation.............\n",
      "loss: 0.704727\n",
      "---------------Epoch 25----------------\n",
      "Training...............\n",
      "loss: 0.5911158919334412\n",
      "Validation.............\n",
      "loss: 0.717758\n",
      "---------------Epoch 26----------------\n",
      "Training...............\n",
      "loss: 0.5288557410240173\n",
      "Validation.............\n",
      "loss: 0.723105\n",
      "---------------Epoch 27----------------\n",
      "Training...............\n",
      "loss: 0.5444218516349792\n",
      "Validation.............\n",
      "loss: 0.711859\n",
      "---------------Epoch 28----------------\n",
      "Training...............\n",
      "loss: 0.7822411060333252\n",
      "Validation.............\n",
      "loss: 0.694705\n",
      "---------------Epoch 29----------------\n",
      "Training...............\n",
      "loss: 0.4284138083457947\n",
      "Validation.............\n",
      "loss: 0.697260\n",
      "---------------Epoch 30----------------\n",
      "Training...............\n",
      "loss: 0.4189697504043579\n",
      "Validation.............\n",
      "loss: 0.706118\n",
      "---------------Epoch 31----------------\n",
      "Training...............\n",
      "loss: 0.3430136442184448\n",
      "Validation.............\n",
      "loss: 0.700625\n",
      "---------------Epoch 32----------------\n",
      "Training...............\n",
      "loss: 0.3544210195541382\n",
      "Validation.............\n",
      "loss: 0.703301\n",
      "---------------Epoch 33----------------\n",
      "Training...............\n",
      "loss: 0.4525994062423706\n",
      "Validation.............\n",
      "loss: 0.690387\n",
      "---------------Epoch 34----------------\n",
      "Training...............\n",
      "loss: 0.3430267572402954\n",
      "Validation.............\n",
      "loss: 0.715303\n",
      "---------------Epoch 35----------------\n",
      "Training...............\n",
      "loss: 0.40638267993927\n",
      "Validation.............\n",
      "loss: 0.718667\n",
      "---------------Epoch 36----------------\n",
      "Training...............\n",
      "loss: 0.39839625358581543\n",
      "Validation.............\n",
      "loss: 0.702501\n",
      "---------------Epoch 37----------------\n",
      "Training...............\n",
      "loss: 0.320313036441803\n",
      "Validation.............\n",
      "loss: 0.701593\n",
      "---------------Epoch 38----------------\n",
      "Training...............\n",
      "loss: 0.3343242406845093\n",
      "Validation.............\n",
      "loss: 0.694670\n",
      "---------------Epoch 39----------------\n",
      "Training...............\n",
      "loss: 0.28232455253601074\n",
      "Validation.............\n",
      "loss: 0.711744\n",
      "---------------Epoch 40----------------\n",
      "Training...............\n",
      "loss: 0.3508303165435791\n",
      "Validation.............\n",
      "loss: 0.705205\n",
      "---------------Epoch 41----------------\n",
      "Training...............\n",
      "loss: 0.29127466678619385\n",
      "Validation.............\n",
      "loss: 0.700204\n",
      "---------------Epoch 42----------------\n",
      "Training...............\n",
      "loss: 0.26149988174438477\n",
      "Validation.............\n",
      "loss: 0.718114\n",
      "---------------Epoch 43----------------\n",
      "Training...............\n",
      "loss: 0.2722301483154297\n",
      "Validation.............\n",
      "loss: 0.711956\n",
      "---------------Epoch 44----------------\n",
      "Training...............\n",
      "loss: 0.2882500886917114\n",
      "Validation.............\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m loss_train \u001b[39m=\u001b[39m train(train_dataloader,\n\u001b[1;32m      6\u001b[0m              model_tf_cnn_mixer, \n\u001b[1;32m      7\u001b[0m              criterion, \n\u001b[1;32m      8\u001b[0m              optimizer)\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mValidation.............\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m loss_val \u001b[39m=\u001b[39m test(validation_dataloader, \n\u001b[1;32m     11\u001b[0m                 model_tf_cnn_mixer, \n\u001b[1;32m     12\u001b[0m                 criterion, \n\u001b[1;32m     13\u001b[0m                 mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mval\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     15\u001b[0m wandb\u001b[39m.\u001b[39mlog({\u001b[39m\"\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m\"\u001b[39m: loss_train, \u001b[39m\"\u001b[39m\u001b[39mloss_val\u001b[39m\u001b[39m\"\u001b[39m: loss_val})\n\u001b[1;32m     16\u001b[0m scheduler\u001b[39m.\u001b[39mstep(loss_val)\n",
      "Cell \u001b[0;32mIn[80], line 26\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(dataloader, model, loss_fn, mode)\u001b[0m\n\u001b[1;32m     23\u001b[0m y_v \u001b[39m=\u001b[39m y_v\u001b[39m.\u001b[39munsqueeze(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     24\u001b[0m y_a \u001b[39m=\u001b[39m y_a\u001b[39m.\u001b[39munsqueeze(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m pred \u001b[39m=\u001b[39m model(X_txt, X_wav, X_temp, X_eda)\n\u001b[1;32m     28\u001b[0m \u001b[39m# pred_v = pred[:,0].unsqueeze(dim=-1)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39m# pred_a = pred[:,1].unsqueeze(dim=-1)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mconcat([y_v, y_a], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/project/kyungho/multi_modal/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[74], line 36\u001b[0m, in \u001b[0;36mTensorFusionMixer.forward\u001b[0;34m(self, x1, x2, x3, x4)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x1, x2, x3, x4):\n\u001b[0;32m---> 36\u001b[0m     x1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mModelA(x1)\n\u001b[1;32m     37\u001b[0m     x2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mModelB(x2)\n\u001b[1;32m     38\u001b[0m     x3 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mModelC(x3)\n",
      "File \u001b[0;32m~/project/kyungho/multi_modal/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[73], line 16\u001b[0m, in \u001b[0;36mMLPNetwork_pre.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     15\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflatten(x)\n\u001b[0;32m---> 16\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(x)\n\u001b[1;32m     17\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgelu1(x)\n\u001b[1;32m     18\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(x)\n",
      "File \u001b[0;32m~/project/kyungho/multi_modal/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/project/kyungho/multi_modal/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print(f\"---------------Epoch {epoch+1}----------------\")\n",
    "    print(\"Training...............\")\n",
    "    \n",
    "    loss_train = train(train_dataloader,\n",
    "                 model_tf_cnn_mixer, \n",
    "                 criterion, \n",
    "                 optimizer)\n",
    "    print(\"Validation.............\")\n",
    "    loss_val = test(validation_dataloader, \n",
    "                    model_tf_cnn_mixer, \n",
    "                    criterion, \n",
    "                    mode='val')\n",
    "\n",
    "    wandb.log({\"train_loss\": loss_train, \"loss_val\": loss_val})\n",
    "    scheduler.step(loss_val)\n",
    "print(\"Done!\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/home/40toez/project/kyungho/multi_modal/model/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/40toez/project/kyungho/multi_modal/'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실험을 위해 모델 저장\n",
    "PATH = ROOT + 'model/data/paradeigma_test_model_regression_230411.pkl'\n",
    "torch.save(model_tf_cnn_mixer, PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic TensorFusionNet 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.510035\n",
      "Test ccc_mean :  0.231434, Arousal_ccc :  0.393748, Valence_ccc :  0.167040\n"
     ]
    }
   ],
   "source": [
    "test(test_dataloader, model_tf_cnn_mixer, criterion, mode = 'test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bdc1fd12ca460d5768d71e9df3d9063ef832ce64a62e55a1a523c8c99752868e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
