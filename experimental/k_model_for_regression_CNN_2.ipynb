{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkhk172216\u001b[0m (\u001b[33mtoez\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# working model for tensorfusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/40toez/project/kyungho/multi_modal/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu117\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 40\n",
    "LR = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root 경로 설정\n",
    "ROOT = \"/home/40toez/project/kyungho/multi_modal/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotation, embedded data 불러오기\n",
    "with open(ROOT + 'model/data/paradeigma_KEMDY20_annotation_nonmissing.pkl', 'rb') as f:\n",
    "    annotation_20_nonmissing = pickle.load(f)\n",
    "\n",
    "with open(ROOT + 'model/data/paradeigma_KEMDY19_annotation_nonmissing.pkl', 'rb') as f:\n",
    "    annotation_19_nonmissing = pickle.load(f)\n",
    "\n",
    "ROOT = \"/home/40toez/project/kyungho/multi_modal/\"\n",
    "with open(ROOT + 'model/data/paradeigma_KEMDY20_embedding_for_dataset.pkl', 'rb') as f:\n",
    "    embedding_20_dataset = pickle.load(f)\n",
    "\n",
    "with open(ROOT + 'model/data/paradeigma_KEMDY19_embedding_for_dataset.pkl', 'rb') as f:\n",
    "    embedding_19_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24999/4239242207.py:3: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  raw_dataset = raw_dataset.append(annotation_19_nonmissing, ignore_index = True)\n",
      "/tmp/ipykernel_24999/4239242207.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  raw_dataset = raw_dataset.append(annotation_20_nonmissing, ignore_index = True)\n"
     ]
    }
   ],
   "source": [
    "# Annotation 파일 결합\n",
    "raw_dataset = pd.DataFrame()\n",
    "raw_dataset = raw_dataset.append(annotation_19_nonmissing, ignore_index = True)\n",
    "raw_dataset = raw_dataset.append(annotation_20_nonmissing, ignore_index = True)\n",
    "merged_dataset = raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wav, txt data 분리\n",
    "merged_embeddings = {'wav':[], 'txt':[]}\n",
    "for session, embeddings in embedding_19_dataset[0].items():\n",
    "    merged_embeddings['wav'].append(embeddings)\n",
    "    \n",
    "for session, embeddings in embedding_19_dataset[1].items():\n",
    "    merged_embeddings['txt'].append(embeddings)\n",
    "    \n",
    "for session, embeddings in embedding_20_dataset[0].items():\n",
    "    merged_embeddings['wav'].append(embeddings)\n",
    "    \n",
    "for session, embeddings in embedding_20_dataset[1].items():\n",
    "    merged_embeddings['txt'].append(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wav txt 2020 + 2019\n",
    "merged_embeddings['wav'] = torch.concat(merged_embeddings['wav'])\n",
    "merged_embeddings['txt'] = torch.concat(merged_embeddings['txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([21723, 149, 1024]), torch.Size([21723, 80, 768]), 21723)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 확인\n",
    "merged_embeddings['wav'].shape,merged_embeddings['txt'].shape,len(merged_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding 함수\n",
    "def sequence_padding(ts_list, padding_length = 50, mode = 'constant'):\n",
    "    \n",
    "    padding_value=0\n",
    "    \n",
    "    if (type(ts_list) != type([])) :\n",
    "        ts_list = [padding_value] * padding_length\n",
    "    \n",
    "    elif len(ts_list) >= padding_length :\n",
    "        ts_list = ts_list[0:padding_length]\n",
    "    \n",
    "    elif mode == 'constant':\n",
    "        length = padding_length - len(ts_list)\n",
    "        extend_list = [padding_value] * length\n",
    "        ts_list = ts_list + extend_list    \n",
    "\n",
    "    elif mode == 'replicate':\n",
    "        \n",
    "        quotient = padding_length // len(ts_list)\n",
    "        remainder = padding_length % len(ts_list)\n",
    "        result = ts_list * quotient\n",
    "        result += ts_list[:remainder]\n",
    "        ts_list = result\n",
    "\n",
    "    return ts_list            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시계열 데이터 2020 + 2019\n",
    "merged_dataset['Scaled EDA'] = merged_dataset['Scaled EDA'].apply(sequence_padding)\n",
    "merged_dataset['Scaled TEMP'] = merged_dataset['Scaled TEMP'].apply(sequence_padding)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch dataset 만들기\n",
    "- 참고: https://tutorials.pytorch.kr/beginner/basics/data_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EtriDataset(Dataset):\n",
    "    def __init__(self, file_names, \n",
    "                 text_embeddings, \n",
    "                 wav_embeddings, \n",
    "                 Temp,\n",
    "                 EDA,\n",
    "                 Emotion,\n",
    "                 Emotion_ext, \n",
    "                 Arousal, \n",
    "                 Valence):\n",
    "        self.file_names = file_names\n",
    "        self.text_embeddings = text_embeddings\n",
    "        self.wav_embeddings = wav_embeddings\n",
    "        self.temp = Temp\n",
    "        self.eda = EDA\n",
    "        self.label_emotion = Emotion\n",
    "        self.label_emotion_ext = Emotion_ext\n",
    "        self.label_arousal = Arousal\n",
    "        self.label_valence = Valence\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text_embeddings = self.text_embeddings[idx]\n",
    "        wav_embeddings = self.wav_embeddings[idx]\n",
    "        temp = self.temp[idx]\n",
    "        eda = self.eda[idx]\n",
    "        label_emotion = self.label_emotion[idx]\n",
    "        label_emotion_ext = self.label_emotion_ext[idx]\n",
    "        label_arousal = self.label_arousal[idx]\n",
    "        label_valence = self.label_valence[idx]\n",
    "        return text_embeddings, wav_embeddings, temp, eda, label_emotion, label_emotion_ext, label_arousal, label_valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data load 및 나누기: https://076923.github.io/posts/Python-pytorch-11/\n",
    "\n",
    "# session을 통합시킨 데이터 셋을 만들었을 때\n",
    "dataset = EtriDataset(file_names = merged_dataset['Segment ID'],\n",
    "                      text_embeddings = merged_embeddings['txt'],\n",
    "                      wav_embeddings = merged_embeddings['wav'],\n",
    "                      Emotion = merged_dataset['Emotion'],\n",
    "                      Arousal = merged_dataset['Arousal'].apply(lambda x : torch.tensor(x)),\n",
    "                      Valence = merged_dataset['Valence'].apply(lambda x : torch.tensor(x)),\n",
    "                      EDA = torch.concat(list(merged_dataset['Scaled EDA'].apply(lambda x: torch.tensor(x).view(1,-1)))), ## 일부 세션 데이터만 사용하므로, 길이를 맞춰주기 위해 일부 slicing함\n",
    "                      Temp = torch.concat(list(merged_dataset['Scaled EDA'].apply(lambda x: torch.tensor(x).view(1,-1)))), ## 일부 세션 데이터만 사용하므로, 길이를 맞춰주기 위해 일부 slicing함\n",
    "                      Emotion_ext = torch.concat(list(merged_dataset['Scaled EDA'].apply(lambda x: torch.tensor(x).view(1,-1))))\n",
    "                      ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15206 3259 3258\n",
      "Training Data Size : 15206\n",
      "Validation Data Size : 3258\n",
      "Testing Data Size : 3259\n"
     ]
    }
   ],
   "source": [
    "# Train/validation/test 데이터 분할\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(dataset_size * 0.7)\n",
    "validation_size = int(dataset_size * 0.15)\n",
    "test_size = dataset_size - train_size - validation_size\n",
    "\n",
    "train_dataset, validation_dataset, test_dataset = random_split(dataset, [train_size, validation_size, test_size])\n",
    "\n",
    "print(train_size, test_size, validation_size)\n",
    "print(f\"Training Data Size : {len(train_dataset)}\")\n",
    "print(f\"Validation Data Size : {len(validation_dataset)}\")\n",
    "print(f\"Testing Data Size : {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([149, 1024]), torch.Size([33]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data size\n",
    "merged_embeddings['wav'][0].shape , torch.Tensor(merged_dataset['EDA'][0]).shape\n",
    "# raw_dataset[session]['wav_embeddings'][0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader 선언\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size=BATCH_SIZE, \n",
    "                              shuffle=True, \n",
    "                              drop_last=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, \n",
    "                                   batch_size=BATCH_SIZE, \n",
    "                                   shuffle=True, \n",
    "                                   drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, \n",
    "                             batch_size=BATCH_SIZE, \n",
    "                             shuffle=True, \n",
    "                             drop_last=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NetWork 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# print(f\"Using {device} device\")\n",
    "\n",
    "# CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)\n",
    "# 위의 오류가 해결되기 전까진 일단 cpu를 가지고 모델을 돌리기로 한다. \n",
    "device = 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNetwork_pre(nn.Module):\n",
    "    def __init__(self, input_length, input_width):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_length*input_width, 768)\n",
    "        self.gelu1 = nn.GELU()\n",
    "        self.bn1 = nn.BatchNorm1d(768)\n",
    "        self.fc2 = nn.Linear(768, 512)\n",
    "        self.gelu2 = nn.GELU()\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.fc3 = nn.Linear(512, 32)\n",
    "        self.gelu3 = nn.GELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.gelu1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.gelu2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.fc3(x)\n",
    "        output = self.gelu3(x)\n",
    "        return output\n",
    "    \n",
    "class ConvNetwork_pre(nn.Module):\n",
    "    def __init__(self, input_channel):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels = input_channel, out_channels= 32, kernel_size = 3, padding = 1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(in_channels = 32, out_channels = 10, kernel_size = 3, padding = 1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        output = self.relu2(x)\n",
    "        return output\n",
    "\n",
    "# b, 50, 1-> (b, 1,50) -> conv1d(1,1,16) -> conv1d(1,1,16) -> (b, 1, 21)\n",
    "#                                                                     concat(dim=1) (b,2,21) -> conv1d(input_dim = 2, output_dim = 1 , kernel = 12 ) -> (b,1,10)  -> (b,10,1) \n",
    "# b, 50, 1 -> (b,1,50) -> conv1d(1,1,16) -> conv1d(1,1,16) -> (b, 1, 21)\n",
    "# class ConvNetwork_middle(nn,Module):\n",
    "\n",
    "class ConvNetwork_final(nn.Module):\n",
    "    def __init__(self, input_channel):\n",
    "        super().__init__()\n",
    "        self.conv2d_1 = nn.Conv2d(in_channels = input_channel, out_channels = 64, kernel_size=2)\n",
    "        self.leakyrelu_1 = nn.LeakyReLU()\n",
    "        self.maxpool2d_1 = nn.MaxPool2d(2)\n",
    "        self.conv2d_2 = nn.Conv2d(in_channels = 64, out_channels = 32, kernel_size=2)\n",
    "        self.leakyrelu_2 = nn.LeakyReLU()\n",
    "        self.maxpool2d_2 = nn.MaxPool2d(2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(224, 64)\n",
    "        self.leakyrelu_3 = nn.LeakyReLU()\n",
    "        self.batchnorm = nn.BatchNorm1d(64)\n",
    "        self.drop = nn.Dropout(p=0.25)\n",
    "        self.fc2 = nn.Linear(64, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv2d_1(x)\n",
    "        x = self.leakyrelu_1(x)\n",
    "        x = self.maxpool2d_1(x)\n",
    "        x = self.conv2d_2(x)\n",
    "        x = self.leakyrelu_2(x)\n",
    "        x = self.maxpool2d_2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.leakyrelu_3(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.drop(x)\n",
    "        output = self.fc2(x)  \n",
    "        return output\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorFusionMixer(nn.Module):\n",
    "    def __init__(self, ModelA, ModelB, ModelC, ModelD, ModelE):\n",
    "        super().__init__()\n",
    "        self.ModelA = ModelA\n",
    "        self.ModelB = ModelB\n",
    "        self.ModelC = ModelC\n",
    "        self.ModelD = ModelD\n",
    "        self.Model_cnn_final = ModelE\n",
    "        # self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def tensor_fusion(self, batch_arr1, batch_arr2, batch_arr3):\n",
    "        fusion_matrix_lst = []\n",
    "        for i, (arr1, arr2, arr3) in enumerate(zip(batch_arr1, batch_arr2, batch_arr3)):\n",
    "            arr1 = arr1.unsqueeze(-1).unsqueeze(-1)\n",
    "            arr2 = arr2.unsqueeze(0).unsqueeze(-1)\n",
    "            arr3 = arr3.squeeze().unsqueeze(0).unsqueeze(0)\n",
    "            \n",
    "            # outer_matrix = torch.einsum('i,j,kp->ijk', arr1, arr2, arr3)\n",
    "            kron_matrix = torch.kron(torch.kron(arr1,arr2), arr3)\n",
    "            l, w, d = kron_matrix.shape\n",
    "            \n",
    "            kron_matrix = kron_matrix.view(-1, l, w, d)\n",
    "            fusion_matrix_lst.append(kron_matrix)\n",
    "            \n",
    "        fusion_matrix = torch.concat(fusion_matrix_lst)\n",
    "        # fusion_matrix = fusion_matrix.unsqueeze(-1)\n",
    "        \n",
    "        return fusion_matrix\n",
    "        \n",
    "    def forward(self, x1, x2, x3, x4):\n",
    "        x1 = self.ModelA(x1)\n",
    "        x2 = self.ModelB(x2)\n",
    "        x3 = self.ModelC(x3)\n",
    "        x4 = self.ModelD(x4)\n",
    "        \n",
    "        x5 = torch.cat([x3,x4], dim=0)\n",
    "        fusion_matrix = self.tensor_fusion(x1, x2, x5)\n",
    "        \n",
    "        output = self.Model_cnn_final(fusion_matrix) \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFusionMixer(\n",
      "  (ModelA): MLPNetwork_pre(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc1): Linear(in_features=61440, out_features=768, bias=True)\n",
      "    (gelu1): GELU(approximate='none')\n",
      "    (bn1): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (fc2): Linear(in_features=768, out_features=512, bias=True)\n",
      "    (gelu2): GELU(approximate='none')\n",
      "    (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (fc3): Linear(in_features=512, out_features=32, bias=True)\n",
      "    (gelu3): GELU(approximate='none')\n",
      "  )\n",
      "  (ModelB): MLPNetwork_pre(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc1): Linear(in_features=152576, out_features=768, bias=True)\n",
      "    (gelu1): GELU(approximate='none')\n",
      "    (bn1): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (fc2): Linear(in_features=768, out_features=512, bias=True)\n",
      "    (gelu2): GELU(approximate='none')\n",
      "    (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (fc3): Linear(in_features=512, out_features=32, bias=True)\n",
      "    (gelu3): GELU(approximate='none')\n",
      "  )\n",
      "  (ModelC): ConvNetwork_pre(\n",
      "    (conv1): Conv1d(50, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (relu1): ReLU()\n",
      "    (conv2): Conv1d(32, 10, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (relu2): ReLU()\n",
      "  )\n",
      "  (ModelD): ConvNetwork_pre(\n",
      "    (conv1): Conv1d(50, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (relu1): ReLU()\n",
      "    (conv2): Conv1d(32, 10, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (relu2): ReLU()\n",
      "  )\n",
      "  (Model_cnn_final): ConvNetwork_final(\n",
      "    (conv2d_1): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (leakyrelu_1): LeakyReLU(negative_slope=0.01)\n",
      "    (maxpool2d_1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv2d_2): Conv2d(64, 32, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (leakyrelu_2): LeakyReLU(negative_slope=0.01)\n",
      "    (maxpool2d_2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc1): Linear(in_features=224, out_features=64, bias=True)\n",
      "    (leakyrelu_3): LeakyReLU(negative_slope=0.01)\n",
      "    (batchnorm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (drop): Dropout(p=0.25, inplace=False)\n",
      "    (fc2): Linear(in_features=64, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# txt_input_length, txt_input_width = raw_dataset[session]['text_embeddings'][0].shape | 마지막엔 지울 것\n",
    "# _, wav_input_length, wav_input_width = raw_dataset[session]['wav_embeddings'][0].shape\n",
    "txt_input_length, txt_input_width = dataset.text_embeddings[0].shape\n",
    "wav_input_length, wav_input_width = dataset.wav_embeddings[0].shape\n",
    "temp_input_length = dataset.temp[0].shape[0]\n",
    "eda_input_length = dataset.eda[0].shape[0]\n",
    "# temp_input_length = 1\n",
    "# eda_input_length = 1\n",
    "\n",
    "# tf_mixer에 들어갈 wav mlp, txt mlp 선언\n",
    "model_mlp_txt = MLPNetwork_pre(txt_input_length, txt_input_width).to(device)\n",
    "model_mlp_wav = MLPNetwork_pre(wav_input_length, wav_input_width).to(device)\n",
    "model_conv_temp = ConvNetwork_pre(temp_input_length).to(device)\n",
    "model_conv_eda = ConvNetwork_pre(eda_input_length).to(device)\n",
    "\n",
    "model_cnn_final = ConvNetwork_final(32).to(device)\n",
    "\n",
    "# 최종 모델 선언\n",
    "model_tf_cnn_mixer = TensorFusionMixer(ModelA = model_mlp_txt, \n",
    "                                   ModelB = model_mlp_wav,\n",
    "                                   ModelC = model_conv_temp,\n",
    "                                   ModelD = model_conv_eda,\n",
    "                                   ModelE = model_cnn_final).to(device)\n",
    "\n",
    "# model 병렬 학습 처리\n",
    "# 쿠다 문제가 고쳐졌을 경우에만 아래의 코드의 주석을 해제해서 실행한다. \n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "#     model_mlp_txt = nn.DataParallel(model_mlp_txt).to(device)\n",
    "#     model_mlp_wav = nn.DataParallel(model_mlp_wav).to(device)\n",
    "#     model_conv_temp = nn.DataParallel(model_conv_temp).to(device)\n",
    "#     model_conv_eda = nn.DataParallel(model_conv_eda).to(device)\n",
    "#     model_tf_cnn_mixer = nn.DataParallel(model_tf_cnn_mixer).to(device)\n",
    "    \n",
    "print(model_tf_cnn_mixer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def covariance(x, y):\n",
    "    x_mean = torch.mean(x)\n",
    "    y_mean = torch.mean(y)\n",
    "    return torch.mean((x - x_mean) * (y - y_mean))\n",
    "\n",
    "def ccc(arousal_valence_A, arousal_valence_B):\n",
    "    # Calculate means\n",
    "    mean_A = torch.mean(arousal_valence_A, axis=0)\n",
    "    mean_B = torch.mean(arousal_valence_B, axis=0)\n",
    "\n",
    "    # Calculate variances\n",
    "    var_A = torch.var(arousal_valence_A, axis=0, unbiased=True)\n",
    "    var_B = torch.var(arousal_valence_B, axis=0, unbiased=True)\n",
    "\n",
    "    # Calculate covariance\n",
    "    cov_arousal = covariance(arousal_valence_A[:, 0], arousal_valence_B[:, 0])\n",
    "    cov_valence = covariance(arousal_valence_A[:, 1], arousal_valence_B[:, 1])\n",
    "\n",
    "    # Calculate CCC for arousal and valence components\n",
    "    ccc_arousal = (2 * cov_arousal) / (var_A[0] + var_B[0] + (mean_A[0] - mean_B[0]) ** 2)\n",
    "    ccc_valence = (2 * cov_valence) / (var_A[1] + var_B[1] + (mean_A[1] - mean_B[1]) ** 2)\n",
    "\n",
    "    # Average the CCCs for arousal and valence components\n",
    "    ccc = (ccc_arousal + ccc_valence) / 2\n",
    "\n",
    "    return ccc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCCLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self,inputs,targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        # pred_v = inputs[:,0]\n",
    "        # pred_a = inputs[:,1]\n",
    "        # y_v  = targets[:,0]\n",
    "        # y_a  = targets[:,1]\n",
    "\n",
    "        return 1 - ccc(self.inputs, self.targets )\n",
    "    \n",
    "cccl = CCCLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습을 위한 train, test method 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)    \n",
    "    # data 순서: text_embeddings, wav_embeddings, temp, eda, label_emotion, label_emotion_ext, label_arousal, label_valence\n",
    "    for batch, (X_txt, X_wav, X_temp, X_eda, \n",
    "                    label_emotion, label_emotion_ext, label_arousal, label_valence) in enumerate(dataloader): \n",
    "\n",
    "        # 예측 오류 계산 \n",
    "        X_txt, X_wav, X_temp, X_eda, y_v, y_a= X_txt.to(device), X_wav.to(device), X_temp.to(device), X_eda.to(device),label_valence.type(torch.float32).to(device), label_arousal.type(torch.float32).to(device)\n",
    "        \n",
    "        X_temp = X_temp.unsqueeze(dim=-1)\n",
    "        X_eda = X_eda.unsqueeze(dim=-1)\n",
    "        y_v = y_v.unsqueeze(dim=-1)\n",
    "        y_a = y_a.unsqueeze(dim=-1)\n",
    "        \n",
    "        pred = model(X_txt, X_wav, X_temp, X_eda)\n",
    "    \n",
    "        # pred_v = pred[:,0].unsqueeze(dim=-1)\n",
    "        # pred_a = pred[:,1].unsqueeze(dim=-1)\n",
    "        \n",
    "        y = torch.concat([y_v, y_a], dim=-1)\n",
    "\n",
    "        \n",
    "        # # loss_a = loss_fn(pred_a, y_a)\n",
    "        # # loss_v = loss_fn(pred_v, y_v)\n",
    "        # loss = loss_fn(pred, y)\n",
    "        \n",
    "        # pred_ccc_v= ccc(pred_v, y_v)\n",
    "        # pred_ccc_a = ccc(pred_a, y_a)\n",
    "        \n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # ccc_mean = (pred_ccc_a + pred_ccc_v) / 2\n",
    "        \n",
    "\n",
    "        # 역전파\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # loss_a.backward(retain_graph = True)\n",
    "        # loss_v.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X_txt)\n",
    "            # loss_a, loss_v, current = loss_a.item(), loss_v.item(), batch * len(X_txt)\n",
    "            # print(f\"loss_a: {loss_a:>7f}, loss_v: {loss_v:>7f}\")\n",
    "            print(f\"loss: {loss}\")\n",
    "            # print(f\"ccc_mean : {ccc_mean.item():>9f}, Arousal_ccc : {pred_ccc_a.item():>9f}, Valence_ccc : {pred_ccc_v.item():>9f}\".format())\n",
    "\n",
    "    # wandb.log({'loss':loss,'ccc_mean':ccc_mean.item(),'ccc_valence' : pred_ccc_v.item(),'ccc_arousal' : pred_ccc_a.item()})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn, mode = 'test'):\n",
    "    \n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    # test_loss_a = 0\n",
    "    # test_loss_v = 0\n",
    "    # ccc_mean = 0\n",
    "    # ccc_a = 0\n",
    "    # ccc_v = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # data 순서: text_embeddings, wav_embeddings, temp, eda, label_emotion, label_emotion_ext, label_arousal, label_valence\n",
    "        for batch, (X_txt, X_wav, X_temp, X_eda, \n",
    "                    label_emotion, label_emotion_ext, label_arousal, label_valence) in enumerate(dataloader): \n",
    "\n",
    "            # 예측 오류 계산 \n",
    "            X_txt, X_wav, X_temp, X_eda, y_v, y_a= X_txt.to(device), X_wav.to(device), X_temp.to(device), X_eda.to(device),label_valence.type(torch.float32).to(device), label_arousal.type(torch.float32).to(device)\n",
    "            \n",
    "            X_temp = X_temp.unsqueeze(dim=-1)\n",
    "            X_eda = X_eda.unsqueeze(dim=-1)\n",
    "            y_v = y_v.unsqueeze(dim=-1)\n",
    "            y_a = y_a.unsqueeze(dim=-1)\n",
    "\n",
    "            pred = model(X_txt, X_wav, X_temp, X_eda)\n",
    "            \n",
    "            # pred_v = pred[:,0].unsqueeze(dim=-1)\n",
    "            # pred_a = pred[:,1].unsqueeze(dim=-1)\n",
    "            y = torch.concat([y_v, y_a], dim=-1)\n",
    "\n",
    "            \n",
    "            loss = loss_fn(pred, y)\n",
    "            test_loss += loss\n",
    "            \n",
    "            # loss_a = loss_fn(pred_a, y_a)\n",
    "            # loss_v = loss_fn(pred_v, y_v)\n",
    "\n",
    "            # test_loss_a += loss_a\n",
    "            # test_loss_v += loss_v\n",
    "\n",
    "            # pred_ccc_a = ccc(pred_a, y_a)\n",
    "            # pred_ccc_v = ccc(pred_v, y_v)\n",
    "            \n",
    "            # ccc_a += pred_ccc_a\n",
    "            # ccc_v += pred_ccc_v\n",
    "      \n",
    "      \n",
    "    test_loss /= num_batches  \n",
    "    # test_loss_v /= num_batches    \n",
    "    # test_loss_a /= num_batches\n",
    "    \n",
    "    # ccc_v /= num_batches\n",
    "    # ccc_a /= num_batches\n",
    "    # ccc_mean = (ccc_a + ccc_v) / 2\n",
    "\n",
    "    \n",
    "    if mode == 'test':\n",
    "        # print(f\"loss_a: {loss_a:>7f}, loss_v: {loss_v:>7f}\")\n",
    "        print(f\"loss: {test_loss:>7f}\")\n",
    "        # print(f\"Test ccc_mean : {ccc_mean.item():>9f}, Arousal_ccc : {pred_ccc_a.item():>9f}, Valence_ccc : {pred_ccc_v.item():>9f}\")\n",
    "    elif mode == 'val':\n",
    "        # print(f\"loss_a: {loss_a:>7f}, loss_v: {loss_v:>7f}\")\n",
    "        print(f\"loss: {test_loss:>7f}\")\n",
    "        # print(f\"Val ccc_mean : {ccc_mean.item():>9f}, Arousal_ccc : {pred_ccc_a.item():>9f}, Valence_ccc : {pred_ccc_v.item():>9f}\")\n",
    "        # wandb.log({'loss':loss,'ccc_mean':ccc_mean.item(),'ccc_valence' : pred_ccc_v.item(),'ccc_arousal' : pred_ccc_a.item()})\n",
    "    return test_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 지난 저장한 모델이 있다면\n",
    "# PATH = './data/test_model.pkl'\n",
    "# model_tf_mixer = torch.load(PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start training mlp fusion mixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Training Parameters\n",
    "\n",
    "criterion =  CCCLoss()# nn.MSELoss()\n",
    "optimizer = optim.SGD(model_tf_cnn_mixer.parameters(), lr=LR) # regression\n",
    "epochs = EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CCCLoss'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/40toez/project/kyungho/multi_modal/experimental/wandb/run-20230411_111812-an48dxlg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/toez/ETRI_regression/runs/an48dxlg' target=\"_blank\">chocolate-brook-16</a></strong> to <a href='https://wandb.ai/toez/ETRI_regression' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/toez/ETRI_regression' target=\"_blank\">https://wandb.ai/toez/ETRI_regression</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/toez/ETRI_regression/runs/an48dxlg' target=\"_blank\">https://wandb.ai/toez/ETRI_regression/runs/an48dxlg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run = wandb.init(\n",
    "#     # Set the project where this run will be logged\n",
    "#     project=\"ETRI_regression\",\n",
    "#     # Track hyperparameters and run metadata\n",
    "#     config={\n",
    "#         \"Learning rate\": LR,\n",
    "#         \"Epochs\": EPOCHS,\n",
    "#         # \"Optimizer\": optimizer.__class__.__name__,\n",
    "#         \"Loss\": criterion.__class__.__name__\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ccc(torch\u001b[39m.\u001b[39;49mtensor([\u001b[39m1.\u001b[39;49m,\u001b[39m2.\u001b[39;49m,\u001b[39m3.\u001b[39;49m,\u001b[39m4.\u001b[39;49m]),torch\u001b[39m.\u001b[39;49mtensor([\u001b[39m1.\u001b[39;49m,\u001b[39m2.\u001b[39;49m,\u001b[39m3.\u001b[39;49m,\u001b[39m4.\u001b[39;49m]))\n",
      "Cell \u001b[0;32mIn[51], line 18\u001b[0m, in \u001b[0;36mccc\u001b[0;34m(arousal_valence_A, arousal_valence_B)\u001b[0m\n\u001b[1;32m     15\u001b[0m var_B \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mvar(arousal_valence_B, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, unbiased\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m \u001b[39m# Calculate covariance\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m cov_arousal \u001b[39m=\u001b[39m covariance(arousal_valence_A[:, \u001b[39m0\u001b[39;49m], arousal_valence_B[:, \u001b[39m0\u001b[39m])\n\u001b[1;32m     19\u001b[0m cov_valence \u001b[39m=\u001b[39m covariance(arousal_valence_A[:, \u001b[39m1\u001b[39m], arousal_valence_B[:, \u001b[39m1\u001b[39m])\n\u001b[1;32m     21\u001b[0m \u001b[39m# Calculate CCC for arousal and valence components\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Epoch 1----------------\n",
      "Training...............\n",
      "loss: 0.9937795996665955\n",
      "Validation.............\n",
      "loss: 0.992137\n",
      "---------------Epoch 2----------------\n",
      "Training...............\n",
      "loss: 0.9976429343223572\n",
      "Validation.............\n",
      "loss: 0.984033\n",
      "---------------Epoch 3----------------\n",
      "Training...............\n",
      "loss: 0.9898079037666321\n",
      "Validation.............\n",
      "loss: 0.973675\n",
      "---------------Epoch 4----------------\n",
      "Training...............\n",
      "loss: 0.9956458806991577\n",
      "Validation.............\n",
      "loss: 0.961297\n",
      "---------------Epoch 5----------------\n",
      "Training...............\n",
      "loss: 1.0029641389846802\n",
      "Validation.............\n",
      "loss: 0.903185\n",
      "---------------Epoch 6----------------\n",
      "Training...............\n",
      "loss: 0.9747762680053711\n",
      "Validation.............\n",
      "loss: 0.885398\n",
      "---------------Epoch 7----------------\n",
      "Training...............\n",
      "loss: 0.8619399070739746\n",
      "Validation.............\n",
      "loss: 0.865770\n",
      "---------------Epoch 8----------------\n",
      "Training...............\n",
      "loss: 0.9033898711204529\n",
      "Validation.............\n",
      "loss: 0.760209\n",
      "---------------Epoch 9----------------\n",
      "Training...............\n",
      "loss: 0.787244439125061\n",
      "Validation.............\n",
      "loss: 0.715627\n",
      "---------------Epoch 10----------------\n",
      "Training...............\n",
      "loss: 0.6493130922317505\n",
      "Validation.............\n",
      "loss: 0.702656\n",
      "---------------Epoch 11----------------\n",
      "Training...............\n",
      "loss: 0.5597693920135498\n",
      "Validation.............\n",
      "loss: 0.706648\n",
      "---------------Epoch 12----------------\n",
      "Training...............\n",
      "loss: 0.6968682408332825\n",
      "Validation.............\n",
      "loss: 0.688745\n",
      "---------------Epoch 13----------------\n",
      "Training...............\n",
      "loss: 0.6509009599685669\n",
      "Validation.............\n",
      "loss: 0.726581\n",
      "---------------Epoch 14----------------\n",
      "Training...............\n",
      "loss: 0.7608235478401184\n",
      "Validation.............\n",
      "loss: 0.684732\n",
      "---------------Epoch 15----------------\n",
      "Training...............\n",
      "loss: 0.6452142000198364\n",
      "Validation.............\n",
      "loss: 0.687282\n",
      "---------------Epoch 16----------------\n",
      "Training...............\n",
      "loss: 0.6022645235061646\n",
      "Validation.............\n",
      "loss: 0.681562\n",
      "---------------Epoch 17----------------\n",
      "Training...............\n",
      "loss: 0.6386069059371948\n",
      "Validation.............\n",
      "loss: 0.675169\n",
      "---------------Epoch 18----------------\n",
      "Training...............\n",
      "loss: 0.5719537734985352\n",
      "Validation.............\n",
      "loss: 0.702018\n",
      "---------------Epoch 19----------------\n",
      "Training...............\n",
      "loss: 0.614383339881897\n",
      "Validation.............\n",
      "loss: 0.676337\n",
      "---------------Epoch 20----------------\n",
      "Training...............\n",
      "loss: 0.5536472797393799\n",
      "Validation.............\n",
      "loss: 0.695616\n",
      "---------------Epoch 21----------------\n",
      "Training...............\n",
      "loss: 0.6113204956054688\n",
      "Validation.............\n",
      "loss: 0.653566\n",
      "---------------Epoch 22----------------\n",
      "Training...............\n",
      "loss: 0.5471713542938232\n",
      "Validation.............\n",
      "loss: 0.640916\n",
      "---------------Epoch 23----------------\n",
      "Training...............\n",
      "loss: 0.5239666700363159\n",
      "Validation.............\n",
      "loss: 0.645287\n",
      "---------------Epoch 24----------------\n",
      "Training...............\n",
      "loss: 0.5788387656211853\n",
      "Validation.............\n",
      "loss: 0.654622\n",
      "---------------Epoch 25----------------\n",
      "Training...............\n",
      "loss: 0.506170392036438\n",
      "Validation.............\n",
      "loss: 0.653947\n",
      "---------------Epoch 26----------------\n",
      "Training...............\n",
      "loss: 0.5372259020805359\n",
      "Validation.............\n",
      "loss: 0.676607\n",
      "---------------Epoch 27----------------\n",
      "Training...............\n",
      "loss: 0.557163417339325\n",
      "Validation.............\n",
      "loss: 0.635859\n",
      "---------------Epoch 28----------------\n",
      "Training...............\n",
      "loss: 0.5570347309112549\n",
      "Validation.............\n",
      "loss: 0.634649\n",
      "---------------Epoch 29----------------\n",
      "Training...............\n",
      "loss: 0.45052337646484375\n",
      "Validation.............\n",
      "loss: 0.650486\n",
      "---------------Epoch 30----------------\n",
      "Training...............\n",
      "loss: 0.5190108418464661\n",
      "Validation.............\n",
      "loss: 0.630971\n",
      "---------------Epoch 31----------------\n",
      "Training...............\n",
      "loss: 0.4231584668159485\n",
      "Validation.............\n",
      "loss: 0.663121\n",
      "---------------Epoch 32----------------\n",
      "Training...............\n",
      "loss: 0.5398592352867126\n",
      "Validation.............\n",
      "loss: 0.643000\n",
      "---------------Epoch 33----------------\n",
      "Training...............\n",
      "loss: 0.4788937568664551\n",
      "Validation.............\n",
      "loss: 0.633335\n",
      "---------------Epoch 34----------------\n",
      "Training...............\n",
      "loss: 0.42941009998321533\n",
      "Validation.............\n",
      "loss: 0.656403\n",
      "---------------Epoch 35----------------\n",
      "Training...............\n",
      "loss: 0.48412448167800903\n",
      "Validation.............\n",
      "loss: 0.673349\n",
      "---------------Epoch 36----------------\n",
      "Training...............\n",
      "loss: 0.437685489654541\n",
      "Validation.............\n",
      "loss: 0.665918\n",
      "---------------Epoch 37----------------\n",
      "Training...............\n",
      "loss: 0.4443831443786621\n",
      "Validation.............\n",
      "loss: 0.679249\n",
      "---------------Epoch 38----------------\n",
      "Training...............\n",
      "loss: 0.48514050245285034\n",
      "Validation.............\n",
      "loss: 0.645631\n",
      "---------------Epoch 39----------------\n",
      "Training...............\n",
      "loss: 0.43691229820251465\n",
      "Validation.............\n",
      "loss: 0.643166\n",
      "---------------Epoch 40----------------\n",
      "Training...............\n",
      "loss: 0.40563082695007324\n",
      "Validation.............\n",
      "loss: 0.658636\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_history_train = {}\n",
    "loss_history_val = {}\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"---------------Epoch {epoch+1}----------------\")\n",
    "    print(\"Training...............\")\n",
    "    train(train_dataloader,\n",
    "                 model_tf_cnn_mixer, \n",
    "                 criterion, \n",
    "                 optimizer)\n",
    "    loss_history_train[epoch+1] = loss_train\n",
    "    print(\"Validation.............\")\n",
    "    loss_val = test(validation_dataloader, \n",
    "                    model_tf_cnn_mixer, \n",
    "                    criterion, \n",
    "                    mode='val')\n",
    "    loss_history_val[epoch+1] = loss_val\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f28c9dbd510>]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_history_val.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실험을 위해 모델 저장\n",
    "PATH = '.model/data/paradeigma_test_model_multilabel_CNN.pkl'\n",
    "torch.save(model_tf_cnn_mixer, PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic TensorFusionNet 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bdc1fd12ca460d5768d71e9df3d9063ef832ce64a62e55a1a523c8c99752868e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
