{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# working model for tensorfusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu117\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from torchmetrics import F1Score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "from datasets import load_dataset, Dataset, Audio, Features\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch import nn, optim\n",
    "print(torch.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n",
    "\n",
    "dict_for_dataset = [embeddings_wav_train, embeddings_wav_val, embeddings_wav_test,\n",
    "                    embeddings_txt_train, embeddings_txt_val, embeddings_txt_test,\n",
    "                    KEMDY20_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arplab/project/paradeigma/multi_modal/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('/home/arplab/project/paradeigma/multi_modal/model/data/paradeigma_KEMDY20_for_dataset.pkl', 'rb') as f:\n",
    "    dict_for_dataset = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_wav_train, embeddings_wav_val, embeddings_wav_test, embeddings_txt_train, embeddings_txt_val, embeddings_txt_test, KEMDY20_dict = dict_for_dataset\n",
    "train_df = KEMDY20_dict['train_df'][1]\n",
    "val_df = KEMDY20_dict['val_df'][1]\n",
    "test_df = KEMDY20_dict['test_df'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_padding(ts_list, padding_length = 50, mode = 'constant'):\n",
    "    \n",
    "    padding_value=0\n",
    "    \n",
    "    if (type(ts_list) != type([])) :\n",
    "        ts_list = [padding_value] * padding_length\n",
    "    \n",
    "    elif len(ts_list) >= padding_length :\n",
    "        ts_list = ts_list[0:padding_length]\n",
    "    \n",
    "    elif mode == 'constant':\n",
    "        length = padding_length - len(ts_list)\n",
    "        extend_list = [padding_value] * length\n",
    "        ts_list = ts_list + extend_list    \n",
    "        \n",
    "    elif mode == 'replicate':\n",
    "        \n",
    "        quotient = padding_length // len(ts_list)\n",
    "        remainder = padding_length % len(ts_list)\n",
    "        result = ts_list * quotient\n",
    "        result += ts_list[:remainder]\n",
    "        ts_list = result\n",
    "        \n",
    "    return ts_list \n",
    "\n",
    "train_df['Scaled EDA'] = train_df['Scaled EDA'].apply(sequence_padding)\n",
    "train_df['Scaled TEMP'] = train_df['Scaled TEMP'].apply(sequence_padding)\n",
    "val_df['Scaled EDA'] = val_df['Scaled EDA'].apply(sequence_padding)\n",
    "val_df['Scaled TEMP'] = val_df['Scaled TEMP'].apply(sequence_padding)\n",
    "test_df['Scaled EDA'] = test_df['Scaled EDA'].apply(sequence_padding)\n",
    "test_df['Scaled TEMP'] = test_df['Scaled TEMP'].apply(sequence_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EtriDataset(Dataset):\n",
    "    def __init__(self, file_names, \n",
    "                 text_embeddings, \n",
    "                 wav_embeddings, \n",
    "                 Temp,\n",
    "                 EDA,\n",
    "                 Emotion,\n",
    "                 Emotion_ext, \n",
    "                 Arousal, \n",
    "                 Valence):\n",
    "        self.file_names = file_names\n",
    "        self.text_embeddings = text_embeddings\n",
    "        self.wav_embeddings = wav_embeddings\n",
    "        self.temp = Temp\n",
    "        self.eda = EDA\n",
    "        self.label_emotion = Emotion\n",
    "        self.label_emotion_ext = Emotion_ext\n",
    "        self.label_arousal = Arousal\n",
    "        self.label_valence = Valence\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text_embeddings = self.text_embeddings[idx]\n",
    "        wav_embeddings = self.wav_embeddings[idx]\n",
    "        temp = self.temp[idx]\n",
    "        eda = self.eda[idx]\n",
    "        label_emotion = self.label_emotion[idx]\n",
    "        label_emotion_ext = self.label_emotion_ext[idx]\n",
    "        label_arousal = self.label_arousal[idx]\n",
    "        label_valence = self.label_valence[idx]\n",
    "        return text_embeddings, wav_embeddings, temp, eda, label_emotion, label_emotion_ext, label_arousal, label_valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EtriDataset(file_names = train_df['Segment ID'],\n",
    "                      text_embeddings = embeddings_txt_train,\n",
    "                      wav_embeddings = embeddings_wav_train,\n",
    "                      Emotion = train_df['Emotion'],\n",
    "                      Arousal = train_df['Arousal'],\n",
    "                      Valence = train_df['Valence'],\n",
    "                      EDA = torch.concat(list(train_df['Scaled EDA'].apply(lambda x: torch.tensor(x).view(1,-1)))), \n",
    "                      Temp = torch.concat(list(train_df['Scaled TEMP'].apply(lambda x: torch.tensor(x).view(1,-1)))), \n",
    "                      Emotion_ext = torch.concat(list(train_df['emotion_vector'].apply(lambda x: torch.tensor(x).view(1,-1))))\n",
    "                      )\n",
    "\n",
    "validation_dataset = EtriDataset(file_names = val_df['Segment ID'],\n",
    "                      text_embeddings = embeddings_txt_val,\n",
    "                      wav_embeddings = embeddings_wav_val,\n",
    "                      Emotion = val_df['Emotion'],\n",
    "                      Arousal = val_df['Arousal'],\n",
    "                      Valence = val_df['Valence'],\n",
    "                      EDA = torch.concat(list(val_df['Scaled EDA'].apply(lambda x: torch.tensor(x).view(1,-1)))), \n",
    "                      Temp = torch.concat(list(val_df['Scaled TEMP'].apply(lambda x: torch.tensor(x).view(1,-1)))), \n",
    "                      Emotion_ext = torch.concat(list(val_df['emotion_vector'].apply(lambda x: torch.tensor(x).view(1,-1))))\n",
    "                      )\n",
    "\n",
    "test_dataset = EtriDataset(file_names = test_df['Segment ID'],\n",
    "                      text_embeddings = embeddings_txt_test,\n",
    "                      wav_embeddings = embeddings_wav_test,\n",
    "                      Emotion = test_df['Emotion'],\n",
    "                      Arousal = test_df['Arousal'],\n",
    "                      Valence = test_df['Valence'],\n",
    "                      EDA = torch.concat(list(test_df['Scaled EDA'].apply(lambda x: torch.tensor(x).view(1,-1)))), \n",
    "                      Temp = torch.concat(list(test_df['Scaled TEMP'].apply(lambda x: torch.tensor(x).view(1,-1)))),\n",
    "                      Emotion_ext = torch.concat(list(test_df['emotion_vector'].apply(lambda x: torch.tensor(x).view(1,-1))))\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Size : 2746\n",
      "Validation Data Size : 2535\n",
      "Testing Data Size : 2580\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training Data Size : {len(train_dataset)}\")\n",
    "print(f\"Validation Data Size : {len(validation_dataset)}\")\n",
    "print(f\"Testing Data Size : {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True, drop_last=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'neutral': 0,\n",
       "  'happy': 1,\n",
       "  'surprise': 2,\n",
       "  'disgust': 3,\n",
       "  'angry': 4,\n",
       "  'sad': 5,\n",
       "  'fear': 6,\n",
       "  'surprise;neutral': 7,\n",
       "  'neutral;sad': 8,\n",
       "  'happy;neutral': 9,\n",
       "  'angry;neutral': 10,\n",
       "  'neutral;disqust': 11,\n",
       "  'neutral;fear': 12,\n",
       "  'happy;surprise': 13,\n",
       "  'happy;angry;neutral': 14,\n",
       "  'angry;disqust': 15,\n",
       "  'happy;surprise;neutral': 16,\n",
       "  'happy;fear': 17,\n",
       "  'happy;neutral;fear': 18,\n",
       "  'angry;neutral;disqust': 19,\n",
       "  'neutral;disqust;sad': 20,\n",
       "  'angry;neutral;disqust;fear;sad': 21,\n",
       "  'happy;sad': 22,\n",
       "  'happy;neutral;disqust': 23},\n",
       " {0: 'neutral',\n",
       "  1: 'happy',\n",
       "  2: 'surprise',\n",
       "  3: 'disgust',\n",
       "  4: 'angry',\n",
       "  5: 'sad',\n",
       "  6: 'fear',\n",
       "  7: 'surprise;neutral',\n",
       "  8: 'neutral;sad',\n",
       "  9: 'happy;neutral',\n",
       "  10: 'angry;neutral',\n",
       "  11: 'neutral;disqust',\n",
       "  12: 'neutral;fear',\n",
       "  13: 'happy;surprise',\n",
       "  14: 'happy;angry;neutral',\n",
       "  15: 'angry;disqust',\n",
       "  16: 'happy;surprise;neutral',\n",
       "  17: 'happy;fear',\n",
       "  18: 'happy;neutral;fear',\n",
       "  19: 'angry;neutral;disqust',\n",
       "  20: 'neutral;disqust;sad',\n",
       "  21: 'angry;neutral;disqust;fear;sad',\n",
       "  22: 'happy;sad',\n",
       "  23: 'happy;neutral;disqust'})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoding Emotion for whole data\n",
    "# 사전에 실제로 encoding한 끝 수가 마지막 linear layer의 끝자리랑 맞아야 합니다. 아니면 CUDA error: CUBLAS_STATUS_EXECUTION_FAILED가 나는 것 같아요.\n",
    "# 예를 들어, label이 0~9, 11,13이렇게 12개가 되었어도, 0~13은 14개니까 마지막 레이어에서 14개 unit을 받아야 multiclass classification이 에러없이 진행됩니다!\n",
    "# 데이터에서 정답 라벨 인코딩: ['neutral', 'happy', 'surprise', 'disgust', 'angry', 'sad', 'fear']\n",
    "# 이 순서를 지켜서 라벨링을 해야함\n",
    "encode_dict = {'neutral':0, 'happy':1, 'surprise':2, 'disgust':3, 'angry':4, 'sad':5, 'fear':6,\n",
    "               'surprise;neutral': 7, 'neutral;sad': 8, 'happy;neutral': 9, 'angry;neutral': 10, \n",
    "               'neutral;disqust': 11, 'neutral;fear': 12, 'happy;surprise': 13, 'happy;angry;neutral': 14,\n",
    "               'angry;disqust': 15, 'happy;surprise;neutral': 16, 'happy;fear': 17,'happy;neutral;fear': 18,\n",
    "               'angry;neutral;disqust': 19, 'neutral;disqust;sad': 20, 'angry;neutral;disqust;fear;sad': 21,\n",
    "               'happy;sad': 22, 'happy;neutral;disqust': 23}\n",
    "decode_dict = {b:i for i, b in encode_dict.items()}\n",
    "encode_dict, decode_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NetWork 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNetwork_pre(nn.Module):\n",
    "    def __init__(self, input_length, input_width):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_length*input_width, 768)\n",
    "        self.gelu1 = nn.GELU()\n",
    "        self.bn1 = nn.BatchNorm1d(768)\n",
    "        self.fc2 = nn.Linear(768, 512)\n",
    "        self.gelu2 = nn.GELU()\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.fc3 = nn.Linear(512, 32)\n",
    "        self.gelu3 = nn.GELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.gelu1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.gelu2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.fc3(x)\n",
    "        output = self.gelu3(x)\n",
    "        return output\n",
    "    \n",
    "class ConvNetwork_pre(nn.Module):\n",
    "    def __init__(self, input_channel):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels = input_channel, out_channels= 32, kernel_size = 3, padding = 1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(in_channels = 32, out_channels = 10, kernel_size = 3, padding = 1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        output = self.relu2(x)\n",
    "        return output\n",
    "\n",
    "class ConvNetwork_final(nn.Module):\n",
    "    def __init__(self, input_channel):\n",
    "        super().__init__()\n",
    "        self.conv2d_1 = nn.Conv2d(in_channels = input_channel, out_channels = 64, kernel_size=2)\n",
    "        self.leakyrelu_1 = nn.LeakyReLU()\n",
    "        self.maxpool2d_1 = nn.MaxPool2d(2)\n",
    "        self.conv2d_2 = nn.Conv2d(in_channels = 64, out_channels = 32, kernel_size=2)\n",
    "        self.leakyrelu_2 = nn.LeakyReLU()\n",
    "        self.maxpool2d_2 = nn.MaxPool2d(2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(224, 64)\n",
    "        self.leakyrelu_3 = nn.LeakyReLU()\n",
    "        self.batchnorm = nn.BatchNorm1d(64)\n",
    "        self.drop = nn.Dropout(p=0.25)\n",
    "        self.fc2 = nn.Linear(64, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv2d_1(x)\n",
    "        x = self.leakyrelu_1(x)\n",
    "        x = self.maxpool2d_1(x)\n",
    "        x = self.conv2d_2(x)\n",
    "        x = self.leakyrelu_2(x)\n",
    "        x = self.maxpool2d_2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.leakyrelu_3(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.drop(x)\n",
    "        output = self.fc2(x)  \n",
    "        return output\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorFusionMixer(nn.Module):\n",
    "    def __init__(self, ModelA, ModelB, ModelC, ModelD, ModelE):\n",
    "        super().__init__()\n",
    "        self.ModelA = ModelA\n",
    "        self.ModelB = ModelB\n",
    "        self.ModelC = ModelC\n",
    "        self.ModelD = ModelD\n",
    "        self.Model_cnn_final = ModelE\n",
    "        # self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def tensor_fusion(self, batch_arr1, batch_arr2, batch_arr3):\n",
    "        fusion_matrix_lst = []\n",
    "        for i, (arr1, arr2, arr3) in enumerate(zip(batch_arr1, batch_arr2, batch_arr3)):\n",
    "            arr1 = arr1.unsqueeze(-1).unsqueeze(-1)\n",
    "            arr2 = arr2.unsqueeze(0).unsqueeze(-1)\n",
    "            arr3 = arr3.squeeze().unsqueeze(0).unsqueeze(0)\n",
    "            \n",
    "            # outer_matrix = torch.einsum('i,j,kp->ijk', arr1, arr2, arr3)\n",
    "            kron_matrix = torch.kron(torch.kron(arr1,arr2), arr3)\n",
    "            l, w, d = kron_matrix.shape\n",
    "            \n",
    "            kron_matrix = kron_matrix.view(-1, l, w, d)\n",
    "            fusion_matrix_lst.append(kron_matrix)\n",
    "            \n",
    "        fusion_matrix = torch.concat(fusion_matrix_lst)\n",
    "        # fusion_matrix = fusion_matrix.unsqueeze(-1)\n",
    "        \n",
    "        return fusion_matrix\n",
    "        \n",
    "    def forward(self, x1, x2, x3, x4):\n",
    "        x1 = self.ModelA(x1)\n",
    "        x2 = self.ModelB(x2)\n",
    "        x3 = self.ModelC(x3)\n",
    "        x4 = self.ModelD(x4)\n",
    "        \n",
    "        x5 = torch.cat([x3,x4], dim=0)\n",
    "        fusion_matrix = self.tensor_fusion(x1, x2, x5)\n",
    "        \n",
    "        output = self.Model_cnn_final(fusion_matrix) # 새로운 emotion사용\n",
    "        # output = self.softmax(x) # 기존 emotion사용\n",
    "        return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFusionMixer(\n",
      "  (ModelA): MLPNetwork_pre(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc1): Linear(in_features=61440, out_features=768, bias=True)\n",
      "    (gelu1): GELU(approximate='none')\n",
      "    (bn1): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (fc2): Linear(in_features=768, out_features=512, bias=True)\n",
      "    (gelu2): GELU(approximate='none')\n",
      "    (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (fc3): Linear(in_features=512, out_features=32, bias=True)\n",
      "    (gelu3): GELU(approximate='none')\n",
      "  )\n",
      "  (ModelB): MLPNetwork_pre(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc1): Linear(in_features=114432, out_features=768, bias=True)\n",
      "    (gelu1): GELU(approximate='none')\n",
      "    (bn1): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (fc2): Linear(in_features=768, out_features=512, bias=True)\n",
      "    (gelu2): GELU(approximate='none')\n",
      "    (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (fc3): Linear(in_features=512, out_features=32, bias=True)\n",
      "    (gelu3): GELU(approximate='none')\n",
      "  )\n",
      "  (ModelC): ConvNetwork_pre(\n",
      "    (conv1): Conv1d(50, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (relu1): ReLU()\n",
      "    (conv2): Conv1d(32, 10, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (relu2): ReLU()\n",
      "  )\n",
      "  (ModelD): ConvNetwork_pre(\n",
      "    (conv1): Conv1d(50, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (relu1): ReLU()\n",
      "    (conv2): Conv1d(32, 10, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (relu2): ReLU()\n",
      "  )\n",
      "  (Model_cnn_final): ConvNetwork_final(\n",
      "    (conv2d_1): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (leakyrelu_1): LeakyReLU(negative_slope=0.01)\n",
      "    (maxpool2d_1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv2d_2): Conv2d(64, 32, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (leakyrelu_2): LeakyReLU(negative_slope=0.01)\n",
      "    (maxpool2d_2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc1): Linear(in_features=224, out_features=64, bias=True)\n",
      "    (leakyrelu_3): LeakyReLU(negative_slope=0.01)\n",
      "    (batchnorm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (drop): Dropout(p=0.25, inplace=False)\n",
      "    (fc2): Linear(in_features=64, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "_, txt_input_length, txt_input_width = train_dataset.text_embeddings.shape\n",
    "_, wav_input_length, wav_input_width = train_dataset.wav_embeddings.shape\n",
    "temp_input_length = train_dataset.temp.shape[1]\n",
    "eda_input_length = train_dataset.eda.shape[1]\n",
    "\n",
    "# tf_mixer에 들어갈 wav mlp, txt mlp 선언\n",
    "model_mlp_txt = MLPNetwork_pre(txt_input_length,txt_input_width).to(device)\n",
    "model_mlp_wav = MLPNetwork_pre(wav_input_length,wav_input_width).to(device)\n",
    "model_conv_temp = ConvNetwork_pre(temp_input_length).to(device)\n",
    "model_conv_eda = ConvNetwork_pre(eda_input_length).to(device)\n",
    "\n",
    "model_cnn_final = ConvNetwork_final(32).to(device)\n",
    "\n",
    "# 최종 모델 선언\n",
    "model_tf_cnn_mixer = TensorFusionMixer(ModelA = model_mlp_txt, \n",
    "                                   ModelB = model_mlp_wav,\n",
    "                                   ModelC = model_conv_temp,\n",
    "                                   ModelD = model_conv_eda,\n",
    "                                   ModelE = model_cnn_final).to(device)\n",
    "\n",
    "# model 병렬 학습 처리\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model_mlp_txt = nn.DataParallel(model_mlp_txt).to(device)\n",
    "    model_mlp_wav = nn.DataParallel(model_mlp_wav).to(device)\n",
    "    model_conv_temp = nn.DataParallel(model_conv_temp).to(device)\n",
    "    model_conv_eda = nn.DataParallel(model_conv_eda).to(device)\n",
    "    model_tf_cnn_mixer = nn.DataParallel(model_tf_cnn_mixer).to(device)\n",
    "print(model_tf_cnn_mixer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CCC, self).__init__()\n",
    "        self.mean = torch.mean\n",
    "        self.var = torch.var\n",
    "        self.sum = torch.sum\n",
    "        self.sqrt = torch.sqrt\n",
    "        self.std = torch.std\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        mean_gt = self.mean (target, 0)\n",
    "        mean_pred = self.mean (pred, 0)\n",
    "        var_gt = self.var (target, 0)\n",
    "        var_pred = self.var (pred, 0)\n",
    "        v_pred = pred - mean_pred\n",
    "        v_gt = target - mean_gt\n",
    "        cor = self.sum (v_pred * v_gt) / (self.sqrt(self.sum(v_pred ** 2)) * self.sqrt(self.sum(v_gt ** 2)))\n",
    "        sd_gt = self.std(target)\n",
    "        sd_pred = self.std(pred)\n",
    "        numerator = 2 * cor * sd_gt * sd_pred\n",
    "        denominator = var_gt + var_pred + (mean_gt-mean_pred) ** 2\n",
    "        ccc = numerator / denominator\n",
    "        return ccc\n",
    "    \n",
    "ccc = CCC()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습을 위한 train, test method 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)    \n",
    "    # data 순서: text_embeddings, wav_embeddings, temp, eda, label_emotion, label_emotion_ext, label_arousal, label_valence\n",
    "    for batch, (X_txt, X_wav, X_temp, X_eda, \n",
    "                    label_emotion, label_emotion_ext, label_arousal, label_valence) in enumerate(dataloader): \n",
    "\n",
    "        # 예측 오류 계산 \n",
    "        X_txt, X_wav, X_temp, X_eda, y_v, y_a= X_txt.to(device), X_wav.to(device), X_temp.to(device), X_eda.to(device),label_valence.type(torch.float32).to(device), label_arousal.type(torch.float32).to(device)\n",
    "        \n",
    "        X_temp = X_temp.unsqueeze(dim=-1)\n",
    "        X_eda = X_eda.unsqueeze(dim=-1)\n",
    "        \n",
    "        pred = model(X_txt, X_wav, X_temp, X_eda)\n",
    "    \n",
    "        pred_v = pred[:,0]\n",
    "        pred_a = pred[:,1]\n",
    "        \n",
    "        loss_a = loss_fn(pred_a, y_a)\n",
    "        loss_v = loss_fn(pred_v, y_v)\n",
    "\n",
    "        pred_ccc_a = ccc(pred_a, y_a)\n",
    "        pred_ccc_v= ccc(pred_v, y_v)\n",
    "        \n",
    "        ccc_mean = (pred_ccc_a + pred_ccc_v) / 2\n",
    "\n",
    "        # 역전파\n",
    "        optimizer.zero_grad()\n",
    "        loss_a.backward(retain_graph = True)\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss_a, loss_v, current = loss_a.item(), loss_v.item(), batch * len(X_txt)\n",
    "            print(f\"loss_a: {loss_a:>7f}, loss_b: {loss_v:>7f},  [{current:>5d}/{size:>5d}]\")\n",
    "            print(f\"ccc_mean : {ccc_mean:>9f}, Arousal_ccc : {pred_ccc_a:>9f}, Valence_ccc : {pred_ccc_v:>9f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn, mode = 'test'):\n",
    "    \n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss_a = 0\n",
    "    test_loss_v = 0\n",
    "    ccc_mean = 0\n",
    "    ccc_a = 0\n",
    "    ccc_v = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # data 순서: text_embeddings, wav_embeddings, temp, eda, label_emotion, label_emotion_ext, label_arousal, label_valence\n",
    "        for batch, (X_txt, X_wav, X_temp, X_eda, \n",
    "                    label_emotion, label_emotion_ext, label_arousal, label_valence) in enumerate(dataloader): \n",
    "\n",
    "            # 예측 오류 계산 \n",
    "            X_txt, X_wav, X_temp, X_eda, y_v, y_a= X_txt.to(device), X_wav.to(device), X_temp.to(device), X_eda.to(device),label_valence.type(torch.float32).to(device), label_arousal.type(torch.float32).to(device)\n",
    "            \n",
    "            X_temp = X_temp.unsqueeze(dim=-1)\n",
    "            X_eda = X_eda.unsqueeze(dim=-1)\n",
    "        \n",
    "            \n",
    "            pred = model(X_txt, X_wav, X_temp, X_eda)\n",
    "            preds.append(pred.argmax(1)) # multi regression후 classification으로 변환할 경우\n",
    "            # preds.append(pred)# 바로 multiclassification할 경우\n",
    "            targets.append(label_emotion) # classification을 할 경우 언제나 사용\n",
    "            print('예측라벨분포:',pred[:2], '정답라벨 분포:', label_emotion_ext[:2], '예측정답:', pred.argmax(1)[:2],'정답:', label_emotion[:2])\n",
    "            # https://discuss.pytorch.org/t/loss-backward-raises-error-grad-can-be-implicitly-created-only-for-scalar-outputs/12152/6\n",
    "            test_loss += loss_fn(pred, y).mean().item()# weighted MSE를 사용할 경우 중간에 sum() or mean()을 넣어줌 \n",
    "            \n",
    "            correct += (pred.argmax(1) == label_emotion.to(device)).type(torch.float).sum().item()\n",
    "            \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    if mode == 'test':\n",
    "        print(torch.cat(preds), torch.cat(preds).shape)\n",
    "        print(\"f1 score: \", f1(torch.cat(preds).to(device), torch.cat(targets).to(device)))\n",
    "        print(f\"Test Error: Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\\n\")\n",
    "    elif mode == 'val':\n",
    "        print(f\"Validation Error: Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1186, 0.9287, 0.9913, 0.9900, 0.9780, 0.9940, 0.9993])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weight 계산\n",
    "single_emotion = [0,1,2,4,9,10,12]\n",
    "total_obs = 0\n",
    "for i in single_emotion:\n",
    "    total_obs += Counter(merged_dataset['Emotion'])[i]\n",
    "total_obs\n",
    "# weigted loss for imbalance data: https://naadispeaks.wordpress.com/2021/07/31/handling-imbalanced-classes-with-weighted-loss-in-pytorch/\n",
    "weight_for_class = []\n",
    "Counter(merged_dataset['Emotion'])\n",
    "for idx, value in sorted(Counter(merged_dataset['Emotion']).items()):\n",
    "    if idx in [0,1,2,4,9,10,12]:\n",
    "        weight_for_class.append(1 - (value/total_obs))\n",
    "weight_for_class = torch.Tensor(weight_for_class)\n",
    "weight_for_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class weighted_MSELoss(nn.Module):\n",
    "    def __init__(self, weight):\n",
    "        super().__init__()\n",
    "        self.weight = weight.to(device)\n",
    "    def forward(self,inputs,targets):\n",
    "        return ((inputs - targets)**2) * self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 지난 저장한 모델이 있다면\n",
    "# PATH = './data/test_model.pkl'\n",
    "# model_tf_mixer = torch.load(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = nn.CrossEntropyLoss(weight=weight_for_class).to(device)\n",
    "# loss_fn = nn.CrossEntropyLoss().to(device) # weigth를 주기위해 위의 loss로 임시 변경\n",
    "loss_fn = weighted_MSELoss(weight = weight_for_class).to(device) # multi target regression(감정별로 count 한 타겟)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "\n",
    "# optimizer = optim.SGD(model_tf_mixer.parameters(), lr=lr) # classification\n",
    "optimizer = optim.Adagrad(model_tf_mixer.parameters(), lr=lr) # regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Epoch 1----------------\n",
      "loss: 2.507085  [    0/ 1084]\n",
      "예측라벨분포: tensor([[-0.0185,  0.0831, -0.0551,  0.0355,  0.0652, -0.1144, -0.0718],\n",
      "        [ 0.0048,  0.1884, -0.0386,  0.0023, -0.0235, -0.0992, -0.0574]],\n",
      "       device='cuda:0') 정답라벨 분포: tensor([[ 4.,  0.,  2.,  2.,  2.,  0.,  0.],\n",
      "        [10.,  0.,  0.,  0.,  0.,  0.,  0.]]) 예측정답: tensor([1, 1], device='cuda:0') 정답: tensor([0., 0.], dtype=torch.float64)\n",
      "예측라벨분포: tensor([[-0.0159,  0.0987, -0.0446,  0.0345, -0.0326, -0.0902, -0.0738],\n",
      "        [-0.0192,  0.0900, -0.0233,  0.0551,  0.0039, -0.1085, -0.0822]],\n",
      "       device='cuda:0') 정답라벨 분포: tensor([[6., 0., 0., 1., 3., 0., 0.],\n",
      "        [3., 7., 0., 0., 0., 0., 0.]]) 예측정답: tensor([1, 1], device='cuda:0') 정답: tensor([0., 1.])\n",
      "예측라벨분포: tensor([[ 0.0168,  0.1094, -0.0552,  0.0126, -0.0218, -0.1104, -0.0788],\n",
      "        [ 0.0051,  0.0562, -0.0492,  0.0153, -0.0418, -0.1013, -0.0996]],\n",
      "       device='cuda:0') 정답라벨 분포: tensor([[ 9.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  0.,  0.,  0.,  0.,  0.,  0.]]) 예측정답: tensor([1, 1], device='cuda:0') 정답: tensor([0., 0.])\n",
      "Validation Error: Accuracy: 6.9%, Avg loss: 2.193210 \n",
      "\n",
      "---------------Epoch 2----------------\n",
      "loss: 2.294038  [    0/ 1084]\n"
     ]
    }
   ],
   "source": [
    "# Set the Training Parameters\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    print(f\"---------------Epoch {epoch+1}----------------\")\n",
    "    train(train_dataloader, model_tf_mixer, loss_fn, optimizer)\n",
    "    test(validation_dataloader, model_tf_mixer, loss_fn, mode = 'val')\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실험을 위해 모델 저장\n",
    "PATH = './data/test_model_multilabelregression.pkl'\n",
    "torch.save(model_tf_mixer, PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic TensorFusionNet 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측라벨분포: tensor([[-0.0006,  0.1168, -0.0863,  0.0128,  0.0321,  0.0305, -0.0390],\n",
      "        [-0.0006,  0.1168, -0.0862,  0.0128,  0.0321,  0.0305, -0.0390]],\n",
      "       device='cuda:0') 정답라벨 분포: tensor([[4., 3., 3., 0., 0., 0., 0.],\n",
      "        [9., 1., 0., 0., 0., 0., 0.]]) 예측정답: tensor([1, 1], device='cuda:0') 정답: tensor([0., 0.])\n",
      "예측라벨분포: tensor([[-0.0006,  0.1168, -0.0862,  0.0128,  0.0321,  0.0305, -0.0390],\n",
      "        [-0.0006,  0.1168, -0.0862,  0.0128,  0.0321,  0.0305, -0.0390]],\n",
      "       device='cuda:0') 정답라벨 분포: tensor([[9., 0., 1., 0., 0., 0., 0.],\n",
      "        [8., 0., 0., 0., 2., 0., 0.]]) 예측정답: tensor([1, 1], device='cuda:0') 정답: tensor([0., 0.])\n",
      "예측라벨분포: tensor([[-0.0006,  0.1168, -0.0862,  0.0128,  0.0321,  0.0305, -0.0390],\n",
      "        [-0.0006,  0.1168, -0.0862,  0.0128,  0.0321,  0.0305, -0.0390]],\n",
      "       device='cuda:0') 정답라벨 분포: tensor([[6., 0., 0., 4., 0., 0., 0.],\n",
      "        [1., 9., 0., 0., 0., 0., 0.]]) 예측정답: tensor([1, 1], device='cuda:0') 정답: tensor([0., 1.], dtype=torch.float64)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       device='cuda:0') torch.Size([192])\n",
      "f1 score:  tensor(0.0885, device='cuda:0')\n",
      "Test Error: Accuracy: 7.3%, Avg loss: 2.411374\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(test_dataloader, model_tf_mixer, loss_fn, mode = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0372, -0.1859,  0.3214,  ..., -0.5023, -0.2907, -0.5705],\n",
      "         [-0.3403, -0.6411,  0.2710,  ..., -0.4994,  0.1688, -0.6985],\n",
      "         [-0.2038,  0.1151,  0.3303,  ...,  0.0532,  0.0731, -0.7255],\n",
      "         ...,\n",
      "         [ 0.0387,  0.1966,  0.1149,  ..., -0.5742, -0.1841, -0.6823],\n",
      "         [ 0.3737,  0.0196, -0.0652,  ..., -0.3645,  0.1043, -0.9926],\n",
      "         [ 0.4129, -0.1367,  0.0095,  ..., -0.4642,  0.2714, -0.9962]],\n",
      "\n",
      "        [[ 1.3079,  0.3727,  1.6503,  ...,  0.9950, -0.2278,  1.1599],\n",
      "         [ 1.6657, -0.2183,  2.2059,  ...,  0.7725,  1.6774,  0.4149],\n",
      "         [-0.3042, -1.3660,  1.2436,  ...,  1.1886, -0.1148,  1.5758],\n",
      "         ...,\n",
      "         [ 0.9146, -0.8450,  1.8701,  ...,  0.0545, -0.1092,  1.0398],\n",
      "         [ 0.9892, -0.2653,  1.9712,  ...,  0.7016,  0.0530,  0.9379],\n",
      "         [ 0.9061, -0.3329,  1.8953,  ...,  0.7556,  0.1093,  0.9114]],\n",
      "\n",
      "        [[-0.3704, -0.2984,  0.9155,  ..., -1.2681, -0.0433,  0.2413],\n",
      "         [ 0.1247, -0.0254,  0.4202,  ..., -0.4393,  0.1123,  0.1968],\n",
      "         [-1.0448, -0.5985,  0.6934,  ..., -0.5695,  1.1614, -0.0849],\n",
      "         ...,\n",
      "         [-0.6295, -0.2794,  0.1143,  ..., -0.8961,  0.5165,  0.2536],\n",
      "         [-0.2717, -0.5565,  0.5098,  ..., -1.0587, -0.0360, -0.1115],\n",
      "         [-0.4691, -0.6067,  0.6975,  ..., -0.9010, -0.1648,  0.0236]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.3711,  0.6465,  0.2282,  ...,  0.1984,  0.3917,  2.0942],\n",
      "         [-0.3295, -0.2967,  0.2349,  ..., -0.5635,  0.5554,  1.1205],\n",
      "         [-0.4232,  0.6536,  0.8567,  ..., -0.2254,  1.1812,  1.4104],\n",
      "         ...,\n",
      "         [ 0.4545,  0.4273,  0.3971,  ..., -0.2936,  0.3598,  2.0109],\n",
      "         [ 0.3698,  1.0537,  0.3064,  ...,  0.0722,  0.5218,  1.6172],\n",
      "         [-0.0170,  1.2301,  0.0804,  ..., -0.0447,  0.2919,  1.2817]],\n",
      "\n",
      "        [[-1.5291, -0.8282,  0.1631,  ..., -0.3010, -0.1365,  1.0752],\n",
      "         [-1.4284, -0.3882,  0.3067,  ..., -0.3796, -0.1911,  0.2225],\n",
      "         [-2.6898, -0.8852,  0.2476,  ...,  0.5852, -0.2555,  1.3906],\n",
      "         ...,\n",
      "         [-1.6336, -0.6777,  0.6971,  ..., -0.2678, -0.1649,  0.3367],\n",
      "         [-1.4160, -0.6439,  0.5943,  ..., -0.2081, -0.0875,  0.2894],\n",
      "         [-1.4706, -0.3374,  0.6136,  ..., -0.3026, -0.0718,  0.1942]],\n",
      "\n",
      "        [[ 1.4557, -2.8615,  0.1629,  ..., -0.6511, -0.0127,  0.0118],\n",
      "         [ 0.2037, -1.1833,  0.7604,  ..., -1.3256,  0.9924,  0.2238],\n",
      "         [-0.9512, -1.3324, -0.5775,  ...,  0.0492,  0.3346,  1.1628],\n",
      "         ...,\n",
      "         [ 1.1983, -2.8069,  0.1824,  ..., -0.8388,  0.0659, -0.0033],\n",
      "         [ 1.0988, -2.6560,  0.0392,  ..., -0.5585,  0.1897, -0.2910],\n",
      "         [ 1.4445, -2.5254, -0.0109,  ..., -1.0336, -0.2785, -0.1369]]],\n",
      "       grad_fn=<StackBackward0>) tensor([[[[-9.0357e-01,  5.2524e-02, -6.6244e-02,  ...,  1.1921e-01,\n",
      "            1.1450e-01,  7.6236e-02],\n",
      "          [-9.0357e-01,  5.2524e-02, -6.6244e-02,  ...,  1.1921e-01,\n",
      "            1.1450e-01,  7.6236e-02],\n",
      "          [-9.0357e-01,  5.2524e-02, -6.6244e-02,  ...,  1.1921e-01,\n",
      "            1.1450e-01,  7.6236e-02],\n",
      "          ...,\n",
      "          [-1.1249e+00,  8.3771e-02,  1.0080e-01,  ..., -2.2124e-02,\n",
      "           -1.3009e-01,  1.4258e-01],\n",
      "          [-1.0617e+00,  8.3941e-02,  1.0207e-01,  ..., -4.3921e-02,\n",
      "           -1.2729e-01,  1.1116e-01],\n",
      "          [-1.0476e+00,  8.9427e-02,  1.0727e-01,  ..., -4.9117e-02,\n",
      "           -1.2314e-01,  1.0688e-01]]],\n",
      "\n",
      "\n",
      "        [[[-9.1485e-01,  9.7829e-02, -2.8695e-01,  ...,  3.6580e-02,\n",
      "            9.3810e-02,  1.4225e-01],\n",
      "          [-9.1485e-01,  9.7830e-02, -2.8695e-01,  ...,  3.6580e-02,\n",
      "            9.3810e-02,  1.4225e-01],\n",
      "          [-9.1485e-01,  9.7830e-02, -2.8695e-01,  ...,  3.6580e-02,\n",
      "            9.3809e-02,  1.4225e-01],\n",
      "          ...,\n",
      "          [-1.0800e+00,  3.3230e-02,  6.2465e-02,  ..., -8.3610e-02,\n",
      "           -1.2309e-01,  1.7907e-01],\n",
      "          [-1.0278e+00,  2.8668e-02,  3.0407e-02,  ..., -8.9343e-02,\n",
      "           -1.3253e-01,  1.3251e-01],\n",
      "          [-1.0329e+00,  3.0989e-02,  2.8338e-02,  ..., -9.0294e-02,\n",
      "           -1.2358e-01,  1.3126e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.0149e+00,  2.5042e-01, -3.6062e-01,  ...,  5.4453e-02,\n",
      "            8.9558e-03,  2.1962e-01],\n",
      "          [-1.0149e+00,  2.5042e-01, -3.6062e-01,  ...,  5.4454e-02,\n",
      "            8.9560e-03,  2.1962e-01],\n",
      "          [-1.0149e+00,  2.5042e-01, -3.6062e-01,  ...,  5.4454e-02,\n",
      "            8.9559e-03,  2.1962e-01],\n",
      "          ...,\n",
      "          [-1.2202e+00,  3.7870e-02,  1.2962e-02,  ..., -7.2869e-02,\n",
      "           -1.5564e-01,  1.1701e-01],\n",
      "          [-1.2198e+00,  2.3194e-02,  1.5494e-02,  ..., -7.7052e-02,\n",
      "           -1.8145e-01,  1.0742e-01],\n",
      "          [-1.2384e+00,  6.6758e-02,  2.9554e-02,  ..., -8.2631e-02,\n",
      "           -1.8847e-01,  1.3013e-01]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-9.3850e-01,  2.2315e-01, -2.2688e-01,  ...,  1.2685e-01,\n",
      "           -6.1108e-02,  1.2668e-01],\n",
      "          [-9.3850e-01,  2.2315e-01, -2.2688e-01,  ...,  1.2685e-01,\n",
      "           -6.1108e-02,  1.2668e-01],\n",
      "          [-9.3850e-01,  2.2315e-01, -2.2688e-01,  ...,  1.2685e-01,\n",
      "           -6.1108e-02,  1.2668e-01],\n",
      "          ...,\n",
      "          [-1.0077e+00,  8.6026e-02,  7.5491e-03,  ..., -4.8786e-02,\n",
      "           -1.5376e-01,  1.5788e-01],\n",
      "          [-1.0043e+00,  8.1126e-02,  1.3322e-02,  ..., -6.0555e-02,\n",
      "           -1.3302e-01,  1.3763e-01],\n",
      "          [-1.0641e+00,  3.8397e-03,  1.6495e-02,  ...,  9.8441e-02,\n",
      "           -1.2825e-01,  1.8190e-01]]],\n",
      "\n",
      "\n",
      "        [[[-9.2240e-01,  1.7208e-01, -2.8582e-01,  ..., -2.0010e-03,\n",
      "            5.7543e-02,  1.0423e-01],\n",
      "          [-9.2240e-01,  1.7208e-01, -2.8582e-01,  ..., -2.0007e-03,\n",
      "            5.7543e-02,  1.0423e-01],\n",
      "          [-9.2240e-01,  1.7208e-01, -2.8582e-01,  ..., -2.0010e-03,\n",
      "            5.7543e-02,  1.0423e-01],\n",
      "          ...,\n",
      "          [-1.0281e+00, -1.1472e-01,  1.0541e-01,  ..., -1.1175e-02,\n",
      "           -6.2159e-02,  1.6739e-01],\n",
      "          [-9.8616e-01, -9.5252e-02,  7.5651e-02,  ..., -3.8075e-02,\n",
      "           -8.7918e-02,  9.3305e-02],\n",
      "          [-9.9209e-01, -1.0069e-01,  7.7865e-02,  ..., -3.6755e-02,\n",
      "           -9.8113e-02,  9.6975e-02]]],\n",
      "\n",
      "\n",
      "        [[[-9.5599e-01,  1.0903e-01, -2.2435e-01,  ...,  6.4858e-02,\n",
      "            8.9231e-02,  9.4291e-02],\n",
      "          [-9.5599e-01,  1.0903e-01, -2.2435e-01,  ...,  6.4858e-02,\n",
      "            8.9231e-02,  9.4292e-02],\n",
      "          [-9.5599e-01,  1.0903e-01, -2.2435e-01,  ...,  6.4858e-02,\n",
      "            8.9231e-02,  9.4292e-02],\n",
      "          ...,\n",
      "          [-1.0777e+00, -5.8531e-03,  4.7329e-02,  ..., -5.7925e-02,\n",
      "           -1.7681e-01, -2.5717e-05],\n",
      "          [-1.0827e+00, -3.8433e-03,  4.1373e-02,  ..., -5.9204e-02,\n",
      "           -1.7450e-01, -3.2594e-03],\n",
      "          [-1.1058e+00,  1.6713e-03,  2.3556e-02,  ..., -5.6343e-02,\n",
      "           -1.8394e-01,  1.1554e-03]]]]) tensor([ 6, 10,  1,  1,  1,  1,  1,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         3,  1,  0,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  6,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  0])\n",
      "tensor([[[-0.3135,  0.3598, -0.3683,  ..., -1.1105,  1.0331, -0.1598],\n",
      "         [-0.3732, -0.8260, -0.1992,  ..., -0.2585,  0.7229, -0.2742],\n",
      "         [-1.2590,  0.2766, -0.8130,  ...,  0.4777,  1.4969, -0.0415],\n",
      "         ...,\n",
      "         [-0.4296,  0.2825,  0.1939,  ..., -1.1886,  1.3916, -0.4094],\n",
      "         [-0.5759,  0.5137, -0.0092,  ..., -1.0802,  1.3445, -0.3047],\n",
      "         [-0.5025,  0.2482,  0.0948,  ..., -1.1301,  1.3666, -0.3354]],\n",
      "\n",
      "        [[-0.5878, -2.1452,  1.0545,  ..., -0.4789, -1.0487, -0.4928],\n",
      "         [-0.2062, -0.7940,  0.2777,  ..., -0.6237, -0.5193,  0.4430],\n",
      "         [-1.2222, -2.0101,  0.8862,  ..., -0.1185, -0.8109, -0.3826],\n",
      "         ...,\n",
      "         [-0.4429, -1.7486,  1.0510,  ..., -0.4025, -0.7158, -0.8259],\n",
      "         [-0.1854, -1.5451,  0.8015,  ..., -0.2343, -0.4666, -0.9025],\n",
      "         [-0.1624, -1.6228,  0.7988,  ..., -0.1428, -0.7545, -0.9342]],\n",
      "\n",
      "        [[-0.4113, -1.5791,  0.4911,  ..., -0.9297, -0.2178,  1.2849],\n",
      "         [ 0.3657, -2.1271,  1.0378,  ..., -0.1303, -0.0836,  0.6600],\n",
      "         [-0.5587, -1.1231,  1.0267,  ...,  0.0122, -0.0634,  0.9482],\n",
      "         ...,\n",
      "         [ 0.0981, -1.1373,  0.4401,  ..., -0.5581,  0.2197,  1.2133],\n",
      "         [ 0.2170, -1.0613,  0.4620,  ..., -0.5551,  0.4318,  1.1486],\n",
      "         [ 0.0779, -1.0698,  0.5305,  ..., -0.4075,  0.6547,  1.1789]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1763, -0.2627,  0.3106,  ..., -0.6048,  0.1521,  2.3133],\n",
      "         [-0.3986,  0.3017,  0.2455,  ..., -0.0210, -1.3808,  1.1792],\n",
      "         [-0.4408,  0.3925,  0.1968,  ..., -0.3023,  0.8824,  0.5384],\n",
      "         ...,\n",
      "         [ 0.3231, -0.1274,  0.3375,  ..., -0.7644,  0.5329,  2.0711],\n",
      "         [ 0.4434, -0.8133,  0.4733,  ..., -0.7896,  0.6990,  1.8203],\n",
      "         [ 0.4874, -0.6437,  0.3727,  ..., -0.7441,  0.6096,  1.8981]],\n",
      "\n",
      "        [[ 0.0809,  0.3388, -0.0343,  ..., -0.4724,  0.3061, -0.2449],\n",
      "         [-0.1504, -0.0187,  0.5355,  ..., -0.5271,  0.8820, -0.2652],\n",
      "         [-0.9447, -0.3966,  0.2720,  ...,  0.0304,  0.5730,  0.2580],\n",
      "         ...,\n",
      "         [-0.0282,  0.4224,  0.3070,  ..., -0.6286,  0.2938, -0.1323],\n",
      "         [ 0.0643,  0.5432,  0.2799,  ..., -0.3650,  0.4507, -0.3414],\n",
      "         [ 0.0622,  0.5037,  0.3131,  ..., -0.5660,  0.3684, -0.1913]],\n",
      "\n",
      "        [[ 0.2948, -0.6879, -0.8523,  ...,  0.0205, -0.0964,  2.5648],\n",
      "         [ 0.2895, -0.9104, -0.3288,  ...,  0.0568,  0.1937,  1.8813],\n",
      "         [-0.5450, -1.4383, -0.6964,  ...,  0.6062,  0.0495,  2.5453],\n",
      "         ...,\n",
      "         [ 0.5464, -0.9311, -0.6468,  ..., -0.0530,  0.2616,  2.2779],\n",
      "         [ 0.3521, -0.7418, -0.9001,  ...,  0.1439,  0.1110,  2.1081],\n",
      "         [ 0.4116, -0.4202, -0.9810,  ...,  0.0376, -0.2109,  1.8367]]],\n",
      "       grad_fn=<StackBackward0>) tensor([[[[-1.0329e+00,  2.5231e-01, -2.9089e-01,  ...,  9.0860e-02,\n",
      "            6.6954e-02,  1.4964e-01],\n",
      "          [-1.0329e+00,  2.5232e-01, -2.9089e-01,  ...,  9.0860e-02,\n",
      "            6.6954e-02,  1.4964e-01],\n",
      "          [-1.0329e+00,  2.5232e-01, -2.9089e-01,  ...,  9.0860e-02,\n",
      "            6.6953e-02,  1.4964e-01],\n",
      "          ...,\n",
      "          [-1.1227e+00,  8.8297e-02,  3.0206e-02,  ..., -5.6011e-02,\n",
      "           -1.6177e-01,  1.1444e-01],\n",
      "          [-1.1144e+00,  1.0119e-01,  1.8686e-02,  ..., -5.9961e-02,\n",
      "           -1.6070e-01,  9.5456e-02],\n",
      "          [-1.1550e+00,  1.4577e-01, -1.7564e-02,  ..., -6.0713e-02,\n",
      "           -1.3913e-01,  9.9464e-02]]],\n",
      "\n",
      "\n",
      "        [[[-9.7718e-01,  1.9968e-01, -2.2850e-01,  ...,  8.1957e-02,\n",
      "            4.5339e-02,  1.3011e-01],\n",
      "          [-9.7718e-01,  1.9968e-01, -2.2850e-01,  ...,  8.1956e-02,\n",
      "            4.5340e-02,  1.3011e-01],\n",
      "          [-9.7718e-01,  1.9968e-01, -2.2850e-01,  ...,  8.1956e-02,\n",
      "            4.5340e-02,  1.3011e-01],\n",
      "          ...,\n",
      "          [-9.4685e-01, -1.5891e-02, -2.6120e-02,  ..., -1.0307e-03,\n",
      "           -1.4277e-01,  1.0436e-01],\n",
      "          [-9.3327e-01, -1.1283e-02, -2.2786e-02,  ..., -4.9326e-03,\n",
      "           -1.4858e-01,  9.5232e-02],\n",
      "          [-9.2747e-01, -1.1818e-02, -2.2289e-02,  ..., -2.7388e-03,\n",
      "           -1.4094e-01,  9.1715e-02]]],\n",
      "\n",
      "\n",
      "        [[[-8.8776e-01,  3.2313e-02, -1.7624e-01,  ...,  4.2995e-02,\n",
      "            8.7901e-02,  1.1043e-01],\n",
      "          [-8.8776e-01,  3.2313e-02, -1.7624e-01,  ...,  4.2995e-02,\n",
      "            8.7901e-02,  1.1043e-01],\n",
      "          [-8.8776e-01,  3.2313e-02, -1.7624e-01,  ...,  4.2994e-02,\n",
      "            8.7901e-02,  1.1043e-01],\n",
      "          ...,\n",
      "          [-1.0266e+00, -2.2378e-02,  1.0031e-01,  ...,  6.0849e-03,\n",
      "           -1.1864e-01,  2.0036e-01],\n",
      "          [-9.5103e-01,  6.9663e-03,  7.7167e-02,  ..., -2.7488e-02,\n",
      "           -1.2608e-01,  1.3826e-01],\n",
      "          [-9.4713e-01,  7.2052e-03,  7.1369e-02,  ..., -3.1162e-02,\n",
      "           -1.2855e-01,  1.2912e-01]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-7.8421e-01,  1.1193e-01, -1.7644e-01,  ...,  1.2812e-01,\n",
      "            1.3950e-01,  1.7572e-01],\n",
      "          [-7.8421e-01,  1.1192e-01, -1.7644e-01,  ...,  1.2812e-01,\n",
      "            1.3950e-01,  1.7572e-01],\n",
      "          [-7.8421e-01,  1.1193e-01, -1.7644e-01,  ...,  1.2812e-01,\n",
      "            1.3950e-01,  1.7572e-01],\n",
      "          ...,\n",
      "          [-1.1580e+00,  1.3777e-01,  1.9458e-02,  ..., -3.1574e-02,\n",
      "           -7.9338e-02,  2.7576e-01],\n",
      "          [-1.0837e+00,  1.2958e-01,  5.5835e-03,  ..., -3.6117e-02,\n",
      "           -1.1474e-01,  2.0381e-01],\n",
      "          [-1.0788e+00,  1.2809e-01,  4.9062e-03,  ..., -3.5082e-02,\n",
      "           -1.1225e-01,  1.9607e-01]]],\n",
      "\n",
      "\n",
      "        [[[-7.5254e-01, -8.7008e-02, -8.7270e-03,  ...,  1.1036e-01,\n",
      "            8.8711e-02,  8.0031e-02],\n",
      "          [-7.5254e-01, -8.7008e-02, -8.7268e-03,  ...,  1.1036e-01,\n",
      "            8.8711e-02,  8.0030e-02],\n",
      "          [-7.5254e-01, -8.7009e-02, -8.7267e-03,  ...,  1.1036e-01,\n",
      "            8.8711e-02,  8.0031e-02],\n",
      "          ...,\n",
      "          [-1.0529e+00,  6.3521e-02,  9.9113e-02,  ...,  5.5668e-03,\n",
      "           -7.0969e-02,  1.1818e-01],\n",
      "          [-9.4167e-01,  3.5350e-02,  8.9927e-02,  ..., -1.1265e-04,\n",
      "           -8.4683e-02,  7.8517e-02],\n",
      "          [-1.2442e+00,  1.8088e-01,  6.0636e-02,  ...,  2.8081e-02,\n",
      "           -7.1830e-02,  1.5234e-01]]],\n",
      "\n",
      "\n",
      "        [[[-6.6984e-01, -1.0658e-01, -8.8424e-02,  ...,  1.2087e-01,\n",
      "            1.3490e-01,  2.2543e-01],\n",
      "          [-6.6984e-01, -1.0658e-01, -8.8423e-02,  ...,  1.2087e-01,\n",
      "            1.3490e-01,  2.2543e-01],\n",
      "          [-6.6984e-01, -1.0658e-01, -8.8423e-02,  ...,  1.2087e-01,\n",
      "            1.3490e-01,  2.2543e-01],\n",
      "          ...,\n",
      "          [-8.8718e-01,  1.0789e-01,  2.1709e-02,  ..., -4.2100e-02,\n",
      "           -1.7743e-01,  1.2572e-01],\n",
      "          [-8.8560e-01,  1.0437e-01,  2.0336e-02,  ..., -3.9385e-02,\n",
      "           -1.7578e-01,  1.1992e-01],\n",
      "          [-8.8841e-01,  1.0582e-01,  1.3285e-02,  ..., -3.6943e-02,\n",
      "           -1.7130e-01,  1.1547e-01]]]]) tensor([1, 1, 1, 7, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 6,\n",
      "        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1])\n",
      "tensor([[[ 0.2706, -2.0748,  0.2613,  ...,  0.9831,  1.2262,  1.1181],\n",
      "         [ 0.0071, -2.6216,  0.4937,  ...,  0.3073,  0.9557,  0.2179],\n",
      "         [ 1.1437, -1.4736,  0.3390,  ...,  0.3786,  1.3776,  0.5759],\n",
      "         ...,\n",
      "         [ 0.5352, -2.0011,  0.3887,  ...,  0.7555,  1.0884,  0.8362],\n",
      "         [ 0.6496, -1.7971,  0.2106,  ...,  0.9879,  1.1381,  0.8817],\n",
      "         [ 0.3137, -1.7771,  0.4166,  ...,  0.8690,  1.3066,  0.6907]],\n",
      "\n",
      "        [[ 0.3743, -0.6669,  0.3080,  ...,  0.3204, -0.3125, -0.5539],\n",
      "         [-0.3305, -0.8900, -0.1327,  ...,  0.5147,  0.1339,  0.2328],\n",
      "         [ 0.4230, -1.7240,  0.8676,  ...,  0.3009,  0.2500, -0.7881],\n",
      "         ...,\n",
      "         [ 0.2675, -1.2558,  0.4221,  ...,  0.4307, -0.4193, -0.7433],\n",
      "         [ 0.3531, -1.0410,  0.4476,  ...,  0.4841, -0.3353, -0.7836],\n",
      "         [ 0.3426, -1.1929,  0.3246,  ...,  0.1969, -0.2609, -0.6880]],\n",
      "\n",
      "        [[ 0.1577, -1.1772, -0.9367,  ...,  0.1546,  0.3775,  1.0188],\n",
      "         [ 0.7773,  0.1587, -0.8758,  ...,  0.2747,  0.7025,  0.4548],\n",
      "         [ 0.0934,  0.4195, -0.6326,  ...,  0.4970,  0.1487,  0.0999],\n",
      "         ...,\n",
      "         [-0.0077, -1.4304, -0.6231,  ...,  0.5612,  0.6655,  0.7615],\n",
      "         [-0.0530, -1.5044, -0.6926,  ...,  0.4903,  0.6846,  0.6522],\n",
      "         [-0.0241, -1.5696, -0.8064,  ...,  0.2045,  0.8276,  0.4099]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.8192, -0.5192,  0.6772,  ..., -0.9680,  0.2846,  1.0216],\n",
      "         [-0.8512, -0.4042,  0.7272,  ..., -1.0851, -0.5025,  0.3903],\n",
      "         [-1.6228, -0.3467,  1.5863,  ..., -0.3701,  0.3190,  0.8019],\n",
      "         ...,\n",
      "         [-0.4316, -0.5323,  0.8392,  ..., -0.9896,  1.0736,  0.6585],\n",
      "         [-0.1718, -0.2727,  0.6835,  ..., -0.8452,  1.4791,  0.5926],\n",
      "         [ 0.0358, -0.6472,  0.5381,  ..., -0.9594,  1.3403,  0.7104]],\n",
      "\n",
      "        [[ 0.0226, -0.6828,  0.9751,  ...,  0.2089, -1.2605, -0.1886],\n",
      "         [ 1.0976, -0.0049,  1.4372,  ...,  0.5795,  0.3054, -0.0175],\n",
      "         [ 0.9599,  1.9136,  0.8325,  ..., -0.1982,  0.3414,  0.3382],\n",
      "         ...,\n",
      "         [ 0.4563, -0.6131,  1.2568,  ...,  0.2645, -0.9909, -0.3038],\n",
      "         [ 0.4799, -0.5580,  1.1645,  ...,  0.1489, -0.8224, -0.4190],\n",
      "         [ 0.4545, -0.4306,  0.9884,  ...,  0.3384, -0.7370, -0.5919]],\n",
      "\n",
      "        [[ 1.6393,  0.8180,  1.3603,  ...,  0.9350, -0.5416,  0.0091],\n",
      "         [ 1.2747,  1.4548,  1.4296,  ...,  0.8422, -0.0632, -0.1956],\n",
      "         [ 0.5959,  0.7922,  0.4096,  ...,  1.1938, -1.1099,  0.0258],\n",
      "         ...,\n",
      "         [ 1.6985,  0.6637,  1.7474,  ...,  0.8837, -0.2234, -0.1947],\n",
      "         [ 1.7179,  0.7289,  1.6155,  ...,  1.0632, -0.1045, -0.0673],\n",
      "         [ 2.0439,  0.7932,  1.4941,  ...,  0.9373,  0.0464, -0.4613]]],\n",
      "       grad_fn=<StackBackward0>) tensor([[[[-0.9258,  0.0477, -0.0934,  ...,  0.1574,  0.0855,  0.1213],\n",
      "          [-0.9258,  0.0477, -0.0934,  ...,  0.1574,  0.0855,  0.1213],\n",
      "          [-0.9258,  0.0477, -0.0934,  ...,  0.1574,  0.0855,  0.1213],\n",
      "          ...,\n",
      "          [-1.1969,  0.1004,  0.0448,  ...,  0.0180, -0.1186,  0.1233],\n",
      "          [-1.1979,  0.1208,  0.0489,  ...,  0.0030, -0.1206,  0.1028],\n",
      "          [-1.2425,  0.1593,  0.0499,  ...,  0.0337, -0.1108,  0.1553]]],\n",
      "\n",
      "\n",
      "        [[[-1.2253,  0.1420, -0.1555,  ...,  0.0450,  0.0184,  0.1162],\n",
      "          [-1.2253,  0.1420, -0.1555,  ...,  0.0450,  0.0184,  0.1162],\n",
      "          [-1.2253,  0.1420, -0.1555,  ...,  0.0450,  0.0184,  0.1162],\n",
      "          ...,\n",
      "          [-1.3102,  0.1529, -0.0142,  ..., -0.1751, -0.2144,  0.0955],\n",
      "          [-1.3104,  0.1530, -0.0139,  ..., -0.1756, -0.2127,  0.0965],\n",
      "          [-1.3116,  0.1532, -0.0149,  ..., -0.1759, -0.2125,  0.0963]]],\n",
      "\n",
      "\n",
      "        [[[-0.8177,  0.1674, -0.2098,  ...,  0.1564,  0.0772,  0.1086],\n",
      "          [-0.8177,  0.1674, -0.2098,  ...,  0.1564,  0.0772,  0.1086],\n",
      "          [-0.8177,  0.1674, -0.2098,  ...,  0.1564,  0.0772,  0.1086],\n",
      "          ...,\n",
      "          [-0.8709, -0.0986,  0.0251,  ..., -0.0527, -0.1465,  0.1116],\n",
      "          [-0.8696, -0.1008,  0.0227,  ..., -0.0548, -0.1454,  0.1082],\n",
      "          [-0.8992, -0.1077,  0.0168,  ..., -0.0444, -0.1509,  0.1239]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.6363, -0.0621, -0.0562,  ...,  0.1645,  0.1488,  0.0590],\n",
      "          [-0.6363, -0.0621, -0.0562,  ...,  0.1645,  0.1488,  0.0590],\n",
      "          [-0.6363, -0.0621, -0.0562,  ...,  0.1645,  0.1488,  0.0590],\n",
      "          ...,\n",
      "          [-0.9920,  0.1409,  0.0267,  ..., -0.0613, -0.0972,  0.1257],\n",
      "          [-0.9585,  0.1190,  0.0240,  ..., -0.0635, -0.0993,  0.1077],\n",
      "          [-0.9534,  0.1134,  0.0204,  ..., -0.0633, -0.0953,  0.1015]]],\n",
      "\n",
      "\n",
      "        [[[-0.9750,  0.2988, -0.3390,  ...,  0.0466, -0.0477,  0.1502],\n",
      "          [-0.9750,  0.2988, -0.3390,  ...,  0.0466, -0.0477,  0.1502],\n",
      "          [-0.9750,  0.2988, -0.3390,  ...,  0.0466, -0.0477,  0.1502],\n",
      "          ...,\n",
      "          [-1.0560,  0.1591,  0.0230,  ..., -0.1238, -0.1589,  0.1883],\n",
      "          [-1.0549,  0.1518,  0.0205,  ..., -0.1230, -0.1753,  0.1804],\n",
      "          [-1.0502,  0.1404,  0.0174,  ..., -0.1211, -0.1989,  0.1731]]],\n",
      "\n",
      "\n",
      "        [[[-1.2709,  0.0327, -0.0804,  ...,  0.0564,  0.0618,  0.1136],\n",
      "          [-1.2709,  0.0327, -0.0804,  ...,  0.0564,  0.0618,  0.1136],\n",
      "          [-1.2709,  0.0327, -0.0804,  ...,  0.0564,  0.0618,  0.1136],\n",
      "          ...,\n",
      "          [-1.3558,  0.1184,  0.0471,  ..., -0.0375, -0.1273,  0.1439],\n",
      "          [-1.3506,  0.0862, -0.0512,  ...,  0.0142,  0.0152,  0.1259],\n",
      "          [-1.4284,  0.1153, -0.0139,  ...,  0.0112, -0.0878,  0.1349]]]]) tensor([ 1,  1,  1,  1,  1,  1,  0,  0,  1,  1,  1,  1,  1,  0,  1,  1,  1,  1,\n",
      "         1,  1,  1,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0,  1, 13,  1,\n",
      "         1,  1,  0,  1,  1,  1,  7,  1, 12,  1,  7,  1,  4,  1,  9,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  6,  4])\n"
     ]
    }
   ],
   "source": [
    "for batch, (X_txt, X_wav, X_temp, X_eda, \n",
    "                        label_emotion, label_emotion_ext, label_arousal, label_valence) in list(enumerate(test_dataloader))[:4]:\n",
    "    print(X_txt,X_wav,label_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = model_tf_mixer(X_txt.to(device), X_wav.to(device))\n",
    "for i in torch.argmax(probs, dim=1):\n",
    "    if decode_dict[int(i)] != 'neutral':\n",
    "        print(decode_dict[int(i)])\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Model for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 10, 14, 15, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 34, 36, 37, 38, 40] \n",
      " [9, 11, 13, 16, 29, 33, 35, 39]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ts_session_index = list(range(1,40+1))\n",
    "# ts data가 없는 session 추출대상 index에서 제외\n",
    "ts_session_index.remove(12) \n",
    "ts_session_index.remove(17)\n",
    "\n",
    "# 80% random하게 train으로 추출, 20%를 테스트로 추출\n",
    "ts_session_index_train = random.sample(ts_session_index, k = int(38*.8))\n",
    "ts_session_index_train = sorted(ts_session_index_train)\n",
    "ts_session_index_test = [i for i in ts_session_index if i not in ts_session_index_train]\n",
    "ts_session_index_test = sorted(ts_session_index_test)\n",
    "print(ts_session_index_train, '\\n',ts_session_index_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>segment_id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>valence</th>\n",
       "      <th>arousal</th>\n",
       "      <th>eda</th>\n",
       "      <th>temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sess01_script01_User002M_001</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>[2.856493, 2.788578, 2.678377, 2.652749, 2.645...</td>\n",
       "      <td>[34.81, 34.81, 34.81, 34.79, 34.79, 34.79, 34....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sess01_script01_User002M_002</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3.1</td>\n",
       "      <td>2.9</td>\n",
       "      <td>[2.647035, 2.653442, 2.631658, 2.614999, 2.623...</td>\n",
       "      <td>[34.79, 34.79, 34.79, 34.77, 34.77, 34.77, 34....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sess01_script01_User002M_003</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3</td>\n",
       "      <td>[2.896217, 2.975664, 3.02692, 3.071769, 3.0922...</td>\n",
       "      <td>[34.75, 34.75, 34.75, 34.75, 34.79, 34.79, 34....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sess01_script01_User002M_004</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>[3.689019, 3.692863, 3.659546, 3.409672, 3.123...</td>\n",
       "      <td>[34.77, 34.77, 34.77, 34.77, 34.77, 34.77, 34....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sess01_script01_User001F_001</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3.8</td>\n",
       "      <td>2.8</td>\n",
       "      <td>[4.412055, 4.536246, 4.630991, 4.669401, 4.661...</td>\n",
       "      <td>[34.55, 34.55, 34.55, 34.55, 34.55, 34.55, 34....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     segment_id  emotion valence arousal  \\\n",
       "0  Sess01_script01_User002M_001  neutral     3.4     2.9   \n",
       "1  Sess01_script01_User002M_002  neutral     3.1     2.9   \n",
       "2  Sess01_script01_User002M_003  neutral     3.1       3   \n",
       "3  Sess01_script01_User002M_004  neutral     3.7     3.1   \n",
       "4  Sess01_script01_User001F_001  neutral     3.8     2.8   \n",
       "\n",
       "                                                 eda  \\\n",
       "0  [2.856493, 2.788578, 2.678377, 2.652749, 2.645...   \n",
       "1  [2.647035, 2.653442, 2.631658, 2.614999, 2.623...   \n",
       "2  [2.896217, 2.975664, 3.02692, 3.071769, 3.0922...   \n",
       "3  [3.689019, 3.692863, 3.659546, 3.409672, 3.123...   \n",
       "4  [4.412055, 4.536246, 4.630991, 4.669401, 4.661...   \n",
       "\n",
       "                                                temp  \n",
       "0  [34.81, 34.81, 34.81, 34.79, 34.79, 34.79, 34....  \n",
       "1  [34.79, 34.79, 34.79, 34.77, 34.77, 34.77, 34....  \n",
       "2  [34.75, 34.75, 34.75, 34.75, 34.79, 34.79, 34....  \n",
       "3  [34.77, 34.77, 34.77, 34.77, 34.77, 34.77, 34....  \n",
       "4  [34.55, 34.55, 34.55, 34.55, 34.55, 34.55, 34....  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_dataset[1].head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bdc1fd12ca460d5768d71e9df3d9063ef832ce64a62e55a1a523c8c99752868e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
