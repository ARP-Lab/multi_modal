{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# working model for tensorfusion\n",
    "\n",
    "1. 해당 Note는 EDA와 TEMP 데이터를 제외한 Wav와 Text 데이터만을 사용했습니다.\n",
    "\n",
    "2. Tensorfusion은 kronecker product로 하였고 이후 Conv1D를 달았습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu117\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from torchmetrics import F1Score\n",
    "from torchmetrics.classification import MulticlassPrecision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from datasets import Dataset\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "import random\n",
    "from torchsummary import summary as summary\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wav, text데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../../paradeigma/multi_modal/model/data/paradeigma_KEMDY19_annotation_nonmissing.pkl',\n",
       " '../../../paradeigma/multi_modal/model/data/paradeigma_KEMDY19_embedding_for_dataset.pkl',\n",
       " '../../../paradeigma/multi_modal/model/data/paradeigma_KEMDY20_annotation_nonmissing.pkl',\n",
       " '../../../paradeigma/multi_modal/model/data/paradeigma_KEMDY20_embedding_for_dataset.pkl']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wav and text data load\n",
    "# kemdy19, kemdy20의 데이터 정리 + 임베딩 데이터 불러오기 \n",
    "dataset_file_lst = ['../../../paradeigma/multi_modal/model/data/paradeigma_KEMDY19_annotation_nonmissing.pkl',\n",
    "                    '../../../paradeigma/multi_modal/model/data/paradeigma_KEMDY19_embedding_for_dataset.pkl',\n",
    "                    '../../../paradeigma/multi_modal/model/data/paradeigma_KEMDY20_annotation_nonmissing.pkl',\n",
    "                    '../../../paradeigma/multi_modal/model/data/paradeigma_KEMDY20_embedding_for_dataset.pkl']\n",
    "dataset_file_lst = sorted(dataset_file_lst)\n",
    "dataset_file_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../paradeigma/multi_modal/model/data/paradeigma_KEMDY19_annotation_nonmissing.pkl kemdy19_annot\n",
      "../../../paradeigma/multi_modal/model/data/paradeigma_KEMDY19_embedding_for_dataset.pkl kemdy19_emb\n",
      "../../../paradeigma/multi_modal/model/data/paradeigma_KEMDY20_annotation_nonmissing.pkl kemdy20_annot\n",
      "../../../paradeigma/multi_modal/model/data/paradeigma_KEMDY20_embedding_for_dataset.pkl kemdy20_emb\n"
     ]
    }
   ],
   "source": [
    "var_names = ['kemdy19_annot', 'kemdy19_emb', 'kemdy20_annot', 'kemdy20_emb']\n",
    "for file, var_name in zip(dataset_file_lst, var_names):\n",
    "    print(file, var_name)\n",
    "    with open(file, 'rb') as f:\n",
    "        globals()[var_name] = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<2020년>\n",
    "'angry;disqust',  'angry;disqust;fear;neutral;sad',  'angry;disqust;neutral', 'angry;happy;neutral', 'angry;neutral', 'disqust;happy;neutral', 'disqust;neutral', 'disqust;neutral;sad', 'fear;happy', 'fear;happy;neutral', 'fear;neutral', 'happy;neutral', 'happy;neutral;surprise', 'happy;sad', 'happy;surprise', 'neutral;sad', 'neutral;surprise'\n",
    "\n",
    "<2019년>\n",
    "'angry;disgust;fear;neutral;surprise', 'angry;fear', 'angry;fear;neutral', 'angry;fear;surprise', 'angry;happy', 'angry;neutral;surprise', 'angry;sad', 'angry;surprise', 'disgust;fear', 'disgust;happy','disgust;neutral;surprise', 'disgust;sad', 'disgust;surprise', 'fear;neutral;surprise', 'fear;sad', 'fear;surprise', 'happy;neutral;sad', 'neutral;sad;surprise', 'sad;surprise'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_dict = {'angry':0, 'disgust':1, 'fear':2,'happy':3,'neutral':4, 'sad':5, 'surprise':6,  \n",
    "               'neutral;surprise': 450, 'neutral;sad': 460, 'happy;neutral': 340, \n",
    "               'angry;neutral': 40, 'disgust;neutral': 140, 'fear;neutral': 240, \n",
    "               'happy;surprise': 350, 'angry;happy;neutral': 7340, 'angry;disgust': 10, \n",
    "               'happy;neutral;surprise': 3450, 'fear;happy': 230,'fear;happy;neutral': 2340,\n",
    "               'angry;disgust;neutral': 7140, 'disgust;neutral;sad': 1460, \n",
    "               'happy;sad': 360, 'disgust;happy;neutral': 3410, 'angry;fear': 20, 'angry;fear;neutral':7240,\n",
    "               'angry;fear;surprise': 7250, 'angry;happy': 730, 'angry;neutral;surprise':7450, \n",
    "               'angry;sad': 60, 'angry;surprise': 50, 'disgust;fear':120, 'disgust;happy': 130,\n",
    "               'disgust;neutral;surprise':1450, 'disgust;sad': 160, 'disgust;surprise':150, \n",
    "               'fear;neutral;surprise':2450, 'fear;sad': 260, 'fear;surprise':250, 'happy;neutral;sad':3460,\n",
    "               'neutral;sad;surprise':4560, 'sad;surprise': 560,\n",
    "               'angry;disgust;fear;neutral;sad': 10000,'angry;disgust;fear;neutral;surprise':20000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprise': 6, 'angry;disgust': 10, 'angry;fear': 20, 'angry;neutral': 40, 'angry;surprise': 50, 'angry;sad': 60, 'disgust;fear': 120, 'disgust;happy': 130, 'disgust;neutral': 140, 'disgust;surprise': 150, 'disgust;sad': 160, 'fear;happy': 230, 'fear;neutral': 240, 'fear;surprise': 250, 'fear;sad': 260, 'happy;neutral': 340, 'happy;surprise': 350, 'happy;sad': 360, 'neutral;surprise': 450, 'neutral;sad': 460, 'sad;surprise': 560, 'angry;happy': 730, 'disgust;neutral;surprise': 1450, 'disgust;neutral;sad': 1460, 'fear;happy;neutral': 2340, 'fear;neutral;surprise': 2450, 'disgust;happy;neutral': 3410, 'happy;neutral;surprise': 3450, 'happy;neutral;sad': 3460, 'neutral;sad;surprise': 4560, 'angry;disgust;neutral': 7140, 'angry;fear;neutral': 7240, 'angry;fear;surprise': 7250, 'angry;happy;neutral': 7340, 'angry;neutral;surprise': 7450, 'angry;disgust;fear;neutral;sad': 10000, 'angry;disgust;fear;neutral;surprise': 20000}\n"
     ]
    }
   ],
   "source": [
    "encode_dict = {k: v for k, v in sorted(encode_dict.items(), key=lambda item: item[1])}\n",
    "decode_dict = {b:i for i, b in encode_dict.items()}\n",
    "print(encode_dict, '\\n', decode_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    6\n",
       " 1    2\n",
       " 2    0\n",
       " Name: Emotion, dtype: int64,\n",
       " 0    4\n",
       " 1    4\n",
       " 2    4\n",
       " Name: Emotion, dtype: int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kemdy19_annot.Emotion = list(kemdy19_annot.Emotion.map(encode_dict))\n",
    "kemdy20_annot.Emotion = list(kemdy20_annot.Emotion.map(encode_dict))\n",
    "kemdy19_annot.Emotion[:3], kemdy20_annot.Emotion[:3]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA, TEMP Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def add_padding(pd_series, length = 50):\n",
    "    if isinstance(pd_series, float):\n",
    "        if math.isnan(pd_series):\n",
    "            return np.zeros(10)\n",
    "    if len(pd_series) < length:\n",
    "        pd_series = np.concatenate([pd_series, np.zeros(length - len(pd_series))])\n",
    "        return np.array(pd_series)\n",
    "    elif len(pd_series) == length:\n",
    "        return np.array(pd_series)\n",
    "    elif len(pd_series) > length:\n",
    "        pd_series = pd_series[:length]\n",
    "        return np.array(pd_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.37470392, -0.702349  , -0.57776951,  0.36337337,  0.15086029,\n",
       "         0.23434567,  0.6442029 ,  0.8643034 ,  0.14326694,  0.33301774,\n",
       "         0.55311825, -0.16032486, -0.10719511,  0.09773054,  0.10531797,\n",
       "         0.12809209,  0.40890977,  0.23435159, -0.29694591, -0.36525051,\n",
       "        -0.25140359, -0.46391666, -0.14514408,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ]),\n",
       " array([-0.21123698, -0.18926761, -0.18665215, -0.1861293 , -0.1861293 ,\n",
       "        -0.18874476, -0.20757542, -0.25517533, -0.29074444, -0.29179054,\n",
       "        -0.30434459, -0.24105233, -0.24628284, -0.48480603, -0.52822112,\n",
       "        -0.60860162, -0.77337071, -0.85676544, -0.78562721, -0.72181169,\n",
       "        -0.65747333, -0.89756547, -0.72181169, -0.67891945, -0.63655006,\n",
       "        -0.63602721, -0.62085771, -0.58842731, -0.57796588, -0.59156562,\n",
       "        -0.60725797,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ]),\n",
       " array([-0.96177679, -0.96177679, -0.96177679, -0.96177679, -0.96177679,\n",
       "        -0.96177679, -0.96177679, -0.96177679, -0.96177679, -0.96177679,\n",
       "        -0.96177679, -0.96177679, -0.96177679, -0.96177679, -0.96177679,\n",
       "        -1.15004747, -1.15004747, -1.15004747, -1.15004747, -1.15004747,\n",
       "        -1.15004747, -1.15004747, -1.15004747, -0.96177679, -0.96177679,\n",
       "        -0.96177679, -0.96177679, -1.15004747, -1.15004747, -1.15004747,\n",
       "        -1.15004747, -1.15004747, -1.15004747, -1.15004747, -1.15004747,\n",
       "        -1.15004747, -1.15004747, -1.15004747, -1.15004747, -1.15004747,\n",
       "        -1.15004747, -1.15004747, -1.15004747, -1.33831815, -1.33831815,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ]),\n",
       " array([0.7038299 , 0.7038299 , 0.67438937, 0.67438937, 0.67438937,\n",
       "        0.67438937, 0.67438937, 0.67438937, 0.67438937, 0.67438937,\n",
       "        0.64494885, 0.64494885, 0.64494885, 0.64494885, 0.7038299 ,\n",
       "        0.7038299 , 0.7038299 , 0.7038299 , 0.71855016, 0.71855016,\n",
       "        0.71855016, 0.71855016, 0.7038299 , 0.7038299 , 0.7038299 ,\n",
       "        0.7038299 , 0.67438937, 0.67438937, 0.67438937, 0.67438937,\n",
       "        0.67438937, 0.67438937, 0.67438937, 0.67438937, 0.64494885,\n",
       "        0.64494885, 0.64494885, 0.64494885, 0.67438937, 0.67438937,\n",
       "        0.67438937, 0.67438937, 0.7038299 , 0.7038299 , 0.7038299 ,\n",
       "        0.7038299 , 0.7038299 , 0.7038299 , 0.7038299 , 0.7038299 ]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kemdy19_annot['Scaled EDA'] = kemdy19_annot['Scaled EDA'].apply(add_padding)\n",
    "kemdy20_annot['Scaled EDA'] = kemdy20_annot['Scaled EDA'].apply(add_padding)\n",
    "kemdy19_annot['Scaled TEMP'] = kemdy19_annot['Scaled TEMP'].apply(add_padding)\n",
    "kemdy20_annot['Scaled TEMP'] = kemdy20_annot['Scaled TEMP'].apply(add_padding)\n",
    "# check\n",
    "kemdy20_annot['Scaled EDA'][10], kemdy19_annot['Scaled EDA'][10], kemdy20_annot['Scaled TEMP'][3],kemdy19_annot['Scaled TEMP'][15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session 19 length: wav - 9008, txt - 9008\n",
      "session 20 length: wav - 12715, txt - 12715\n"
     ]
    }
   ],
   "source": [
    "length_wav = 0\n",
    "length_txt = 0\n",
    "for i,j in zip(kemdy19_emb[0], kemdy19_emb[1]):\n",
    "    length_wav += len(kemdy19_emb[0][i])\n",
    "    length_txt += len(kemdy19_emb[1][j])\n",
    "    # lengt += len(i)\n",
    "print(f'session 19 length: wav - {length_wav}, txt - {length_txt}')\n",
    "\n",
    "length_wav = 0\n",
    "length_txt = 0\n",
    "for i,j in zip(kemdy20_emb[0], kemdy20_emb[1]):\n",
    "    length_wav += len(kemdy20_emb[0][i])\n",
    "    length_txt += len(kemdy20_emb[1][j])\n",
    "    \n",
    "print(f'session 20 length: wav - {length_wav}, txt - {length_txt}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session pick \n",
    "- test(.2), validation(.2), train(.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choice_and_remove_list(original_list, k = 8):\n",
    "    removed_new_list = []\n",
    "    chosen_list = random.sample(original_list, k = k)\n",
    "    for session in original_list:\n",
    "        if session in chosen_list:\n",
    "            pass\n",
    "        else:\n",
    "            removed_new_list.append(session) \n",
    "    return sorted(removed_new_list), sorted(chosen_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sess01', 'Sess03', 'Sess07', 'Sess08', 'Sess09', 'Sess10', 'Sess13', 'Sess15', 'Sess18', 'Sess19', 'Sess20', 'Sess23', 'Sess24', 'Sess26', 'Sess29', 'Sess32', 'Sess35', 'Sess36', 'Sess37', 'Sess38', 'Sess39', 'Sess40']\n",
      "['Sess02', 'Sess04', 'Sess06', 'Sess14', 'Sess16', 'Sess21', 'Sess28', 'Sess30']\n",
      "['Sess05', 'Sess11', 'Sess22', 'Sess25', 'Sess27', 'Sess31', 'Sess33', 'Sess34']\n",
      "\n",
      "\n",
      "['Sess01', 'Sess02', 'Sess03', 'Sess04', 'Sess05', 'Sess06', 'Sess07', 'Sess08', 'Sess09', 'Sess10', 'Sess11', 'Sess12', 'Sess13', 'Sess14', 'Sess15', 'Sess16', 'Sess17', 'Sess18', 'Sess19', 'Sess20']\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# session을 train vs test&val로 나눠줌\n",
    "session_20_lst = ['Sess0' + str(i+1) if i < 9 else 'Sess' + str(i+1) for i in range(40)]\n",
    "session_20_lst.remove('Sess12')\n",
    "session_20_lst.remove('Sess17')\n",
    "sessions_20_train_lst, sessions_20_test_lst = choice_and_remove_list(session_20_lst, k = 8)\n",
    "sessions_20_train_lst, sessions_20_val_lst = choice_and_remove_list(sessions_20_train_lst, k = 8)\n",
    "print(sessions_20_train_lst, sessions_20_test_lst, sessions_20_val_lst, sep = '\\n')\n",
    "\n",
    "session_19_lst = ['Sess0' + str(i+1) if i < 9 else 'Sess' + str(i+1) for i in range(20)]\n",
    "sessions_19_train_lst, sessions_19_test_lst = choice_and_remove_list(session_19_lst, k = 0)\n",
    "sessions_19_train_lst, sessions_19_val_lst = choice_and_remove_list(sessions_19_train_lst, k = 0)\n",
    "print('\\n', sessions_19_train_lst,sessions_19_test_lst, sessions_19_val_lst, sep = '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data reconstruct\n",
    "- dict - session - txt, wav, segment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "kemdy20_emb_new = {}\n",
    "for session in kemdy20_emb[0].keys():\n",
    "    kemdy20_emb_new[session] = {}\n",
    "    kemdy20_emb_new[session]['wav'] = kemdy20_emb[0][session]\n",
    "    kemdy20_emb_new[session]['txt'] = kemdy20_emb[1][session]\n",
    "    kemdy20_emb_new[session]['segment_id'] = kemdy20_annot['Segment ID'][kemdy20_annot['Segment ID'].str.startswith(session)]\n",
    "    # print(session, len(kemdy20_emb_new[session]['wav']), len(kemdy20_emb_new[session]['txt']), len(kemdy20_emb_new[session]['segment_id']))\n",
    "\n",
    "# print('\\n')\n",
    "kemdy19_emb_new = {}\n",
    "for session in kemdy19_emb[0].keys():\n",
    "    kemdy19_emb_new[session] = {}\n",
    "    kemdy19_emb_new[session]['wav'] = kemdy19_emb[0][session]\n",
    "    kemdy19_emb_new[session]['txt'] = kemdy19_emb[1][session]\n",
    "    kemdy19_emb_new[session]['segment_id'] = kemdy19_annot['Segment ID'][kemdy19_annot['Segment ID'].str.startswith(session)]\n",
    "    # print(session, len(kemdy19_emb_new[session]['wav']), len(kemdy19_emb_new[session]['txt']), len(kemdy19_emb_new[session]['segment_id']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나눠준 세션을 emb(wav, text순), annot set에 적용\n",
    "def get_data_by_session(data, session_lst):\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        print('dataframe')\n",
    "        for idx, session in enumerate(session_lst):\n",
    "            if idx == 0:\n",
    "                dataframe = data[data['Segment ID'].str.startswith(session)]\n",
    "            else:\n",
    "                dataframe = pd.concat([dataframe, data[data['Segment ID'].str.startswith(session)]])\n",
    "        return dataframe\n",
    "    \n",
    "    elif isinstance(data, dict):\n",
    "        print('dict')\n",
    "        emb_data = {}\n",
    "        emb_data['wav'] = []\n",
    "        emb_data['txt'] = []\n",
    "        emb_data['segment_id'] = []\n",
    "        for session in session_lst:\n",
    "            emb_data['wav'].extend(data[session]['wav'])\n",
    "            emb_data['txt'].extend(data[session]['txt'])\n",
    "            emb_data['segment_id'].extend(data[session]['segment_id'])            \n",
    "        return emb_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe\n",
      "dataframe\n",
      "dataframe\n",
      "dataframe\n"
     ]
    }
   ],
   "source": [
    "kemdy19_annot_train = get_data_by_session(kemdy19_annot, sessions_19_train_lst)\n",
    "\n",
    "kemdy20_annot_train = get_data_by_session(kemdy20_annot, sessions_20_train_lst)\n",
    "kemdy20_annot_test = get_data_by_session(kemdy20_annot, sessions_20_test_lst)\n",
    "kemdy20_annot_val = get_data_by_session(kemdy20_annot, sessions_20_val_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict\n",
      "dict\n",
      "dict\n",
      "dict\n"
     ]
    }
   ],
   "source": [
    "# session 정보가 없어집니다. \n",
    "kemdy19_emb_train = get_data_by_session(kemdy19_emb_new, sessions_19_train_lst)\n",
    "\n",
    "kemdy20_emb_train = get_data_by_session(kemdy20_emb_new, sessions_20_train_lst)\n",
    "kemdy20_emb_test = get_data_by_session(kemdy20_emb_new, sessions_20_test_lst)\n",
    "kemdy20_emb_val = get_data_by_session(kemdy20_emb_new, sessions_20_val_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7427 7427 7427\n",
      "2570 2570 2570\n",
      "2718 2718 2718\n",
      "12715\n"
     ]
    }
   ],
   "source": [
    "# kemdy 20 - 12715가 정상\n",
    "print(len(kemdy20_emb_train['wav']), len(kemdy20_emb_train['txt']), len(kemdy20_emb_train['segment_id']))\n",
    "print(len(kemdy20_emb_test['wav']), len(kemdy20_emb_test['txt']), len(kemdy20_emb_test['segment_id']))\n",
    "print(len(kemdy20_emb_val['wav']), len(kemdy20_emb_val['txt']), len(kemdy20_emb_val['segment_id']))\n",
    "print(len(kemdy20_emb_train['wav']) + len(kemdy20_emb_test['wav'])+ len(kemdy20_emb_val['wav']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Data neutral pick하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kemdy19에서 0개, kemdy20에서 6134개 추출\n"
     ]
    }
   ],
   "source": [
    "# 각 데이터 셋에서 몇개 뽑아야 되는지 계산 neutral: 4\n",
    "\n",
    "target_neutral_num = Counter(kemdy20_annot_train['Emotion'])[4]\n",
    "\n",
    "target_neutral_num_19 = 0\n",
    "target_neutral_num_20 = target_neutral_num - target_neutral_num_19\n",
    "print(f'kemdy19에서 {target_neutral_num_19}개, kemdy20에서 {target_neutral_num_20}개 추출')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12964"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 train dataset에서 뽑아야 되는 갯수만큼 랜덤으로 뽑아옴\n",
    "kemdy19_annot_train_not_neut = kemdy19_annot_train[kemdy19_annot_train['Emotion'] != 4]\n",
    "kemdy20_annot_train_not_neut = kemdy20_annot_train[kemdy20_annot_train['Emotion'] != 4]\n",
    "\n",
    "kemdy19_annot_train_neut = kemdy19_annot_train[kemdy19_annot_train['Emotion'] == 4].sample(target_neutral_num_19)\n",
    "kemdy20_annot_train_neut = kemdy20_annot_train[kemdy20_annot_train['Emotion'] == 4].sample(target_neutral_num_20)\n",
    "len(kemdy19_annot_train_not_neut) + len(kemdy19_annot_train_neut) + len(kemdy20_annot_train_not_neut) + len(kemdy20_annot_train_neut)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## embedding, Test and validation dataset 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2570 2570 2570\n"
     ]
    }
   ],
   "source": [
    "emb_test_final = {}\n",
    "emb_test_final['wav'] = []\n",
    "emb_test_final['txt'] = []\n",
    "emb_test_final['segment_id'] = []\n",
    "\n",
    "# emb_test_final['wav'] = kemdy19_emb_test['wav']\n",
    "emb_test_final['wav'].extend(kemdy20_emb_test['wav'])\n",
    "\n",
    "# emb_test_final['txt'] = kemdy19_emb_test['txt']\n",
    "emb_test_final['txt'].extend(kemdy20_emb_test['txt'])\n",
    "\n",
    "# emb_test_final['segment_id'] = kemdy19_emb_test['segment_id']\n",
    "emb_test_final['segment_id'].extend(kemdy20_emb_test['segment_id'])\n",
    "\n",
    "\n",
    "print(len(emb_test_final['wav']), len(emb_test_final['txt']), len(emb_test_final['segment_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2718 2718 2718\n"
     ]
    }
   ],
   "source": [
    "emb_val_final = {}\n",
    "emb_val_final['wav'] = []\n",
    "emb_val_final['txt'] = []\n",
    "emb_val_final['segment_id'] = []\n",
    "\n",
    "# emb_val_final['wav'] = kemdy19_emb_val['wav']\n",
    "emb_val_final['wav'].extend(kemdy20_emb_val['wav'])\n",
    "\n",
    "# emb_val_final['txt'] = kemdy19_emb_val['txt']\n",
    "emb_val_final['txt'].extend(kemdy20_emb_val['txt'])\n",
    "\n",
    "# emb_val_final['segment_id'] = kemdy19_emb_val['segment_id']\n",
    "emb_val_final['segment_id'].extend(kemdy20_emb_val['segment_id'])\n",
    "\n",
    "print(len(emb_val_final['wav']), len(emb_val_final['txt']), len(emb_val_final['segment_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12964 2570 2718\n"
     ]
    }
   ],
   "source": [
    "annot_train_final = pd.concat([kemdy19_annot_train_neut, kemdy20_annot_train_neut, kemdy19_annot_train_not_neut, kemdy20_annot_train_not_neut])\n",
    "\n",
    "annot_test_final = pd.concat([kemdy20_annot_test])\n",
    "annot_val_final = pd.concat([kemdy20_annot_val])\n",
    "\n",
    "annot_train_final.reset_index(drop=True, inplace=True)\n",
    "annot_test_final.reset_index(drop=True, inplace=True)\n",
    "annot_val_final.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(len(annot_train_final), len(annot_test_final), len(annot_val_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12964 12964 12964\n"
     ]
    }
   ],
   "source": [
    "emb_train_final = {}\n",
    "emb_train_final['wav'] = []\n",
    "emb_train_final['txt'] = []\n",
    "emb_train_final['segment_id'] = []\n",
    "for segment_annot_id in kemdy19_annot_train_neut['Segment ID']:\n",
    "    for wav, txt, segment_emb_id in zip(kemdy19_emb_train['wav'], kemdy19_emb_train['txt'], kemdy19_emb_train['segment_id']):\n",
    "        if segment_annot_id == segment_emb_id:\n",
    "            emb_train_final['wav'].append(wav)\n",
    "            emb_train_final['txt'].append(txt)\n",
    "            emb_train_final['segment_id'].append(segment_emb_id)\n",
    "            \n",
    "for segment_annot_id in kemdy19_annot_train_not_neut['Segment ID']:\n",
    "    for wav, txt, segment_emb_id in zip(kemdy19_emb_train['wav'], kemdy19_emb_train['txt'], kemdy19_emb_train['segment_id']):\n",
    "        if segment_annot_id == segment_emb_id:\n",
    "            emb_train_final['wav'].append(wav)\n",
    "            emb_train_final['txt'].append(txt)\n",
    "            emb_train_final['segment_id'].append(segment_emb_id)\n",
    "        \n",
    "\n",
    "for segment_annot_id in kemdy20_annot_train_neut['Segment ID']:\n",
    "    for wav, txt, segment_emb_id in zip(kemdy20_emb_train['wav'], kemdy20_emb_train['txt'], kemdy20_emb_train['segment_id']):\n",
    "        if segment_emb_id == segment_annot_id:\n",
    "            emb_train_final['wav'].append(wav)\n",
    "            emb_train_final['txt'].append(txt)\n",
    "            emb_train_final['segment_id'].append(segment_emb_id)\n",
    "        \n",
    "for segment_annot_id in kemdy20_annot_train_not_neut['Segment ID']:\n",
    "    for wav, txt, segment_emb_id in zip(kemdy20_emb_train['wav'], kemdy20_emb_train['txt'], kemdy20_emb_train['segment_id']):\n",
    "        if segment_annot_id == segment_emb_id:\n",
    "            emb_train_final['wav'].append(wav)\n",
    "            emb_train_final['txt'].append(txt)\n",
    "            emb_train_final['segment_id'].append(segment_emb_id)\n",
    "            \n",
    "print(len(emb_train_final['wav']), len(emb_train_final['txt']), len(emb_train_final['segment_id']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch dataset 만들기\n",
    "- 참고: https://tutorials.pytorch.kr/beginner/basics/data_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "torch.set_default_dtype(torch.float32)\n",
    "print(torch.get_default_dtype())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EtriDataset(Dataset):\n",
    "    def __init__(self, file_names, \n",
    "                 text_embeddings, \n",
    "                 wav_embeddings, \n",
    "                 Temp,\n",
    "                 EDA,\n",
    "                 Emotion,\n",
    "                 Emotion_vec, \n",
    "                 Arousal, \n",
    "                 Valence):\n",
    "        self.file_names = file_names\n",
    "        self.text_embeddings = text_embeddings\n",
    "        self.wav_embeddings = wav_embeddings\n",
    "        self.temp = Temp\n",
    "        self.eda = EDA\n",
    "        self.label_emotion = Emotion\n",
    "        self.label_emotion_vec = Emotion_vec\n",
    "        self.label_arousal = Arousal\n",
    "        self.label_valence = Valence\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text_embeddings = self.text_embeddings[idx]\n",
    "        wav_embeddings = self.wav_embeddings[idx]\n",
    "        temp = self.temp[idx]\n",
    "        eda = self.eda[idx]\n",
    "        label_emotion = self.label_emotion[idx]\n",
    "        label_emotion_ext = self.label_emotion_vec[idx]\n",
    "        label_arousal = self.label_arousal[idx]\n",
    "        label_valence = self.label_valence[idx]\n",
    "        return text_embeddings, wav_embeddings, temp, eda, label_emotion, label_emotion_ext, label_arousal, label_valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_819618/245938389.py:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  EDA = torch.Tensor(annot_train_final['Scaled EDA']),\n"
     ]
    }
   ],
   "source": [
    "# session을 통합시킨 데이터 셋을 만들었을 때\n",
    "dataset_train = EtriDataset(file_names = annot_train_final['Segment ID'],\n",
    "                      text_embeddings = torch.stack(emb_train_final['txt']),\n",
    "                      wav_embeddings = torch.stack(emb_train_final['wav']),\n",
    "                      Emotion = annot_train_final['Emotion'],\n",
    "                      Arousal = annot_train_final['Arousal'],\n",
    "                      Valence = annot_train_final['Valence'],\n",
    "                      EDA = torch.Tensor(annot_train_final['Scaled EDA']), \n",
    "                      Temp = torch.Tensor(annot_train_final['Scaled TEMP']), \n",
    "                      Emotion_vec = torch.Tensor(annot_train_final['emotion_vector'])) \n",
    "\n",
    "\n",
    "dataset_test = EtriDataset(file_names = annot_test_final['Segment ID'],\n",
    "                      text_embeddings = torch.stack(emb_test_final['txt']),\n",
    "                      wav_embeddings = torch.stack(emb_test_final['wav']),\n",
    "                      Emotion = annot_test_final['Emotion'],\n",
    "                      Arousal = annot_test_final['Arousal'],\n",
    "                      Valence = annot_test_final['Valence'],\n",
    "                      EDA = torch.Tensor(annot_test_final['Scaled EDA']), \n",
    "                      Temp = torch.Tensor(annot_test_final['Scaled TEMP']), \n",
    "                      Emotion_vec = torch.Tensor(annot_test_final['emotion_vector']))\n",
    "\n",
    "dataset_val = EtriDataset(file_names = annot_val_final['Segment ID'],\n",
    "                      text_embeddings = torch.stack(emb_val_final['txt']),\n",
    "                      wav_embeddings = torch.stack(emb_val_final['wav']),\n",
    "                      Emotion = annot_val_final['Emotion'],\n",
    "                      Arousal = annot_val_final['Arousal'],\n",
    "                      Valence = annot_val_final['Valence'],\n",
    "                      EDA = torch.Tensor(annot_val_final['Scaled EDA']), \n",
    "                      Temp = torch.Tensor(annot_val_final['Scaled TEMP']), \n",
    "                      Emotion_vec = torch.Tensor(annot_val_final['emotion_vector'])) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Size : 12964\n",
      "Validation Data Size : 2718\n",
      "Testing Data Size : 2570\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training Data Size : {len(dataset_train)}\")\n",
    "print(f\"Validation Data Size : {len(dataset_val)}\")\n",
    "print(f\"Testing Data Size : {len(dataset_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset_train, batch_size=256, shuffle=True, drop_last=True)\n",
    "validation_dataloader = DataLoader(dataset_val, batch_size=128, shuffle=True, drop_last=True)\n",
    "test_dataloader = DataLoader(dataset_test, batch_size=128, shuffle=True, drop_last=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NetWork 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNetwork_pre(nn.Module):\n",
    "    def __init__(self, input_length, input_width):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_length*input_width, 768)\n",
    "        self.gelu1 = nn.GELU()\n",
    "        self.bn1 = nn.BatchNorm1d(768)\n",
    "        self.fc2 = nn.Linear(768, 512)\n",
    "        self.gelu2 = nn.GELU()\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.fc3 = nn.Linear(512, 32)\n",
    "        self.gelu3 = nn.GELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.gelu1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.gelu2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.fc3(x)\n",
    "        output = self.gelu3(x)\n",
    "        return output\n",
    "\n",
    "class ConvNetwork_final(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv2d_1 = nn.Conv1d(in_channels = 32, out_channels = 16, kernel_size=8)\n",
    "        self.leakyrelu_1 = nn.LeakyReLU()\n",
    "        self.conv2d_2 = nn.Conv1d(in_channels = 16, out_channels = 8, kernel_size=8)\n",
    "        self.leakyrelu_2 = nn.LeakyReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(144, 64)\n",
    "        self.leakyrelu_3 = nn.LeakyReLU()\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.drop1 = nn.Dropout(p=0.25)\n",
    "        self.fc2 = nn.Linear(64, 7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv2d_1(x)\n",
    "        x = self.leakyrelu_1(x)\n",
    "        x = self.conv2d_2(x)\n",
    "        x = self.leakyrelu_2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.leakyrelu_3(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.drop1(x)\n",
    "        output = self.fc2(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorFusionMixer(nn.Module):\n",
    "    def __init__(self, ModelA, ModelB, ModelC):\n",
    "        super().__init__()\n",
    "        self.ModelA = ModelA\n",
    "        self.ModelB = ModelB\n",
    "        self.Model_cnn_final = ModelC\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def tensor_fusion(self, batch_arr1, batch_arr2):\n",
    "        fusion_matrix_lst = []\n",
    "        for i, (arr1, arr2) in enumerate(zip(batch_arr1, batch_arr2)):\n",
    "            \n",
    "            arr1 = arr1.unsqueeze(-1)\n",
    "            arr2 = arr2.unsqueeze(0)\n",
    "\n",
    "\n",
    "            kron_matrix = torch.kron(arr2, arr1)\n",
    "            l, w = kron_matrix.shape\n",
    "\n",
    "            kron_matrix = kron_matrix.view(-1, l, w)\n",
    "            fusion_matrix_lst.append(kron_matrix)\n",
    "\n",
    "        fusion_matrix = torch.concat(fusion_matrix_lst)\n",
    "\n",
    "        return fusion_matrix\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "            x1 = self.ModelA(x1)\n",
    "            x2 = self.ModelB(x2)\n",
    "            fusion_matrix = self.tensor_fusion(x1, x2)\n",
    "            x = self.Model_cnn_final(fusion_matrix) \n",
    "            output = self.softmax(x)\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 768 149 1024\n",
      "Let's use 4 GPUs!\n",
      "DataParallel(\n",
      "  (module): TensorFusionMixer(\n",
      "    (ModelA): MLPNetwork_pre(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=61440, out_features=768, bias=True)\n",
      "      (gelu1): GELU(approximate='none')\n",
      "      (bn1): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc2): Linear(in_features=768, out_features=512, bias=True)\n",
      "      (gelu2): GELU(approximate='none')\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc3): Linear(in_features=512, out_features=32, bias=True)\n",
      "      (gelu3): GELU(approximate='none')\n",
      "    )\n",
      "    (ModelB): MLPNetwork_pre(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=152576, out_features=768, bias=True)\n",
      "      (gelu1): GELU(approximate='none')\n",
      "      (bn1): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc2): Linear(in_features=768, out_features=512, bias=True)\n",
      "      (gelu2): GELU(approximate='none')\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc3): Linear(in_features=512, out_features=32, bias=True)\n",
      "      (gelu3): GELU(approximate='none')\n",
      "    )\n",
      "    (Model_cnn_final): ConvNetwork_final(\n",
      "      (conv2d_1): Conv1d(32, 16, kernel_size=(8,), stride=(1,))\n",
      "      (leakyrelu_1): LeakyReLU(negative_slope=0.01)\n",
      "      (conv2d_2): Conv1d(16, 8, kernel_size=(8,), stride=(1,))\n",
      "      (leakyrelu_2): LeakyReLU(negative_slope=0.01)\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=144, out_features=64, bias=True)\n",
      "      (leakyrelu_3): LeakyReLU(negative_slope=0.01)\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop1): Dropout(p=0.25, inplace=False)\n",
      "      (fc2): Linear(in_features=64, out_features=7, bias=True)\n",
      "    )\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "txt_input_length, txt_input_width = torch.Tensor(emb_train_final['txt'][0]).shape\n",
    "wav_input_length, wav_input_width = torch.Tensor(emb_train_final['wav'][0]).shape\n",
    "\n",
    "# tf_mixer에 들어갈 wav mlp, txt mlp 선언\n",
    "model_mlp_txt = MLPNetwork_pre(txt_input_length,txt_input_width).to(device)\n",
    "model_mlp_wav = MLPNetwork_pre(wav_input_length,wav_input_width).to(device)\n",
    "\n",
    "model_cnn_final = ConvNetwork_final().to(device)\n",
    "\n",
    "# 최종 모델 선언\n",
    "model_tf_cnn_mixer = TensorFusionMixer(ModelA = model_mlp_txt, \n",
    "                                   ModelB = model_mlp_wav,\n",
    "                                   ModelC = model_cnn_final).to(device)\n",
    "\n",
    "# model 병렬 학습 처리\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model_mlp_txt = nn.DataParallel(model_mlp_txt).to(device)\n",
    "    model_mlp_wav = nn.DataParallel(model_mlp_wav).to(device)\n",
    "    model_tf_cnn_mixer = nn.DataParallel(model_tf_cnn_mixer).to(device)\n",
    "\n",
    "print(model_tf_cnn_mixer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습을 위한 train, test method 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)    \n",
    "    # data 순서: text_embeddings, wav_embeddings, temp, eda, label_emotion, label_emotion_ext, label_arousal, label_valence\n",
    "    for batch, (X_txt, X_wav, _, _, \n",
    "                    label_emotion, label_emotion_vec, label_arousal, label_valence) in enumerate(dataloader): \n",
    "        y = label_emotion_vec # 라벨을 변경하고자 하면 이 변수만 바꿔주면 나머지는 y로 적용\n",
    "        # 예측 오류 계산 \n",
    "        X_txt, X_wav, y= X_txt.to(device), X_wav.to(device), y.type(torch.float32).to(device)\n",
    "        \n",
    "        \n",
    "        pred = model(X_txt, X_wav)\n",
    "        y = F.softmax(y, dim = 1)\n",
    "        \n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # 역전파\n",
    "        optimizer.zero_grad()\n",
    "        loss.mean().backward() \n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.mean().mean().item(), batch * len(X_txt) \n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_label_threshold = 0.080 # 두 개의 softmax 차이값 어디까지를 multilabel로 볼 것인가.\n",
    "def cal_multiple_class(probs, threshold = multi_label_threshold):\n",
    "    values = probs.topk(2)\n",
    "    pred_list = []\n",
    "    # print(values)\n",
    "    diffs = abs(torch.diff(values.values))\n",
    "    for idx,diff in zip(values.indices, diffs):\n",
    "        if diff <= threshold:\n",
    "            sorted_label, idx = torch.sort(idx)\n",
    "            if sorted_label[0] == 0:\n",
    "                pred_list.append(100*7 + 10*sorted_label[1].item())\n",
    "            else:\n",
    "                pred_list.append(100*sorted_label[0].item() + 10*sorted_label[1].item())\n",
    "        else:\n",
    "            pred_list.append(idx[0].item())\n",
    "    return torch.Tensor(pred_list).to(device), probs.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from sklearn.metrics import f1_score as f1_skearn\n",
    "\n",
    "def test(dataloader, model, loss_fn, mode = 'test'):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    f1 = F1Score(task= 'multiclass', num_classes=37).to(device)   \n",
    "    precision = MulticlassPrecision(num_classes=37)\n",
    "    preds = []\n",
    "    targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # data 순서: text_embeddings, wav_embeddings, temp, eda, label_emotion, label_emotion_ext, label_arousal, label_valence\n",
    "        for batch, (X_txt, X_wav, X_temp, X_eda, \n",
    "                        label_emotion, label_emotion_vec, label_arousal, label_valence) in enumerate(dataloader): \n",
    "            y = label_emotion_vec # 라벨을 변경하고자 하면 이 변수만 바꿔주면 나머지는 y로 적용\n",
    "            \n",
    "            # 예측 오류 계산\n",
    "            X_txt, X_wav, y= X_txt.to(device), X_wav.to(device),y.type(torch.float32).to(device)\n",
    "            \n",
    "            \n",
    "            pred = model(X_txt, X_wav)\n",
    "            pred_for_acc, _ = cal_multiple_class(pred)\n",
    "            \n",
    "            preds.append(pred_for_acc)\n",
    "            y = F.softmax(y, dim = 1)\n",
    "\n",
    "            targets.append(label_emotion) \n",
    "\n",
    "            test_loss += loss_fn(pred, y).mean().item()\n",
    "            \n",
    "            correct += (pred_for_acc == label_emotion.to(device)).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    f1_score = f1(torch.cat(preds).to(device), torch.cat(targets).to(device))\n",
    "    f1_score_weighted = f1_skearn(torch.cat(preds).detach().cpu().numpy(), torch.cat(targets).detach().cpu().numpy(), average= 'weighted')\n",
    "    \n",
    "    accuracy = (100*correct)\n",
    "    \n",
    "    if mode == 'test':\n",
    "        print(torch.cat(preds), torch.cat(preds).shape)\n",
    "        print(\"f1 score: \", f1_score)\n",
    "        print(f\"Test Error: Accuracy: {(accuracy):>0.1f}%, Avg loss: {test_loss:>8f}\\n\")\n",
    "    elif mode == 'val':\n",
    "        print(f\"Validation Error: Accuracy: {(accuracy):>0.1f}%, Avg val loss: {test_loss:>8f} \\n\")\n",
    "    \n",
    "    return f1_score, f1_score_weighted, accuracy, test_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total (single) obs:  12057\n",
      "Counter({4: 6134, 3: 1800, 0: 1620, 6: 1000, 5: 729, 1: 392, 2: 382, 340: 316, 40: 129, 450: 95, 460: 86, 140: 78, 10: 44, 350: 29, 250: 23, 240: 22, 50: 13, 60: 13, 20: 9, 560: 6, 7140: 5, 7340: 3, 260: 3, 150: 3, 730: 3, 7240: 3, 120: 3, 1450: 2, 230: 2, 4560: 2, 3450: 2, 1460: 2, 360: 2, 7250: 1, 7450: 1, 2450: 1, 3460: 1, 2340: 1, 130: 1, 20000: 1, 160: 1, 10000: 1})\n",
      "0 is in single emotion, 1620\n",
      "1 is in single emotion, 392\n",
      "2 is in single emotion, 382\n",
      "3 is in single emotion, 1800\n",
      "4 is in single emotion, 6134\n",
      "5 is in single emotion, 729\n",
      "6 is in single emotion, 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.8657, 0.9673, 0.9683, 0.8506, 0.4912, 0.9395, 0.9170],\n",
       "       dtype=torch.float16)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_emotion = [0,1,2,3,4,5,6]\n",
    "total_obs = 0\n",
    "for i in single_emotion:\n",
    "    total_obs += Counter(annot_train_final['Emotion'])[i]\n",
    "\n",
    "print('total (single) obs: ', total_obs)\n",
    "print(Counter(annot_train_final['Emotion']))\n",
    "\n",
    "weight_for_class = []\n",
    "for key, value in sorted(Counter(annot_train_final['Emotion']).items()):\n",
    "    if key in single_emotion:\n",
    "        print(f'{key} is in single emotion, {value}')\n",
    "        weight_for_class.append(1 - (value/total_obs))\n",
    "            \n",
    "weight_for_class = torch.Tensor(weight_for_class).type(torch.float16)\n",
    "weight_for_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class weighted_MSELoss(nn.Module):\n",
    "    def __init__(self, weight):\n",
    "        super().__init__()\n",
    "        self.weight = weight.to(device)\n",
    "    def forward(self,inputs,targets):\n",
    "        return ((inputs - targets)**2) * self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "loss_fn = weighted_MSELoss(weight = weight_for_class).to(device) \n",
    "optimizer = optim.Adam(model_tf_cnn_mixer.parameters(), lr=lr, weight_decay= 0.0012)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start training mlp fusion mixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkhk172216\u001b[0m (\u001b[33mtoez\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/arplab/project/paradeigma/multi_modal/experimental/wandb/run-20230420_102927-j8qq04p8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/toez/ETRI-kyungho-multiclassification_NOTS/runs/j8qq04p8' target=\"_blank\">Experiment Name(kyungho)</a></strong> to <a href='https://wandb.ai/toez/ETRI-kyungho-multiclassification_NOTS' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/toez/ETRI-kyungho-multiclassification_NOTS' target=\"_blank\">https://wandb.ai/toez/ETRI-kyungho-multiclassification_NOTS</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/toez/ETRI-kyungho-multiclassification_NOTS/runs/j8qq04p8' target=\"_blank\">https://wandb.ai/toez/ETRI-kyungho-multiclassification_NOTS/runs/j8qq04p8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/toez/ETRI-kyungho-multiclassification_NOTS/runs/j8qq04p8?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fda7bc2fe80>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#wandb init\n",
    "epochs = 15\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"ETRI-kyungho-multiclassification_NOTS\",\n",
    "    name = f'Experiment Name(kyungho)',\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": lr,\n",
    "    \"architecture\": \"CNN Tensor Fusion Mixer\",\n",
    "    \"dataset\": \"ETRI Kemdy20\",\n",
    "    \"epochs\": epochs,\n",
    "    \"Optimizer\": optimizer.__class__.__name__,\n",
    "    \"Loss\": loss_fn.__class__.__name__,\n",
    "    \"multi label threshold\": multi_label_threshold,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Epoch 1----------------\n",
      "loss: 0.060106  [    0/12964]\n",
      "Validation Error: Accuracy: 52.8%, Avg val loss: 0.042178 \n",
      "\n",
      "best_acc: 52.75938189845475\n",
      "best_f1: tensor(0.5335, device='cuda:0')\n",
      "best_f1: 0.4692226352321508\n",
      "---------------Epoch 2----------------\n",
      "loss: 0.054037  [    0/12964]\n",
      "Validation Error: Accuracy: 61.7%, Avg val loss: 0.035416 \n",
      "\n",
      "best_acc: 61.73657100809419\n",
      "best_f1: tensor(0.6243, device='cuda:0')\n",
      "best_f1: 0.5932512154031657\n",
      "---------------Epoch 3----------------\n",
      "loss: 0.055136  [    0/12964]\n",
      "Validation Error: Accuracy: 32.3%, Avg val loss: 0.049552 \n",
      "\n",
      "---------------Epoch 4----------------\n",
      "loss: 0.052756  [    0/12964]\n",
      "Validation Error: Accuracy: 72.7%, Avg val loss: 0.031164 \n",
      "\n",
      "best_acc: 72.70051508462105\n",
      "best_f1: tensor(0.7351, device='cuda:0')\n",
      "best_f1: 0.7567662664952928\n",
      "---------------Epoch 5----------------\n",
      "loss: 0.055373  [    0/12964]\n",
      "Validation Error: Accuracy: 36.3%, Avg val loss: 0.047419 \n",
      "\n",
      "---------------Epoch 6----------------\n",
      "loss: 0.051534  [    0/12964]\n",
      "Validation Error: Accuracy: 25.5%, Avg val loss: 0.051584 \n",
      "\n",
      "---------------Epoch 7----------------\n",
      "loss: 0.053820  [    0/12964]\n",
      "Validation Error: Accuracy: 39.8%, Avg val loss: 0.044376 \n",
      "\n",
      "---------------Epoch 8----------------\n",
      "loss: 0.047163  [    0/12964]\n",
      "Validation Error: Accuracy: 49.6%, Avg val loss: 0.041315 \n",
      "\n",
      "---------------Epoch 9----------------\n",
      "loss: 0.048751  [    0/12964]\n",
      "Validation Error: Accuracy: 44.2%, Avg val loss: 0.042317 \n",
      "\n",
      "---------------Epoch 10----------------\n",
      "loss: 0.047012  [    0/12964]\n",
      "Validation Error: Accuracy: 18.9%, Avg val loss: 0.061952 \n",
      "\n",
      "---------------Epoch 11----------------\n",
      "loss: 0.049266  [    0/12964]\n",
      "Validation Error: Accuracy: 35.3%, Avg val loss: 0.048935 \n",
      "\n",
      "---------------Epoch 12----------------\n",
      "loss: 0.041339  [    0/12964]\n",
      "Validation Error: Accuracy: 24.9%, Avg val loss: 0.060334 \n",
      "\n",
      "---------------Epoch 13----------------\n",
      "loss: 0.041554  [    0/12964]\n",
      "Validation Error: Accuracy: 22.4%, Avg val loss: 0.058952 \n",
      "\n",
      "---------------Epoch 14----------------\n",
      "loss: 0.045990  [    0/12964]\n",
      "Validation Error: Accuracy: 23.3%, Avg val loss: 0.059699 \n",
      "\n",
      "---------------Epoch 15----------------\n",
      "loss: 0.042745  [    0/12964]\n",
      "Validation Error: Accuracy: 42.9%, Avg val loss: 0.047314 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▅▇▃█▃▂▄▅▄▁▃▂▁▂▄</td></tr><tr><td>f1 score</td><td>▅▇▃█▃▂▄▅▄▁▃▂▁▂▄</td></tr><tr><td>loss</td><td>▄▂▅▁▅▆▄▃▄█▅█▇▇▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>42.89919</td></tr><tr><td>f1 score</td><td>0.43378</td></tr><tr><td>loss</td><td>0.04731</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Experiment Name(kyungho)</strong> at: <a href='https://wandb.ai/toez/ETRI-kyungho-multiclassification_NOTS/runs/j8qq04p8' target=\"_blank\">https://wandb.ai/toez/ETRI-kyungho-multiclassification_NOTS/runs/j8qq04p8</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230420_102927-j8qq04p8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! best f1_score: 0.7351190447807312, f1_weighted 0.7567662664952928 | best accuracy: 72.70051508462105\n"
     ]
    }
   ],
   "source": [
    "# Set the Training Parameters\n",
    "from copy import deepcopy\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "best_acc = 0\n",
    "best_f1 = 0\n",
    "best_f1_weighted = 0\n",
    "best_acc_model = None \n",
    "best_f1_model = None\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"---------------Epoch {epoch+1}----------------\")\n",
    "    train(train_dataloader, model_tf_cnn_mixer, loss_fn, optimizer)\n",
    "    f1_score, f1_score_weighted, accuracy, loss = test(validation_dataloader, model_tf_cnn_mixer, loss_fn, mode = 'val')\n",
    "    if accuracy > best_acc:\n",
    "        best_acc = accuracy\n",
    "        best_acc_model = deepcopy(model_tf_cnn_mixer)\n",
    "        print('best_acc:', best_acc)\n",
    "    if f1_score > best_f1:\n",
    "        best_f1 = f1_score\n",
    "        best_f1_model = deepcopy(model_tf_cnn_mixer)\n",
    "        print('best_f1:', best_f1)\n",
    "        \n",
    "    if f1_score_weighted > best_f1_weighted:\n",
    "        best_f1_weighted = f1_score_weighted\n",
    "        best_f1_weighted_model = deepcopy(model_tf_cnn_mixer)\n",
    "        print('best_f1:', best_f1_weighted)\n",
    "        \n",
    "    loss_list.append(loss)\n",
    "    acc_list.append(accuracy)\n",
    "    wandb.log({'accuracy': accuracy, 'loss': loss, 'f1 score': f1_score})\n",
    "wandb.finish()\n",
    "print(\"Done!\", f'best f1_score: {best_f1}, f1_weighted {best_f1_weighted} | best accuracy: {best_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  4.,   0.,   4.,  ...,   4.,   4., 460.], device='cuda:0') torch.Size([2560])\n",
      "f1 score:  tensor(0.4059, device='cuda:0')\n",
      "Test Error: Accuracy: 40.4%, Avg loss: 0.051221\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.4059, device='cuda:0'),\n",
       " 0.3043544289798236,\n",
       " 40.42801556420233,\n",
       " 0.0512214444577694)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(test_dataloader, model_tf_cnn_mixer, loss_fn, mode = 'test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bdc1fd12ca460d5768d71e9df3d9063ef832ce64a62e55a1a523c8c99752868e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
