{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkhk172216\u001b[0m (\u001b[33mtoez\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# working model for tensorfusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/40toez/project/kyungho/multi_modal/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu117\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 1\n",
    "LR = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root 경로 설정\n",
    "ROOT = \"/home/40toez/project/kyungho/multi_modal/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotation, embedded data 불러오기\n",
    "with open(ROOT + 'model/data/paradeigma_KEMDY20_annotation_nonmissing.pkl', 'rb') as f:\n",
    "    annotation_20_nonmissing = pickle.load(f)\n",
    "\n",
    "with open(ROOT + 'model/data/paradeigma_KEMDY19_annotation_nonmissing.pkl', 'rb') as f:\n",
    "    annotation_19_nonmissing = pickle.load(f)\n",
    "\n",
    "with open(ROOT + 'model/data/paradeigma_KEMDY20_embedding_for_dataset.pkl', 'rb') as f:\n",
    "    embedding_20_dataset = pickle.load(f)\n",
    "\n",
    "with open(ROOT + 'model/data/paradeigma_KEMDY19_embedding_for_dataset.pkl', 'rb') as f:\n",
    "    embedding_19_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11673/4239242207.py:3: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  raw_dataset = raw_dataset.append(annotation_19_nonmissing, ignore_index = True)\n",
      "/tmp/ipykernel_11673/4239242207.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  raw_dataset = raw_dataset.append(annotation_20_nonmissing, ignore_index = True)\n"
     ]
    }
   ],
   "source": [
    "# Annotation 파일 결합\n",
    "raw_dataset = pd.DataFrame()\n",
    "raw_dataset = raw_dataset.append(annotation_19_nonmissing, ignore_index = True)\n",
    "raw_dataset = raw_dataset.append(annotation_20_nonmissing, ignore_index = True)\n",
    "merged_dataset = raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wav, txt data 분리\n",
    "merged_embeddings = {'wav':[], 'txt':[]}\n",
    "for session, embeddings in embedding_19_dataset[0].items():\n",
    "    merged_embeddings['wav'].append(embeddings)\n",
    "    \n",
    "for session, embeddings in embedding_19_dataset[1].items():\n",
    "    merged_embeddings['txt'].append(embeddings)\n",
    "    \n",
    "for session, embeddings in embedding_20_dataset[0].items():\n",
    "    merged_embeddings['wav'].append(embeddings)\n",
    "    \n",
    "for session, embeddings in embedding_20_dataset[1].items():\n",
    "    merged_embeddings['txt'].append(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wav txt 2020 + 2019\n",
    "merged_embeddings['wav'] = torch.concat(merged_embeddings['wav'])\n",
    "merged_embeddings['txt'] = torch.concat(merged_embeddings['txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([21723, 149, 1024]), torch.Size([21723, 80, 768]), 21723)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 확인\n",
    "merged_embeddings['wav'].shape,merged_embeddings['txt'].shape,len(merged_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding 함수\n",
    "def sequence_padding(ts_list, padding_length = 50, mode = 'constant'):\n",
    "    \n",
    "    padding_value=0\n",
    "    \n",
    "    if (type(ts_list) != type([])) :\n",
    "        ts_list = [padding_value] * padding_length\n",
    "    \n",
    "    elif len(ts_list) >= padding_length :\n",
    "        ts_list = ts_list[0:padding_length]\n",
    "    \n",
    "    elif mode == 'constant':\n",
    "        length = padding_length - len(ts_list)\n",
    "        extend_list = [padding_value] * length\n",
    "        ts_list = ts_list + extend_list    \n",
    "\n",
    "    elif mode == 'replicate':\n",
    "        \n",
    "        quotient = padding_length // len(ts_list)\n",
    "        remainder = padding_length % len(ts_list)\n",
    "        result = ts_list * quotient\n",
    "        result += ts_list[:remainder]\n",
    "        ts_list = result\n",
    "\n",
    "    return ts_list            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시계열 데이터 2020 + 2019\n",
    "merged_dataset['Scaled EDA'] = merged_dataset['Scaled EDA'].apply(sequence_padding)\n",
    "merged_dataset['Scaled TEMP'] = merged_dataset['Scaled TEMP'].apply(sequence_padding)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch dataset 만들기\n",
    "- 참고: https://tutorials.pytorch.kr/beginner/basics/data_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EtriDataset(Dataset):\n",
    "    def __init__(self, file_names, \n",
    "                 text_embeddings, \n",
    "                 wav_embeddings, \n",
    "                 Temp,\n",
    "                 EDA,\n",
    "                 Emotion,\n",
    "                 Emotion_ext, \n",
    "                 Arousal, \n",
    "                 Valence):\n",
    "        self.file_names = file_names\n",
    "        self.text_embeddings = text_embeddings\n",
    "        self.wav_embeddings = wav_embeddings\n",
    "        self.temp = Temp\n",
    "        self.eda = EDA\n",
    "        self.label_emotion = Emotion\n",
    "        self.label_emotion_ext = Emotion_ext\n",
    "        self.label_arousal = Arousal\n",
    "        self.label_valence = Valence\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text_embeddings = self.text_embeddings[idx]\n",
    "        wav_embeddings = self.wav_embeddings[idx]\n",
    "        temp = self.temp[idx]\n",
    "        eda = self.eda[idx]\n",
    "        label_emotion = self.label_emotion[idx]\n",
    "        label_emotion_ext = self.label_emotion_ext[idx]\n",
    "        label_arousal = self.label_arousal[idx]\n",
    "        label_valence = self.label_valence[idx]\n",
    "        return text_embeddings, wav_embeddings, temp, eda, label_emotion, label_emotion_ext, label_arousal, label_valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data load 및 나누기: https://076923.github.io/posts/Python-pytorch-11/\n",
    "\n",
    "# session을 통합시킨 데이터 셋을 만들었을 때\n",
    "dataset = EtriDataset(file_names = merged_dataset['Segment ID'],\n",
    "                      text_embeddings = merged_embeddings['txt'],\n",
    "                      wav_embeddings = merged_embeddings['wav'],\n",
    "                      Emotion = merged_dataset['Emotion'],\n",
    "                      Arousal = merged_dataset['Arousal'],\n",
    "                      Valence = merged_dataset['Valence'],\n",
    "                      EDA = torch.concat(list(merged_dataset['Scaled EDA'].apply(lambda x: torch.tensor(x).view(1,-1)))), ## 일부 세션 데이터만 사용하므로, 길이를 맞춰주기 위해 일부 slicing함\n",
    "                      Temp = torch.concat(list(merged_dataset['Scaled EDA'].apply(lambda x: torch.tensor(x).view(1,-1)))), ## 일부 세션 데이터만 사용하므로, 길이를 맞춰주기 위해 일부 slicing함\n",
    "                      Emotion_ext = torch.concat(list(merged_dataset['Scaled EDA'].apply(lambda x: torch.tensor(x).view(1,-1))))) ## 일부 세션 데이터만 사용하므로, 길이를 맞춰주기 위해 일부 slicing함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15206 3259 3258\n",
      "Training Data Size : 15206\n",
      "Validation Data Size : 3258\n",
      "Testing Data Size : 3259\n"
     ]
    }
   ],
   "source": [
    "# Train/validation/test 데이터 분할\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(dataset_size * 0.7)\n",
    "validation_size = int(dataset_size * 0.15)\n",
    "test_size = dataset_size - train_size - validation_size\n",
    "\n",
    "train_dataset, validation_dataset, test_dataset = random_split(dataset, [train_size, validation_size, test_size])\n",
    "\n",
    "print(train_size, test_size, validation_size)\n",
    "print(f\"Training Data Size : {len(train_dataset)}\")\n",
    "print(f\"Validation Data Size : {len(validation_dataset)}\")\n",
    "print(f\"Testing Data Size : {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([149, 1024]), torch.Size([33]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data size\n",
    "merged_embeddings['wav'][0].shape , torch.Tensor(merged_dataset['EDA'][0]).shape\n",
    "# raw_dataset[session]['wav_embeddings'][0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader 선언\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size=BATCH_SIZE, \n",
    "                              shuffle=True, \n",
    "                              drop_last=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, \n",
    "                                   batch_size=BATCH_SIZE, \n",
    "                                   shuffle=True, \n",
    "                                   drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, \n",
    "                             batch_size=BATCH_SIZE, \n",
    "                             shuffle=True, \n",
    "                             drop_last=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NetWork 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNetwork_pre(nn.Module):\n",
    "    def __init__(self, input_length, input_width):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_length*input_width, 768)\n",
    "        self.gelu1 = nn.GELU()\n",
    "        self.bn1 = nn.BatchNorm1d(768)\n",
    "        self.fc2 = nn.Linear(768, 512)\n",
    "        self.gelu2 = nn.GELU()\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.fc3 = nn.Linear(512, 32)\n",
    "        self.gelu3 = nn.GELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.gelu1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.gelu2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.fc3(x)\n",
    "        output = self.gelu3(x)\n",
    "        return output\n",
    "    \n",
    "class ConvNetwork_pre(nn.Module):\n",
    "    def __init__(self, input_channel):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels = input_channel, out_channels= 32, kernel_size = 3, padding = 1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(in_channels = 32, out_channels = 10, kernel_size = 3, padding = 1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        output = self.relu2(x)\n",
    "        return output\n",
    "\n",
    "class ConvNetwork_final(nn.Module):\n",
    "    def __init__(self, input_channel):\n",
    "        super().__init__()\n",
    "        self.conv2d_1 = nn.Conv2d(in_channels = input_channel, out_channels = 64, kernel_size=2)\n",
    "        self.leakyrelu_1 = nn.LeakyReLU()\n",
    "        self.maxpool2d_1 = nn.MaxPool2d(2)\n",
    "        self.conv2d_2 = nn.Conv2d(in_channels = 64, out_channels = 32, kernel_size=2)\n",
    "        self.leakyrelu_2 = nn.LeakyReLU()\n",
    "        self.maxpool2d_2 = nn.MaxPool2d(2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(224, 64)\n",
    "        self.leakyrelu_3 = nn.LeakyReLU()\n",
    "        self.batchnorm = nn.BatchNorm1d(64)\n",
    "        self.drop = nn.Dropout(p=0.25)\n",
    "        self.fc2 = nn.Linear(64, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv2d_1(x)\n",
    "        x = self.leakyrelu_1(x)\n",
    "        x = self.maxpool2d_1(x)\n",
    "        x = self.conv2d_2(x)\n",
    "        x = self.leakyrelu_2(x)\n",
    "        x = self.maxpool2d_2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.leakyrelu_3(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.drop(x)\n",
    "        output = self.fc2(x)  \n",
    "        return output\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorFusionMixer(nn.Module):\n",
    "    def __init__(self, ModelA, ModelB, ModelC, ModelD, ModelE):\n",
    "        super().__init__()\n",
    "        self.ModelA = ModelA\n",
    "        self.ModelB = ModelB\n",
    "        self.ModelC = ModelC\n",
    "        self.ModelD = ModelD\n",
    "        self.Model_cnn_final = ModelE\n",
    "        # self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def tensor_fusion(self, batch_arr1, batch_arr2, batch_arr3):\n",
    "        fusion_matrix_lst = []\n",
    "        for i, (arr1, arr2, arr3) in enumerate(zip(batch_arr1, batch_arr2, batch_arr3)):\n",
    "            arr1 = arr1.unsqueeze(-1).unsqueeze(-1)\n",
    "            arr2 = arr2.unsqueeze(0).unsqueeze(-1)\n",
    "            arr3 = arr3.squeeze().unsqueeze(0).unsqueeze(0)\n",
    "            \n",
    "            # outer_matrix = torch.einsum('i,j,kp->ijk', arr1, arr2, arr3)\n",
    "            kron_matrix = torch.kron(torch.kron(arr1,arr2), arr3)\n",
    "            l, w, d = kron_matrix.shape\n",
    "            \n",
    "            kron_matrix = kron_matrix.view(-1, l, w, d)\n",
    "            fusion_matrix_lst.append(kron_matrix)\n",
    "            \n",
    "        fusion_matrix = torch.concat(fusion_matrix_lst)\n",
    "        # fusion_matrix = fusion_matrix.unsqueeze(-1)\n",
    "        \n",
    "        return fusion_matrix\n",
    "        \n",
    "    def forward(self, x1, x2, x3, x4):\n",
    "        x1 = self.ModelA(x1)\n",
    "        x2 = self.ModelB(x2)\n",
    "        x3 = self.ModelC(x3)\n",
    "        x4 = self.ModelD(x4)\n",
    "        \n",
    "        x5 = torch.cat([x3,x4], dim=0)\n",
    "        fusion_matrix = self.tensor_fusion(x1, x2, x5)\n",
    "        \n",
    "        output = self.Model_cnn_final(fusion_matrix) \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 2 GPUs!\n",
      "DataParallel(\n",
      "  (module): TensorFusionMixer(\n",
      "    (ModelA): MLPNetwork_pre(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=61440, out_features=768, bias=True)\n",
      "      (gelu1): GELU(approximate='none')\n",
      "      (bn1): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc2): Linear(in_features=768, out_features=512, bias=True)\n",
      "      (gelu2): GELU(approximate='none')\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc3): Linear(in_features=512, out_features=32, bias=True)\n",
      "      (gelu3): GELU(approximate='none')\n",
      "    )\n",
      "    (ModelB): MLPNetwork_pre(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=152576, out_features=768, bias=True)\n",
      "      (gelu1): GELU(approximate='none')\n",
      "      (bn1): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc2): Linear(in_features=768, out_features=512, bias=True)\n",
      "      (gelu2): GELU(approximate='none')\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc3): Linear(in_features=512, out_features=32, bias=True)\n",
      "      (gelu3): GELU(approximate='none')\n",
      "    )\n",
      "    (ModelC): ConvNetwork_pre(\n",
      "      (conv1): Conv1d(50, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (relu1): ReLU()\n",
      "      (conv2): Conv1d(32, 10, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (relu2): ReLU()\n",
      "    )\n",
      "    (ModelD): ConvNetwork_pre(\n",
      "      (conv1): Conv1d(50, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (relu1): ReLU()\n",
      "      (conv2): Conv1d(32, 10, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (relu2): ReLU()\n",
      "    )\n",
      "    (Model_cnn_final): ConvNetwork_final(\n",
      "      (conv2d_1): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "      (leakyrelu_1): LeakyReLU(negative_slope=0.01)\n",
      "      (maxpool2d_1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (conv2d_2): Conv2d(64, 32, kernel_size=(2, 2), stride=(1, 1))\n",
      "      (leakyrelu_2): LeakyReLU(negative_slope=0.01)\n",
      "      (maxpool2d_2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=224, out_features=64, bias=True)\n",
      "      (leakyrelu_3): LeakyReLU(negative_slope=0.01)\n",
      "      (batchnorm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop): Dropout(p=0.25, inplace=False)\n",
      "      (fc2): Linear(in_features=64, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# txt_input_length, txt_input_width = raw_dataset[session]['text_embeddings'][0].shape | 마지막엔 지울 것\n",
    "# _, wav_input_length, wav_input_width = raw_dataset[session]['wav_embeddings'][0].shape\n",
    "txt_input_length, txt_input_width = dataset.text_embeddings[0].shape\n",
    "wav_input_length, wav_input_width = dataset.wav_embeddings[0].shape\n",
    "temp_input_length = dataset.temp[0].shape[0]\n",
    "eda_input_length = dataset.eda[0].shape[0]\n",
    "# temp_input_length = 1\n",
    "# eda_input_length = 1\n",
    "\n",
    "# tf_mixer에 들어갈 wav mlp, txt mlp 선언\n",
    "model_mlp_txt = MLPNetwork_pre(txt_input_length, txt_input_width).to(device)\n",
    "model_mlp_wav = MLPNetwork_pre(wav_input_length, wav_input_width).to(device)\n",
    "model_conv_temp = ConvNetwork_pre(temp_input_length).to(device)\n",
    "model_conv_eda = ConvNetwork_pre(eda_input_length).to(device)\n",
    "\n",
    "model_cnn_final = ConvNetwork_final(32).to(device)\n",
    "\n",
    "# 최종 모델 선언\n",
    "model_tf_cnn_mixer = TensorFusionMixer(ModelA = model_mlp_txt, \n",
    "                                   ModelB = model_mlp_wav,\n",
    "                                   ModelC = model_conv_temp,\n",
    "                                   ModelD = model_conv_eda,\n",
    "                                   ModelE = model_cnn_final).to(device)\n",
    "\n",
    "# model 병렬 학습 처리\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model_mlp_txt = nn.DataParallel(model_mlp_txt).to(device)\n",
    "    model_mlp_wav = nn.DataParallel(model_mlp_wav).to(device)\n",
    "    model_conv_temp = nn.DataParallel(model_conv_temp).to(device)\n",
    "    model_conv_eda = nn.DataParallel(model_conv_eda).to(device)\n",
    "    model_tf_cnn_mixer = nn.DataParallel(model_tf_cnn_mixer).to(device)\n",
    "    \n",
    "print(model_tf_cnn_mixer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def covariance(x, y):\n",
    "    x_mean = torch.mean(x)\n",
    "    y_mean = torch.mean(y)\n",
    "    return torch.mean((x - x_mean) * (y - y_mean))\n",
    "\n",
    "def ccc(arousal_valence_A, arousal_valence_B):\n",
    "    # Calculate means\n",
    "    mean_A = torch.mean(arousal_valence_A, axis=0)\n",
    "    mean_B = torch.mean(arousal_valence_B, axis=0)\n",
    "\n",
    "    # Calculate variances\n",
    "    var_A = torch.var(arousal_valence_A, axis=0, unbiased=True)\n",
    "    var_B = torch.var(arousal_valence_B, axis=0, unbiased=True)\n",
    "\n",
    "    # Calculate covariance\n",
    "    cov_arousal = covariance(arousal_valence_A[:, 0], arousal_valence_B[:, 0])\n",
    "    cov_valence = covariance(arousal_valence_A[:, 1], arousal_valence_B[:, 1])\n",
    "\n",
    "    # Calculate CCC for arousal and valence components\n",
    "    ccc_arousal = (2 * cov_arousal) / (var_A[0] + var_B[0] + (mean_A[0] - mean_B[0]) ** 2)\n",
    "    ccc_valence = (2 * cov_valence) / (var_A[1] + var_B[1] + (mean_A[1] - mean_B[1]) ** 2)\n",
    "\n",
    "    # Average the CCCs for arousal and valence components\n",
    "    ccc = (ccc_arousal + ccc_valence) / 2\n",
    "\n",
    "    return ccc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCCLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self,inputs,targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        # pred_v = inputs[:,0]\n",
    "        # pred_a = inputs[:,1]\n",
    "        # y_v  = targets[:,0]\n",
    "        # y_a  = targets[:,1]\n",
    "\n",
    "        return 1 - ccc(self.inputs, self.targets )\n",
    "    \n",
    "cccl = CCCLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9724)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (256,2)\n",
    "p = torch.rand(shape)\n",
    "q = torch.rand(shape)\n",
    "\n",
    "a = cccl(p,q)\n",
    "a"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습을 위한 train, test method 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)    \n",
    "    # data 순서: text_embeddings, wav_embeddings, temp, eda, label_emotion, label_emotion_ext, label_arousal, label_valence\n",
    "\n",
    "    for batch, (X_txt, X_wav, X_temp, X_eda, \n",
    "                    label_emotion, label_emotion_ext, label_arousal, label_valence) in enumerate(dataloader): \n",
    "\n",
    "        # 예측 오류 계산 \n",
    "        X_txt, X_wav, X_temp, X_eda, y_v, y_a= X_txt.to(device), X_wav.to(device), X_temp.to(device), X_eda.to(device),label_valence.type(torch.float32).to(device), label_arousal.type(torch.float32).to(device)\n",
    "        \n",
    "        X_temp = X_temp.unsqueeze(dim=-1)\n",
    "        X_eda = X_eda.unsqueeze(dim=-1)\n",
    "        y_v = y_v.unsqueeze(dim=-1)\n",
    "        y_a = y_a.unsqueeze(dim=-1)\n",
    "\n",
    "        pred = model(X_txt, X_wav, X_temp, X_eda)\n",
    "    \n",
    "        # pred_v = pred[:,0].unsqueeze(dim=-1)\n",
    "        # pred_a = pred[:,1].unsqueeze(dim=-1)\n",
    "        \n",
    "        y = torch.concat([y_v, y_a], dim=-1)\n",
    "\n",
    "        \n",
    "        # # loss_a = loss_fn(pred_a, y_a)\n",
    "        # # loss_v = loss_fn(pred_v, y_v)\n",
    "        # loss = loss_fn(pred, y)\n",
    "        \n",
    "        # pred_ccc_v= ccc(pred_v, y_v)\n",
    "        # pred_ccc_a = ccc(pred_a, y_a)\n",
    "        \n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        \n",
    "        # ccc_mean = (pred_ccc_a + pred_ccc_v) / 2\n",
    "\n",
    "        # 역전파\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # loss_a.backward(retain_graph = True)\n",
    "        # loss_v.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X_txt)\n",
    "            # loss_a, loss_v, current = loss_a.item(), loss_v.item(), batch * len(X_txt)\n",
    "            # print(f\"loss_a: {loss_a:>7f}, loss_v: {loss_v:>7f}\")\n",
    "            print(f\"loss: {loss}\")\n",
    "            # print(f\"ccc_mean : {ccc_mean.item():>9f}, Arousal_ccc : {pred_ccc_a.item():>9f}, Valence_ccc : {pred_ccc_v.item():>9f}\".format())\n",
    "        \n",
    "    # wandb.log({'loss':loss,'ccc_mean':ccc_mean.item(),'ccc_valence' : pred_ccc_v.item(),'ccc_arousal' : pred_ccc_a.item()})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn, mode = 'test'):\n",
    "    \n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    # test_loss_a = 0\n",
    "    # test_loss_v = 0\n",
    "    ccc_mean = 0\n",
    "    ccc_a = 0\n",
    "    ccc_v = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # data 순서: text_embeddings, wav_embeddings, temp, eda, label_emotion, label_emotion_ext, label_arousal, label_valence\n",
    "        for batch, (X_txt, X_wav, X_temp, X_eda, \n",
    "                    label_emotion, label_emotion_ext, label_arousal, label_valence) in enumerate(dataloader): \n",
    "\n",
    "            # 예측 오류 계산 \n",
    "            X_txt, X_wav, X_temp, X_eda, y_v, y_a= X_txt.to(device), X_wav.to(device), X_temp.to(device), X_eda.to(device),label_valence.type(torch.float32).to(device), label_arousal.type(torch.float32).to(device)\n",
    "            \n",
    "            X_temp = X_temp.unsqueeze(dim=-1)\n",
    "            X_eda = X_eda.unsqueeze(dim=-1)\n",
    "            y_v = y_v.unsqueeze(dim=-1)\n",
    "            y_a = y_a.unsqueeze(dim=-1)\n",
    "\n",
    "            pred = model(X_txt, X_wav, X_temp, X_eda)\n",
    "            \n",
    "            # pred_v = pred[:,0].unsqueeze(dim=-1)\n",
    "            # pred_a = pred[:,1].unsqueeze(dim=-1)\n",
    "            y = torch.concat([y_v, y_a], dim=-1)\n",
    "\n",
    "            \n",
    "            loss = loss_fn(pred, y)\n",
    "            test_loss += loss\n",
    "            \n",
    "            # loss_a = loss_fn(pred_a, y_a)\n",
    "            # loss_v = loss_fn(pred_v, y_v)\n",
    "\n",
    "            # test_loss_a += loss_a\n",
    "            # test_loss_v += loss_v\n",
    "\n",
    "            # pred_ccc_a = ccc(pred_a, y_a)\n",
    "            # pred_ccc_v = ccc(pred_v, y_v)\n",
    "            \n",
    "            # ccc_a += pred_ccc_a\n",
    "            # ccc_v += pred_ccc_v\n",
    "      \n",
    "      \n",
    "    test_loss /= num_batches  \n",
    "    # test_loss_v /= num_batches    \n",
    "    # test_loss_a /= num_batches\n",
    "    \n",
    "    # ccc_v /= num_batches\n",
    "    # ccc_a /= num_batches\n",
    "    # ccc_mean = (ccc_a + ccc_v) / 2\n",
    "\n",
    "    \n",
    "    if mode == 'test':\n",
    "        # print(f\"loss_a: {loss_a:>7f}, loss_v: {loss_v:>7f}\")\n",
    "        print(f\"loss: {loss.item():>7f}\")\n",
    "        # print(f\"Test ccc_mean : {ccc_mean.item():>9f}, Arousal_ccc : {pred_ccc_a.item():>9f}, Valence_ccc : {pred_ccc_v.item():>9f}\")\n",
    "    elif mode == 'val':\n",
    "        # print(f\"loss_a: {loss_a:>7f}, loss_v: {loss_v:>7f}\")\n",
    "        print(f\"loss: {loss.item():>7f}\")\n",
    "        # print(f\"Val ccc_mean : {ccc_mean.item():>9f}, Arousal_ccc : {pred_ccc_a.item():>9f}, Valence_ccc : {pred_ccc_v.item():>9f}\")\n",
    "        # wandb.log({'loss':loss,'ccc_mean':ccc_mean.item(),'ccc_valence' : pred_ccc_v.item(),'ccc_arousal' : pred_ccc_a.item()})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 지난 저장한 모델이 있다면\n",
    "# PATH = './data/test_model.pkl'\n",
    "# model_tf_mixer = torch.load(PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start training mlp fusion mixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Training Parameters\n",
    "\n",
    "criterion = CCCLoss() #nn.MSELoss()\n",
    "optimizer = optim.Adagrad(model_tf_cnn_mixer.parameters(), lr=LR) # regression\n",
    "epochs = EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CCCLoss'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wandb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m run \u001b[39m=\u001b[39m wandb\u001b[39m.\u001b[39minit(\n\u001b[1;32m      2\u001b[0m     \u001b[39m# Set the project where this run will be logged\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     project\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mETRI_regression\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[39m# Track hyperparameters and run metadata\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     config\u001b[39m=\u001b[39m{\n\u001b[1;32m      6\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m\"\u001b[39m: lr,\n\u001b[1;32m      7\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m\"\u001b[39m: epochs,\n\u001b[1;32m      8\u001b[0m     })\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wandb' is not defined"
     ]
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"ETRI_regression\",\n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"Learning rate\": LR,\n",
    "        \"Epochs\": EPOCHS,\n",
    "        \"Optimizer\": optimizer.__class__.__name__,\n",
    "        \"Loss\": criterion.__class__.__name__\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Epoch 1----------------\n",
      "Training...............\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m---------------Epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m----------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTraining...............\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m train(train_dataloader, model_tf_cnn_mixer, criterion, optimizer)\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mValidation.............\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m test(validation_dataloader, model_tf_cnn_mixer, criterion, mode \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[44], line 38\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39m# ccc_mean = (pred_ccc_a + pred_ccc_v) / 2\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \n\u001b[1;32m     36\u001b[0m \u001b[39m# 역전파\u001b[39;00m\n\u001b[1;32m     37\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 38\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     39\u001b[0m \u001b[39m# loss_a.backward(retain_graph = True)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39m# loss_v.backward()\u001b[39;00m\n\u001b[1;32m     41\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/project/kyungho/multi_modal/.venv/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/project/kyungho/multi_modal/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(f\"---------------Epoch {epoch+1}----------------\")\n",
    "    print(\"Training...............\")\n",
    "    train(train_dataloader, \n",
    "          model_tf_cnn_mixer, \n",
    "          criterion, \n",
    "          optimizer)\n",
    "    print(\"Validation.............\")\n",
    "    test(validation_dataloader, \n",
    "         model_tf_cnn_mixer, \n",
    "         criterion, \n",
    "         mode='val')\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실험을 위해 모델 저장\n",
    "PATH = '.model/data/paradeigma_test_model_multilabel_CNN.pkl'\n",
    "torch.save(model_tf_cnn_mixer, PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic TensorFusionNet 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.510035\n",
      "Test ccc_mean :  0.231434, Arousal_ccc :  0.393748, Valence_ccc :  0.167040\n"
     ]
    }
   ],
   "source": [
    "test(test_dataloader, model_tf_cnn_mixer, criterion, mode = 'test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bdc1fd12ca460d5768d71e9df3d9063ef832ce64a62e55a1a523c8c99752868e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
