{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# working model for tensorfusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu117\n"
     ]
    }
   ],
   "source": [
    "# 필요 라이브러리\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "from torchmetrics import F1Score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import random\n",
    "\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 15\n",
    "LR = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = \"/home/arplab/project/paradeigma/multi_modal/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Annotation, embedded data 불러오기\n",
    "with open(ROOT + 'model/data/paradeigma_KEMDY20_annotation_nonmissing.pkl', 'rb') as f:\n",
    "    annotation_20_nonmissing = pickle.load(f)\n",
    "\n",
    "with open(ROOT + 'model/data/paradeigma_KEMDY19_annotation_nonmissing.pkl', 'rb') as f:\n",
    "    annotation_19_nonmissing = pickle.load(f)\n",
    "    \n",
    "with open(ROOT + 'model/data/paradeigma_KEMDY20_embedding_for_dataset.pkl', 'rb') as f:\n",
    "    embedding_20_dataset = pickle.load(f)\n",
    "\n",
    "with open(ROOT + 'model/data/paradeigma_KEMDY19_embedding_for_dataset.pkl', 'rb') as f:\n",
    "    embedding_19_dataset = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19년도 20년도 embedding 병합\n",
    "\n",
    "wav_19_em = []\n",
    "for embedding in embedding_19_dataset[0].values():\n",
    "    wav_19_em.append(embedding)\n",
    "wav_19_em = torch.cat(wav_19_em) \n",
    "    \n",
    "txt_19_em = []\n",
    "for embedding in embedding_19_dataset[1].values():\n",
    "    txt_19_em.append(embedding)\n",
    "txt_19_em = torch.cat(txt_19_em) \n",
    "\n",
    "wav_20_em = []\n",
    "for embedding in embedding_20_dataset[0].values():\n",
    "    wav_20_em.append(embedding)\n",
    "wav_20_em = torch.cat(wav_20_em) \n",
    "    \n",
    "txt_20_em = []\n",
    "for embedding in embedding_20_dataset[1].values():\n",
    "    txt_20_em.append(embedding)\n",
    "txt_20_em = torch.cat(txt_20_em) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9008, 149, 1024])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav_19_em.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19년도 nonneutral data embedding\n",
    "nonneutral_19_index = pd.Series(annotation_19_nonmissing[annotation_19_nonmissing['Emotion'] != 'neutral'].index)\n",
    "nonneutral_19_em_wav = []\n",
    "nonneutral_19_em_txt = []\n",
    "for index in nonneutral_19_index:\n",
    "    nonneutral_19_em_wav.append(wav_20_em[index].view(1, 149, 1024))\n",
    "    nonneutral_19_em_txt.append(txt_20_em[index].view(1, 80, 768))\n",
    "    \n",
    "nonneutral_19_em_wav = torch.concat(nonneutral_19_em_wav, dim=0)\n",
    "nonneutral_19_em_txt = torch.concat(nonneutral_19_em_txt, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sess01',\n",
       " 'Sess02',\n",
       " 'Sess03',\n",
       " 'Sess04',\n",
       " 'Sess08',\n",
       " 'Sess09',\n",
       " 'Sess10',\n",
       " 'Sess13',\n",
       " 'Sess14',\n",
       " 'Sess15',\n",
       " 'Sess16',\n",
       " 'Sess18',\n",
       " 'Sess19',\n",
       " 'Sess21',\n",
       " 'Sess22',\n",
       " 'Sess23',\n",
       " 'Sess26',\n",
       " 'Sess33',\n",
       " 'Sess34',\n",
       " 'Sess36',\n",
       " 'Sess37',\n",
       " 'Sess38']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session_20_list = list(embedding_20_dataset[0].keys())\n",
    "test_val = random.sample(session_20_list, k=16)\n",
    "test_session = sorted(test_val[:8])\n",
    "val_session = sorted(test_val[8:])\n",
    "train_session = sorted(list(set(session_20_list) - (set(test_session) | set(val_session))))\n",
    "train_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_219623/3324316.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df.append(df, ignore_index=True)\n",
      "/tmp/ipykernel_219623/3324316.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  val_df = val_df.append(df, ignore_index=True)\n",
      "/tmp/ipykernel_219623/3324316.py:33: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  test_df = test_df.append(df, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "def session_check(segment_id, session):\n",
    "    if session in segment_id:\n",
    "        return True\n",
    "    else : return False\n",
    "\n",
    "train_df = pd.DataFrame()\n",
    "train_embedding_wav = []\n",
    "train_embedding_txt = []\n",
    "for session in train_session:    \n",
    "    df = annotation_20_nonmissing[annotation_20_nonmissing['Segment ID'].apply(session_check, session=session)]\n",
    "    train_df = train_df.append(df, ignore_index=True)\n",
    "    train_embedding_wav.append(embedding_20_dataset[0][session])\n",
    "    train_embedding_txt.append(embedding_20_dataset[1][session])\n",
    "train_embedding_wav = torch.concat(train_embedding_wav)\n",
    "train_embedding_txt = torch.concat(train_embedding_txt)\n",
    "    \n",
    "val_df = pd.DataFrame()\n",
    "val_embedding_wav = []\n",
    "val_embedding_txt = []\n",
    "for session in val_session:    \n",
    "    df = annotation_20_nonmissing[annotation_20_nonmissing['Segment ID'].apply(session_check, session=session)]\n",
    "    val_df = val_df.append(df, ignore_index=True)\n",
    "    val_embedding_wav.append(embedding_20_dataset[0][session])\n",
    "    val_embedding_txt.append(embedding_20_dataset[1][session])\n",
    "val_embedding_wav = torch.concat(val_embedding_wav)\n",
    "val_embedding_txt = torch.concat(val_embedding_txt)\n",
    "    \n",
    "test_df = pd.DataFrame()\n",
    "test_embedding_wav = []\n",
    "test_embedding_txt = []\n",
    "for session in test_session:    \n",
    "    df = annotation_20_nonmissing[annotation_20_nonmissing['Segment ID'].apply(session_check, session=session)]\n",
    "    test_df = test_df.append(df, ignore_index=True)\n",
    "    test_embedding_wav.append(embedding_20_dataset[0][session])\n",
    "    test_embedding_txt.append(embedding_20_dataset[1][session])\n",
    "test_embedding_wav = torch.concat(test_embedding_wav)\n",
    "test_embedding_txt = torch.concat(test_embedding_txt)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_219623/4029699924.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df.append(annotation_19_nonmissing.loc[nonneutral_19_index], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 12842 torch.Size([12842, 149, 1024]) torch.Size([12842, 80, 768])\n",
      "Val 2434 torch.Size([2434, 149, 1024]) torch.Size([2434, 80, 768])\n",
      "Test 2976 torch.Size([2976, 149, 1024]) torch.Size([2976, 80, 768])\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df.append(annotation_19_nonmissing.loc[nonneutral_19_index], ignore_index=True)\n",
    "\n",
    "train_embedding_wav = torch.concat([train_embedding_wav, nonneutral_19_em_wav])\n",
    "train_embedding_txt = torch.concat([train_embedding_txt, nonneutral_19_em_txt])\n",
    "\n",
    "print('Train',len(train_df), train_embedding_wav.shape, train_embedding_txt.shape)\n",
    "print('Val',len(val_df), val_embedding_wav.shape, val_embedding_txt.shape)\n",
    "print('Test',len(test_df), test_embedding_wav.shape, test_embedding_txt.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_dict = {'angry':0, 'disgust':1, 'fear':2,'happy':3,'neutral':4, 'sad':5, 'surprise':6,  \n",
    "               'neutral;surprise': 450, 'neutral;sad': 460, 'happy;neutral': 340, \n",
    "               'angry;neutral': 40, 'disgust;neutral': 140, 'fear;neutral': 240, \n",
    "               'happy;surprise': 350, 'angry;happy;neutral': 7340, 'angry;disgust': 10, \n",
    "               'happy;neutral;surprise': 3450, 'fear;happy': 230,'fear;happy;neutral': 2340,\n",
    "               'angry;disgust;neutral': 7140, 'disgust;neutral;sad': 1460, \n",
    "               'happy;sad': 360, 'disgust;happy;neutral': 3410, 'angry;fear': 20, 'angry;fear;neutral':7240,\n",
    "               'angry;fear;surprise': 7250, 'angry;happy': 730, 'angry;neutral;surprise':7450, \n",
    "               'angry;sad': 60, 'angry;surprise': 50, 'disgust;fear':120, 'disgust;happy': 130,\n",
    "               'disgust;neutral;surprise':1450, 'disgust;sad': 160, 'disgust;surprise':150, \n",
    "               'fear;neutral;surprise':2450, 'fear;sad': 260, 'fear;surprise':250, 'happy;neutral;sad':3460,\n",
    "               'neutral;sad;surprise':4560, 'sad;surprise': 560,\n",
    "               'angry;disgust;fear;neutral;sad': 10000,'angry;disgust;fear;neutral;surprise':20000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprise': 6, 'angry;disgust': 10, 'angry;fear': 20, 'angry;neutral': 40, 'angry;surprise': 50, 'angry;sad': 60, 'disgust;fear': 120, 'disgust;happy': 130, 'disgust;neutral': 140, 'disgust;surprise': 150, 'disgust;sad': 160, 'fear;happy': 230, 'fear;neutral': 240, 'fear;surprise': 250, 'fear;sad': 260, 'happy;neutral': 340, 'happy;surprise': 350, 'happy;sad': 360, 'neutral;surprise': 450, 'neutral;sad': 460, 'sad;surprise': 560, 'angry;happy': 730, 'disgust;neutral;surprise': 1450, 'disgust;neutral;sad': 1460, 'fear;happy;neutral': 2340, 'fear;neutral;surprise': 2450, 'disgust;happy;neutral': 3410, 'happy;neutral;surprise': 3450, 'happy;neutral;sad': 3460, 'neutral;sad;surprise': 4560, 'angry;disgust;neutral': 7140, 'angry;fear;neutral': 7240, 'angry;fear;surprise': 7250, 'angry;happy;neutral': 7340, 'angry;neutral;surprise': 7450, 'angry;disgust;fear;neutral;sad': 10000, 'angry;disgust;fear;neutral;surprise': 20000}\n"
     ]
    }
   ],
   "source": [
    "encode_dict = {k: v for k, v in sorted(encode_dict.items(), key=lambda item: item[1])}\n",
    "decode_dict = {v: k for k, v in sorted(encode_dict.items(), key=lambda item: item[1])}\n",
    "print(encode_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Emotion'] = train_df['Emotion'].apply(lambda x: encode_dict[x])\n",
    "val_df['Emotion'] = val_df['Emotion'].apply(lambda x: encode_dict[x])\n",
    "test_df['Emotion'] = test_df['Emotion'].apply(lambda x: encode_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding 함수\n",
    "def sequence_padding(ts_list, padding_length = 50, mode = 'constant'):\n",
    "    \n",
    "    padding_value=0\n",
    "    \n",
    "    if (type(ts_list) != type([])) :\n",
    "        ts_list = [padding_value] * padding_length\n",
    "    \n",
    "    elif len(ts_list) >= padding_length :\n",
    "        ts_list = ts_list[0:padding_length]\n",
    "    \n",
    "    elif mode == 'constant':\n",
    "        length = padding_length - len(ts_list)\n",
    "        extend_list = [padding_value] * length\n",
    "        ts_list = ts_list + extend_list    \n",
    "\n",
    "    elif mode == 'replicate':\n",
    "        \n",
    "        quotient = padding_length // len(ts_list)\n",
    "        remainder = padding_length % len(ts_list)\n",
    "        result = ts_list * quotient\n",
    "        result += ts_list[:remainder]\n",
    "        ts_list = result\n",
    "\n",
    "    return ts_list            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Segment ID</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Valence</th>\n",
       "      <th>Arousal</th>\n",
       "      <th>emotion_vector</th>\n",
       "      <th>valence_vector</th>\n",
       "      <th>arousal_vector</th>\n",
       "      <th>EDA</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>EDA length</th>\n",
       "      <th>TEMP length</th>\n",
       "      <th>Scaled EDA</th>\n",
       "      <th>Scaled TEMP</th>\n",
       "      <th>Scaled EDA length</th>\n",
       "      <th>Scaled TEMP length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sess01_script01_User002M_001</td>\n",
       "      <td>4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>[0, 0, 0, 0, 10, 0, 0]</td>\n",
       "      <td>[0, 0, 6, 4, 0]</td>\n",
       "      <td>[0, 1, 9, 0, 0]</td>\n",
       "      <td>[-0.06791500000000017, -0.110201, -0.025627999...</td>\n",
       "      <td>[34.81, 34.81, 34.81, 34.79, 34.79, 34.79, 34....</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>[-0.3956120608509403, -0.6460741139849967, -0....</td>\n",
       "      <td>[-0.5852354223355396, -0.5852354223355396, -0....</td>\n",
       "      <td>31</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sess01_script01_User002M_002</td>\n",
       "      <td>4</td>\n",
       "      <td>3.1</td>\n",
       "      <td>2.9</td>\n",
       "      <td>[0, 0, 0, 0, 10, 0, 0]</td>\n",
       "      <td>[0, 0, 9, 1, 0]</td>\n",
       "      <td>[0, 1, 9, 0, 0]</td>\n",
       "      <td>[0.006407000000000274, -0.021784000000000248, ...</td>\n",
       "      <td>[34.79, 34.79, 34.79, 34.77, 34.77, 34.77, 34....</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>[0.04460079241088465, -0.12237588402818134, -0...</td>\n",
       "      <td>[-0.7735061037287296, -0.7735061037287296, -0....</td>\n",
       "      <td>46</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sess01_script01_User002M_003</td>\n",
       "      <td>4</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[0, 0, 0, 0, 10, 0, 0]</td>\n",
       "      <td>[0, 0, 9, 1, 0]</td>\n",
       "      <td>[0, 0, 10, 0, 0]</td>\n",
       "      <td>[0.07944700000000005, 0.05125599999999997, 0.0...</td>\n",
       "      <td>[34.75, 34.75, 34.75, 34.75, 34.79, 34.79, 34....</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>[0.4772202967601137, 0.31024362032105035, 0.27...</td>\n",
       "      <td>[-1.1500474665150429, -1.1500474665150429, -1....</td>\n",
       "      <td>32</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sess01_script01_User002M_004</td>\n",
       "      <td>4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>[0, 0, 0, 1, 9, 0, 0]</td>\n",
       "      <td>[0, 0, 3, 7, 0]</td>\n",
       "      <td>[0, 0, 9, 1, 0]</td>\n",
       "      <td>[0.0038439999999999586, -0.03331699999999982, ...</td>\n",
       "      <td>[34.77, 34.77, 34.77, 34.77, 34.77, 34.77, 34....</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>[0.029420017634772744, -0.19068640899613712, -...</td>\n",
       "      <td>[-0.9617767851218528, -0.9617767851218528, -0....</td>\n",
       "      <td>44</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sess01_script01_User001F_001</td>\n",
       "      <td>4</td>\n",
       "      <td>3.8</td>\n",
       "      <td>2.8</td>\n",
       "      <td>[0, 0, 0, 4, 6, 0, 0]</td>\n",
       "      <td>[0, 0, 3, 6, 1]</td>\n",
       "      <td>[0, 2, 8, 0, 0]</td>\n",
       "      <td>[0.1241910000000006, 0.09474499999999964, 0.03...</td>\n",
       "      <td>[34.55, 34.55, 34.55, 34.55, 34.55, 34.55, 34....</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>[1.8771197424131327, 1.4195480026451441, 0.544...</td>\n",
       "      <td>[-2.4621880639731812, -2.4621880639731812, -2....</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12837</th>\n",
       "      <td>Sess20_impro04_M018</td>\n",
       "      <td>5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.1</td>\n",
       "      <td>[0, 0, 0, 0, 3, 7, 0]</td>\n",
       "      <td>[2, 4, 3, 1, 0]</td>\n",
       "      <td>[0, 9, 1, 0, 0]</td>\n",
       "      <td>[0.550024, 0.547461, 0.54618, 0.543617, 0.5461...</td>\n",
       "      <td>[31.27, 31.27, 31.27, 31.27, 31.27, 31.27, 31....</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>[-1.0596542781342695, -1.091127738705599, -1.1...</td>\n",
       "      <td>[-0.12418082175855716, -0.12418082175855716, -...</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12838</th>\n",
       "      <td>Sess20_impro04_F023</td>\n",
       "      <td>5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>[0, 0, 0, 0, 2, 8, 0]</td>\n",
       "      <td>[0, 6, 3, 1, 0]</td>\n",
       "      <td>[1, 5, 4, 0, 0]</td>\n",
       "      <td>[0.216838, 0.220682, 0.221964, 0.223245, 0.221...</td>\n",
       "      <td>[29.77, 29.77, 29.77, 29.77, 29.77, 29.77, 29....</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>[-0.8593367498709915, -0.7994211382956963, -0....</td>\n",
       "      <td>[-1.3249127202479716, -1.3249127202479716, -1....</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12839</th>\n",
       "      <td>Sess20_impro04_M019</td>\n",
       "      <td>5</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>[0, 0, 0, 0, 1, 9, 0]</td>\n",
       "      <td>[4, 2, 3, 1, 0]</td>\n",
       "      <td>[3, 6, 1, 0, 0]</td>\n",
       "      <td>[0.519269, 0.517987, 0.519269, 0.517987, 0.516...</td>\n",
       "      <td>[31.31, 31.31, 31.31, 31.31, 31.29, 31.29, 31....</td>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "      <td>[-1.437323525060235, -1.4530663953108964, -1.4...</td>\n",
       "      <td>[0.697453930425023, 0.697453930425023, 0.69745...</td>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12840</th>\n",
       "      <td>Sess20_impro04_F024</td>\n",
       "      <td>3</td>\n",
       "      <td>3.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>[0, 0, 0, 7, 3, 0, 0]</td>\n",
       "      <td>[0, 0, 1, 9, 0]</td>\n",
       "      <td>[0, 1, 6, 3, 0]</td>\n",
       "      <td>[0.207868, 0.207868, 0.210431, 0.211712, 0.212...</td>\n",
       "      <td>[29.73, 29.73, 29.73, 29.73, 29.73, 29.73, 29....</td>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "      <td>[-0.9991502347384216, -0.9991502347384216, -0....</td>\n",
       "      <td>[-1.6382728540476073, -1.6382728540476073, -1....</td>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12841</th>\n",
       "      <td>Sess20_impro04_M020</td>\n",
       "      <td>3</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[0, 0, 0, 10, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 2, 4, 4]</td>\n",
       "      <td>[0, 0, 3, 4, 3]</td>\n",
       "      <td>[0.583343, 0.610254, 0.628195, 0.651262, 0.674...</td>\n",
       "      <td>[31.33, 31.33, 31.33, 31.33, 31.37, 31.37, 31....</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>[-0.6504992907069812, -0.3200340946730135, -0....</td>\n",
       "      <td>[1.108271306516813, 1.108271306516813, 1.10827...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12842 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Segment ID  Emotion  Valence  Arousal  \\\n",
       "0      Sess01_script01_User002M_001        4      3.4      2.9   \n",
       "1      Sess01_script01_User002M_002        4      3.1      2.9   \n",
       "2      Sess01_script01_User002M_003        4      3.1      3.0   \n",
       "3      Sess01_script01_User002M_004        4      3.7      3.1   \n",
       "4      Sess01_script01_User001F_001        4      3.8      2.8   \n",
       "...                             ...      ...      ...      ...   \n",
       "12837           Sess20_impro04_M018        5      2.3      2.1   \n",
       "12838           Sess20_impro04_F023        5      2.5      2.3   \n",
       "12839           Sess20_impro04_M019        5      2.1      1.8   \n",
       "12840           Sess20_impro04_F024        3      3.9      3.2   \n",
       "12841           Sess20_impro04_M020        3      4.2      4.0   \n",
       "\n",
       "               emotion_vector   valence_vector    arousal_vector  \\\n",
       "0      [0, 0, 0, 0, 10, 0, 0]  [0, 0, 6, 4, 0]   [0, 1, 9, 0, 0]   \n",
       "1      [0, 0, 0, 0, 10, 0, 0]  [0, 0, 9, 1, 0]   [0, 1, 9, 0, 0]   \n",
       "2      [0, 0, 0, 0, 10, 0, 0]  [0, 0, 9, 1, 0]  [0, 0, 10, 0, 0]   \n",
       "3       [0, 0, 0, 1, 9, 0, 0]  [0, 0, 3, 7, 0]   [0, 0, 9, 1, 0]   \n",
       "4       [0, 0, 0, 4, 6, 0, 0]  [0, 0, 3, 6, 1]   [0, 2, 8, 0, 0]   \n",
       "...                       ...              ...               ...   \n",
       "12837   [0, 0, 0, 0, 3, 7, 0]  [2, 4, 3, 1, 0]   [0, 9, 1, 0, 0]   \n",
       "12838   [0, 0, 0, 0, 2, 8, 0]  [0, 6, 3, 1, 0]   [1, 5, 4, 0, 0]   \n",
       "12839   [0, 0, 0, 0, 1, 9, 0]  [4, 2, 3, 1, 0]   [3, 6, 1, 0, 0]   \n",
       "12840   [0, 0, 0, 7, 3, 0, 0]  [0, 0, 1, 9, 0]   [0, 1, 6, 3, 0]   \n",
       "12841  [0, 0, 0, 10, 0, 0, 0]  [0, 0, 2, 4, 4]   [0, 0, 3, 4, 3]   \n",
       "\n",
       "                                                     EDA  \\\n",
       "0      [-0.06791500000000017, -0.110201, -0.025627999...   \n",
       "1      [0.006407000000000274, -0.021784000000000248, ...   \n",
       "2      [0.07944700000000005, 0.05125599999999997, 0.0...   \n",
       "3      [0.0038439999999999586, -0.03331699999999982, ...   \n",
       "4      [0.1241910000000006, 0.09474499999999964, 0.03...   \n",
       "...                                                  ...   \n",
       "12837  [0.550024, 0.547461, 0.54618, 0.543617, 0.5461...   \n",
       "12838  [0.216838, 0.220682, 0.221964, 0.223245, 0.221...   \n",
       "12839  [0.519269, 0.517987, 0.519269, 0.517987, 0.516...   \n",
       "12840  [0.207868, 0.207868, 0.210431, 0.211712, 0.212...   \n",
       "12841  [0.583343, 0.610254, 0.628195, 0.651262, 0.674...   \n",
       "\n",
       "                                                    TEMP  EDA length  \\\n",
       "0      [34.81, 34.81, 34.81, 34.79, 34.79, 34.79, 34....          32   \n",
       "1      [34.79, 34.79, 34.79, 34.77, 34.77, 34.77, 34....          47   \n",
       "2      [34.75, 34.75, 34.75, 34.75, 34.79, 34.79, 34....          33   \n",
       "3      [34.77, 34.77, 34.77, 34.77, 34.77, 34.77, 34....          45   \n",
       "4      [34.55, 34.55, 34.55, 34.55, 34.55, 34.55, 34....           9   \n",
       "...                                                  ...         ...   \n",
       "12837  [31.27, 31.27, 31.27, 31.27, 31.27, 31.27, 31....          31   \n",
       "12838  [29.77, 29.77, 29.77, 29.77, 29.77, 29.77, 29....          46   \n",
       "12839  [31.31, 31.31, 31.31, 31.31, 31.29, 31.29, 31....          56   \n",
       "12840  [29.73, 29.73, 29.73, 29.73, 29.73, 29.73, 29....          52   \n",
       "12841  [31.33, 31.33, 31.33, 31.33, 31.37, 31.37, 31....          11   \n",
       "\n",
       "       TEMP length                                         Scaled EDA  \\\n",
       "0               32  [-0.3956120608509403, -0.6460741139849967, -0....   \n",
       "1               47  [0.04460079241088465, -0.12237588402818134, -0...   \n",
       "2               33  [0.4772202967601137, 0.31024362032105035, 0.27...   \n",
       "3               45  [0.029420017634772744, -0.19068640899613712, -...   \n",
       "4                9  [1.8771197424131327, 1.4195480026451441, 0.544...   \n",
       "...            ...                                                ...   \n",
       "12837           31  [-1.0596542781342695, -1.091127738705599, -1.1...   \n",
       "12838           46  [-0.8593367498709915, -0.7994211382956963, -0....   \n",
       "12839           56  [-1.437323525060235, -1.4530663953108964, -1.4...   \n",
       "12840           52  [-0.9991502347384216, -0.9991502347384216, -0....   \n",
       "12841           11  [-0.6504992907069812, -0.3200340946730135, -0....   \n",
       "\n",
       "                                             Scaled TEMP  Scaled EDA length  \\\n",
       "0      [-0.5852354223355396, -0.5852354223355396, -0....                 31   \n",
       "1      [-0.7735061037287296, -0.7735061037287296, -0....                 46   \n",
       "2      [-1.1500474665150429, -1.1500474665150429, -1....                 32   \n",
       "3      [-0.9617767851218528, -0.9617767851218528, -0....                 44   \n",
       "4      [-2.4621880639731812, -2.4621880639731812, -2....                  8   \n",
       "...                                                  ...                ...   \n",
       "12837  [-0.12418082175855716, -0.12418082175855716, -...                 31   \n",
       "12838  [-1.3249127202479716, -1.3249127202479716, -1....                 46   \n",
       "12839  [0.697453930425023, 0.697453930425023, 0.69745...                 56   \n",
       "12840  [-1.6382728540476073, -1.6382728540476073, -1....                 52   \n",
       "12841  [1.108271306516813, 1.108271306516813, 1.10827...                 11   \n",
       "\n",
       "       Scaled TEMP length  \n",
       "0                      32  \n",
       "1                      47  \n",
       "2                      33  \n",
       "3                      45  \n",
       "4                       9  \n",
       "...                   ...  \n",
       "12837                  31  \n",
       "12838                  46  \n",
       "12839                  56  \n",
       "12840                  52  \n",
       "12841                  11  \n",
       "\n",
       "[12842 rows x 15 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시계열 데이터 2020 + 2019\n",
    "train_df['Scaled EDA'] = train_df['Scaled EDA'].apply(sequence_padding)\n",
    "train_df['Scaled TEMP'] = train_df['Scaled TEMP'].apply(sequence_padding)\n",
    "\n",
    "val_df['Scaled EDA'] = val_df['Scaled EDA'].apply(sequence_padding)\n",
    "val_df['Scaled TEMP'] = val_df['Scaled TEMP'].apply(sequence_padding)\n",
    "\n",
    "test_df['Scaled EDA'] = test_df['Scaled EDA'].apply(sequence_padding)\n",
    "test_df['Scaled TEMP'] = test_df['Scaled TEMP'].apply(sequence_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stats.stackexchange.com/questions/107874/how-to-deal-with-a-skewed-class-in-binary-classification-having-many-features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch dataset 만들기\n",
    "- 참고: https://tutorials.pytorch.kr/beginner/basics/data_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EtriDataset(Dataset):\n",
    "    def __init__(self, file_names, \n",
    "                 text_embeddings, \n",
    "                 wav_embeddings, \n",
    "                 Temp,\n",
    "                 EDA,\n",
    "                 Emotion,\n",
    "                 Emotion_ext, \n",
    "                 Arousal, \n",
    "                 Valence):\n",
    "        self.file_names = file_names\n",
    "        self.text_embeddings = text_embeddings\n",
    "        self.wav_embeddings = wav_embeddings\n",
    "        self.temp = Temp\n",
    "        self.eda = EDA\n",
    "        self.label_emotion = Emotion\n",
    "        self.label_emotion_ext = Emotion_ext\n",
    "        self.label_arousal = Arousal\n",
    "        self.label_valence = Valence\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text_embeddings = self.text_embeddings[idx]\n",
    "        wav_embeddings = self.wav_embeddings[idx]\n",
    "        temp = self.temp[idx]\n",
    "        eda = self.eda[idx]\n",
    "        label_emotion = self.label_emotion[idx]\n",
    "        label_emotion_ext = self.label_emotion_ext[idx]\n",
    "        label_arousal = self.label_arousal[idx]\n",
    "        label_valence = self.label_valence[idx]\n",
    "        return text_embeddings, wav_embeddings, temp, eda, label_emotion, label_emotion_ext, label_arousal, label_valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Segment ID', 'Emotion', 'Valence', 'Arousal', 'emotion_vector',\n",
       "       'valence_vector', 'arousal_vector', 'EDA', 'TEMP', 'EDA length',\n",
       "       'TEMP length', 'Scaled EDA', 'Scaled TEMP', 'Scaled EDA length',\n",
       "       'Scaled TEMP length'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-5.4479e-01, -3.1275e-02, -1.0944e+00,  ..., -6.5808e-01,\n",
       "           1.1027e+00, -2.9669e-01],\n",
       "         [-6.1710e-01, -4.8289e-03, -1.0378e+00,  ..., -6.2813e-01,\n",
       "           1.1145e+00, -2.7613e-01],\n",
       "         [-5.9215e-01, -9.6105e-03, -1.1005e+00,  ..., -6.2060e-01,\n",
       "           1.1014e+00, -2.8010e-01],\n",
       "         ...,\n",
       "         [-5.5526e-01, -4.4838e-04, -1.1606e+00,  ..., -6.1127e-01,\n",
       "           9.6024e-01, -3.0464e-01],\n",
       "         [-5.5526e-01, -4.4838e-04, -1.1606e+00,  ..., -6.1127e-01,\n",
       "           9.6024e-01, -3.0464e-01],\n",
       "         [-5.5526e-01, -4.4838e-04, -1.1606e+00,  ..., -6.1127e-01,\n",
       "           9.6024e-01, -3.0464e-01]],\n",
       "\n",
       "        [[-5.2497e-01, -3.6781e-02, -1.1262e+00,  ..., -6.9652e-01,\n",
       "           1.0444e+00, -2.8279e-01],\n",
       "         [-6.1824e-01,  2.6758e-03, -1.1191e+00,  ..., -7.0198e-01,\n",
       "           1.0386e+00, -2.3687e-01],\n",
       "         [-5.8200e-01, -1.2159e-02, -1.0670e+00,  ..., -6.5854e-01,\n",
       "           1.0871e+00, -2.3476e-01],\n",
       "         ...,\n",
       "         [-5.3314e-01,  2.3818e-03, -1.1792e+00,  ..., -6.4547e-01,\n",
       "           9.1845e-01, -2.8352e-01],\n",
       "         [-5.3314e-01,  2.3818e-03, -1.1792e+00,  ..., -6.4547e-01,\n",
       "           9.1845e-01, -2.8352e-01],\n",
       "         [-5.3314e-01,  2.3818e-03, -1.1792e+00,  ..., -6.4547e-01,\n",
       "           9.1845e-01, -2.8352e-01]],\n",
       "\n",
       "        [[-5.2272e-01, -6.6727e-03, -1.1510e+00,  ..., -6.9877e-01,\n",
       "           1.1183e+00, -3.1647e-01],\n",
       "         [-6.0390e-01,  5.5554e-02, -1.1419e+00,  ..., -7.3669e-01,\n",
       "           1.1257e+00, -2.7364e-01],\n",
       "         [-5.5353e-01, -2.8113e-03, -1.1273e+00,  ..., -6.7168e-01,\n",
       "           1.1365e+00, -2.9110e-01],\n",
       "         ...,\n",
       "         [-5.3940e-01,  1.8224e-02, -1.2095e+00,  ..., -6.4355e-01,\n",
       "           9.5146e-01, -3.2822e-01],\n",
       "         [-5.3940e-01,  1.8224e-02, -1.2095e+00,  ..., -6.4355e-01,\n",
       "           9.5146e-01, -3.2822e-01],\n",
       "         [-5.3940e-01,  1.8224e-02, -1.2095e+00,  ..., -6.4355e-01,\n",
       "           9.5146e-01, -3.2822e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-5.4834e-01, -1.2905e-01, -1.0952e+00,  ..., -7.0459e-01,\n",
       "           1.0531e+00, -2.7203e-01],\n",
       "         [-6.1451e-01, -6.5626e-02, -1.0632e+00,  ..., -6.7706e-01,\n",
       "           1.0485e+00, -2.2611e-01],\n",
       "         [-5.8723e-01, -1.1054e-01, -1.0919e+00,  ..., -6.8077e-01,\n",
       "           1.0622e+00, -2.3522e-01],\n",
       "         ...,\n",
       "         [-5.6536e-01, -8.9563e-02, -1.1457e+00,  ..., -6.5344e-01,\n",
       "           9.1159e-01, -2.8490e-01],\n",
       "         [-5.6536e-01, -8.9563e-02, -1.1457e+00,  ..., -6.5344e-01,\n",
       "           9.1159e-01, -2.8490e-01],\n",
       "         [-5.6536e-01, -8.9563e-02, -1.1457e+00,  ..., -6.5344e-01,\n",
       "           9.1159e-01, -2.8490e-01]],\n",
       "\n",
       "        [[-4.7325e-01, -2.6359e-02, -1.1272e+00,  ..., -6.4084e-01,\n",
       "           1.0132e+00, -3.0245e-01],\n",
       "         [-5.4337e-01,  2.3437e-02, -1.0841e+00,  ..., -5.9555e-01,\n",
       "           1.0470e+00, -2.2657e-01],\n",
       "         [-5.4323e-01, -6.5546e-03, -1.1182e+00,  ..., -6.0792e-01,\n",
       "           1.0467e+00, -2.7886e-01],\n",
       "         ...,\n",
       "         [-4.8489e-01,  1.8075e-02, -1.1831e+00,  ..., -5.8349e-01,\n",
       "           8.8427e-01, -3.1611e-01],\n",
       "         [-4.8489e-01,  1.8075e-02, -1.1831e+00,  ..., -5.8349e-01,\n",
       "           8.8427e-01, -3.1611e-01],\n",
       "         [-4.8489e-01,  1.8075e-02, -1.1831e+00,  ..., -5.8349e-01,\n",
       "           8.8427e-01, -3.1611e-01]],\n",
       "\n",
       "        [[-4.8802e-01,  9.2065e-02, -1.1788e+00,  ..., -6.9952e-01,\n",
       "           1.0025e+00, -2.3259e-01],\n",
       "         [-5.4911e-01,  1.0532e-01, -1.1321e+00,  ..., -7.1062e-01,\n",
       "           1.0426e+00, -1.1272e-01],\n",
       "         [-5.5708e-01,  1.0552e-01, -1.2313e+00,  ..., -6.4444e-01,\n",
       "           1.0417e+00, -1.8742e-01],\n",
       "         ...,\n",
       "         [-5.0307e-01,  1.1174e-01, -1.2525e+00,  ..., -6.5395e-01,\n",
       "           8.5605e-01, -2.3562e-01],\n",
       "         [-5.0307e-01,  1.1174e-01, -1.2525e+00,  ..., -6.5395e-01,\n",
       "           8.5605e-01, -2.3562e-01],\n",
       "         [-5.0307e-01,  1.1174e-01, -1.2525e+00,  ..., -6.5395e-01,\n",
       "           8.5605e-01, -2.3562e-01]]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_embedding_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data load 및 나누기: https://076923.github.io/posts/Python-pytorch-11/\n",
    "\n",
    "# session을 통합시킨 데이터 셋을 만들었을 때\n",
    "train_dataset = EtriDataset(file_names = train_df['Segment ID'],\n",
    "                      text_embeddings = train_embedding_txt,\n",
    "                      wav_embeddings = train_embedding_wav,\n",
    "                      Emotion = train_df['Emotion'],\n",
    "                      Arousal = train_df['Arousal'],\n",
    "                      Valence = train_df['Valence'],\n",
    "                      EDA = torch.concat(list(train_df['Scaled EDA'].apply(lambda x: torch.tensor(x).view(1,-1)))), ## 일부 세션 데이터만 사용하므로, 길이를 맞춰주기 위해 일부 slicing함\n",
    "                      Temp = torch.concat(list(train_df['Scaled TEMP'].apply(lambda x: torch.tensor(x).view(1,-1)))), ## 일부 세션 데이터만 사용하므로, 길이를 맞춰주기 위해 일부 slicing함\n",
    "                      Emotion_ext = torch.concat(list(train_df['emotion_vector'].apply(lambda x: torch.tensor(x).view(1,-1))))) ## 일부 세션 데이터만 사용하므로, 길이를 맞춰주기 위해 일부 slicing함\n",
    "\n",
    "val_dataset = EtriDataset(file_names = val_df['Segment ID'],\n",
    "                      text_embeddings = val_embedding_txt,\n",
    "                      wav_embeddings = val_embedding_wav,\n",
    "                      Emotion = val_df['Emotion'],\n",
    "                      Arousal = val_df['Arousal'],\n",
    "                      Valence = val_df['Valence'],\n",
    "                      EDA = torch.concat(list(val_df['Scaled EDA'].apply(lambda x: torch.tensor(x).view(1,-1)))), ## 일부 세션 데이터만 사용하므로, 길이를 맞춰주기 위해 일부 slicing함\n",
    "                      Temp = torch.concat(list(val_df['Scaled TEMP'].apply(lambda x: torch.tensor(x).view(1,-1)))), ## 일부 세션 데이터만 사용하므로, 길이를 맞춰주기 위해 일부 slicing함\n",
    "                      Emotion_ext = torch.concat(list(val_df['emotion_vector'].apply(lambda x: torch.tensor(x).view(1,-1))))) ## 일부 세션 데이터만 사용하므로, 길이를 맞춰주기 위해 일부 slicing함\n",
    "\n",
    "test_dataset = EtriDataset(file_names = test_df['Segment ID'],\n",
    "                      text_embeddings = test_embedding_txt,\n",
    "                      wav_embeddings = test_embedding_txt,\n",
    "                      Emotion = test_df['Emotion'],\n",
    "                      Arousal = test_df['Arousal'],\n",
    "                      Valence = test_df['Valence'],\n",
    "                      EDA = torch.concat(list(test_df['Scaled EDA'].apply(lambda x: torch.tensor(x).view(1,-1)))), ## 일부 세션 데이터만 사용하므로, 길이를 맞춰주기 위해 일부 slicing함\n",
    "                      Temp = torch.concat(list(test_df['Scaled TEMP'].apply(lambda x: torch.tensor(x).view(1,-1)))), ## 일부 세션 데이터만 사용하므로, 길이를 맞춰주기 위해 일부 slicing함\n",
    "                      Emotion_ext = torch.concat(list(test_df['emotion_vector'].apply(lambda x: torch.tensor(x).view(1,-1))))) ## 일부 세션 데이터만 사용하므로, 길이를 맞춰주기 위해 일부 slicing함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12842 2976 2434\n",
      "Training Data Size : 12842\n",
      "Validation Data Size : 2434\n",
      "Testing Data Size : 2976\n"
     ]
    }
   ],
   "source": [
    "train_size = len(train_dataset)\n",
    "validation_size = len(val_dataset)\n",
    "test_size = len(test_dataset)\n",
    "\n",
    "print(train_size, test_size, validation_size)\n",
    "print(f\"Training Data Size : {len(train_dataset)}\")\n",
    "print(f\"Validation Data Size : {len(val_dataset)}\")\n",
    "print(f\"Testing Data Size : {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "validation_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NetWork 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)\n",
    "# 위의 오류가 해결되기 전까진 일단 cpu를 가지고 모델을 돌리기로 한다. \n",
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNetwork_pre(nn.Module):\n",
    "    def __init__(self, input_length, input_width):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_length*input_width, 768)\n",
    "        self.gelu1 = nn.GELU()\n",
    "        self.bn1 = nn.BatchNorm1d(768)\n",
    "        self.fc2 = nn.Linear(768, 512)\n",
    "        self.gelu2 = nn.GELU()\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.fc3 = nn.Linear(512, 32)\n",
    "        self.gelu3 = nn.GELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.gelu1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.gelu2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.fc3(x)\n",
    "        output = self.gelu3(x)\n",
    "        return output\n",
    "    \n",
    "class ConvNetwork_pre(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels = 1, out_channels= 32, kernel_size = 16)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(in_channels = 32, out_channels = 16, kernel_size = 16)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.transpose(x, 1,2)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        output = self.relu2(x)\n",
    "        return output    \n",
    "    \n",
    "class ConvNetwork_middle(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=32, out_channels=1, kernel_size=11)\n",
    "        self.relu1 = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        output = self.relu1(x)    \n",
    "        return output.squeeze()\n",
    "    \n",
    "class ConvNetwork_final(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "        self.conv2d_1 = nn.Conv2d(in_channels = 10, out_channels = 64, kernel_size=2)\n",
    "        self.leakyrelu_1 = nn.LeakyReLU()\n",
    "        self.maxpool2d_1 = nn.MaxPool2d(2)\n",
    "        self.conv2d_2 = nn.Conv2d(in_channels = 64, out_channels = 32, kernel_size=2)\n",
    "        self.leakyrelu_2 = nn.LeakyReLU()\n",
    "        self.maxpool2d_2 = nn.MaxPool2d(2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(1568, 64)\n",
    "        self.leakyrelu_3 = nn.LeakyReLU()\n",
    "        self.batchnorm = nn.BatchNorm1d(64)\n",
    "        self.drop = nn.Dropout(p=0.25)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv2d_1(x)\n",
    "        x = self.leakyrelu_1(x)\n",
    "        x = self.maxpool2d_1(x)\n",
    "        x = self.conv2d_2(x)\n",
    "        x = self.leakyrelu_2(x)\n",
    "        x = self.maxpool2d_2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.leakyrelu_3(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.drop(x)\n",
    "        output = self.fc2(x)  \n",
    "        return output\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorFusionMixer(nn.Module):\n",
    "    def __init__(self, ModelA, ModelB, ModelC, ModelD, ModelE, ModelF):\n",
    "        super().__init__()\n",
    "        self.ModelA = ModelA\n",
    "        self.ModelB = ModelB\n",
    "        self.ModelC = ModelC\n",
    "        self.ModelD = ModelD\n",
    "        self.ModelE = ModelE\n",
    "        self.Model_cnn_final = ModelF\n",
    "        # self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def tensor_fusion(self, batch_arr1, batch_arr2, batch_arr3):\n",
    "        fusion_matrix_lst = []\n",
    "        for i, (arr1, arr2, arr3) in enumerate(zip(batch_arr1, batch_arr2, batch_arr3)):\n",
    "            # arr1 = arr1.unsqueeze(-1).unsqueeze(-1)\n",
    "            # arr2 = arr2.unsqueeze(0).unsqueeze(-1)\n",
    "            # arr3 = arr3.squeeze().unsqueeze(0).unsqueeze(0)\n",
    "            \n",
    "            arr1 = arr1.unsqueeze(0).unsqueeze(0)\n",
    "            arr2 = arr2.unsqueeze(0).unsqueeze(-1)\n",
    "            arr3 = arr3.unsqueeze(-1).unsqueeze(-1)\n",
    "            \n",
    "            # outer_matrix = torch.einsum('i,j,kp->ijk', arr1, arr2, arr3)\n",
    "            kron_matrix = torch.kron(arr3, torch.kron(arr2, arr1,))\n",
    "            l, w, d = kron_matrix.shape\n",
    "            \n",
    "            kron_matrix = kron_matrix.view(-1, l, w, d)\n",
    "            fusion_matrix_lst.append(kron_matrix)\n",
    "            \n",
    "        fusion_matrix = torch.concat(fusion_matrix_lst)\n",
    "        # fusion_matrix = fusion_matrix.unsqueeze(-1)\n",
    "        \n",
    "        return fusion_matrix\n",
    "        \n",
    "    def forward(self, x1, x2, x3, x4):\n",
    "        x1 = self.ModelA(x1)\n",
    "        x2 = self.ModelB(x2)\n",
    "        x3 = self.ModelC(x3)\n",
    "        x4 = self.ModelD(x4)\n",
    "        x5 = torch.concat([x3, x4], dim=1)\n",
    "        x5 = self.ModelE(x5)\n",
    "        fusion_matrix = self.tensor_fusion(x1, x2, x5)\n",
    "        # x5 = torch.cat([x3,x4], dim=0)\n",
    "        output = self.Model_cnn_final(fusion_matrix) \n",
    "        return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 4 GPUs!\n",
      "DataParallel(\n",
      "  (module): TensorFusionMixer(\n",
      "    (ModelA): MLPNetwork_pre(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=61440, out_features=768, bias=True)\n",
      "      (gelu1): GELU(approximate='none')\n",
      "      (bn1): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc2): Linear(in_features=768, out_features=512, bias=True)\n",
      "      (gelu2): GELU(approximate='none')\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc3): Linear(in_features=512, out_features=32, bias=True)\n",
      "      (gelu3): GELU(approximate='none')\n",
      "    )\n",
      "    (ModelB): MLPNetwork_pre(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=152576, out_features=768, bias=True)\n",
      "      (gelu1): GELU(approximate='none')\n",
      "      (bn1): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc2): Linear(in_features=768, out_features=512, bias=True)\n",
      "      (gelu2): GELU(approximate='none')\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc3): Linear(in_features=512, out_features=32, bias=True)\n",
      "      (gelu3): GELU(approximate='none')\n",
      "    )\n",
      "    (ModelC): ConvNetwork_pre(\n",
      "      (conv1): Conv1d(1, 32, kernel_size=(16,), stride=(1,))\n",
      "      (relu1): ReLU()\n",
      "      (conv2): Conv1d(32, 16, kernel_size=(16,), stride=(1,))\n",
      "      (relu2): ReLU()\n",
      "    )\n",
      "    (ModelD): ConvNetwork_pre(\n",
      "      (conv1): Conv1d(1, 32, kernel_size=(16,), stride=(1,))\n",
      "      (relu1): ReLU()\n",
      "      (conv2): Conv1d(32, 16, kernel_size=(16,), stride=(1,))\n",
      "      (relu2): ReLU()\n",
      "    )\n",
      "    (ModelE): ConvNetwork_middle(\n",
      "      (conv1): Conv1d(32, 1, kernel_size=(11,), stride=(1,))\n",
      "      (relu1): ReLU()\n",
      "    )\n",
      "    (Model_cnn_final): ConvNetwork_final(\n",
      "      (conv2d_1): Conv2d(10, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "      (leakyrelu_1): LeakyReLU(negative_slope=0.01)\n",
      "      (maxpool2d_1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (conv2d_2): Conv2d(64, 32, kernel_size=(2, 2), stride=(1, 1))\n",
      "      (leakyrelu_2): LeakyReLU(negative_slope=0.01)\n",
      "      (maxpool2d_2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=1568, out_features=64, bias=True)\n",
      "      (leakyrelu_3): LeakyReLU(negative_slope=0.01)\n",
      "      (batchnorm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop): Dropout(p=0.25, inplace=False)\n",
      "      (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# txt_input_length, txt_input_width = raw_dataset[session]['text_embeddings'][0].shape | 마지막엔 지울 것\n",
    "# _, wav_input_length, wav_input_width = raw_dataset[session]['wav_embeddings'][0].shape\n",
    "txt_input_length, txt_input_width = train_dataset.text_embeddings[0].shape\n",
    "wav_input_length, wav_input_width = train_dataset.wav_embeddings[0].shape\n",
    "temp_input_length = train_dataset.temp[0].shape[0]\n",
    "eda_input_length = train_dataset.eda[0].shape[0]\n",
    "\n",
    "# tf_mixer에 들어갈 wav mlp, txt mlp 선언\n",
    "model_mlp_txt = MLPNetwork_pre(txt_input_length,txt_input_width).to(device)\n",
    "model_mlp_wav = MLPNetwork_pre(wav_input_length,wav_input_width).to(device)\n",
    "# model_conv_temp = ConvNetwork_pre(temp_input_length).to(device)\n",
    "model_conv_temp = ConvNetwork_pre().to(device)\n",
    "model_conv_eda = ConvNetwork_pre().to(device)\n",
    "# model_conv_eda = ConvNetwork_pre(eda_input_length).to(device)\n",
    "model_conv_middle = ConvNetwork_middle().to(device)\n",
    "# model_cnn_final = ConvNetwork_final(32).to(device)\n",
    "model_cnn_final = ConvNetwork_final().to(device)\n",
    "# 최종 모델 선언\n",
    "model_tf_cnn_mixer = TensorFusionMixer(ModelA = model_mlp_txt, \n",
    "                                   ModelB = model_mlp_wav,\n",
    "                                   ModelC = model_conv_temp,\n",
    "                                   ModelD = model_conv_eda,\n",
    "                                   ModelE = model_conv_middle,\n",
    "                                   ModelF = model_cnn_final\n",
    "                                   ).to(device)\n",
    "\n",
    "# model 병렬 학습 처리\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model_mlp_txt = nn.DataParallel(model_mlp_txt).to(device)\n",
    "    model_mlp_wav = nn.DataParallel(model_mlp_wav).to(device)\n",
    "    model_conv_temp = nn.DataParallel(model_conv_temp).to(device)\n",
    "    model_conv_eda = nn.DataParallel(model_conv_eda).to(device)\n",
    "    model_tf_cnn_mixer = nn.DataParallel(model_tf_cnn_mixer).to(device)\n",
    "    \n",
    "print(model_tf_cnn_mixer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CCC, self).__init__()\n",
    "        self.mean = torch.mean\n",
    "        self.var = torch.var\n",
    "        self.sum = torch.sum\n",
    "        self.sqrt = torch.sqrt\n",
    "        self.std = torch.std\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        mean_gt = self.mean (target, 0)\n",
    "        mean_pred = self.mean (pred, 0)\n",
    "        var_gt = self.var (target, 0)\n",
    "        var_pred = self.var (pred, 0)\n",
    "        v_pred = pred - mean_pred\n",
    "        v_gt = target - mean_gt\n",
    "        cor = self.sum (v_pred * v_gt) / (self.sqrt(self.sum(v_pred ** 2)) * self.sqrt(self.sum(v_gt ** 2)))\n",
    "        sd_gt = self.std(target)\n",
    "        sd_pred = self.std(pred)\n",
    "        numerator = 2 * cor * sd_gt * sd_pred\n",
    "        denominator = var_gt + var_pred + (mean_gt-mean_pred) ** 2\n",
    "        ccc = numerator / denominator\n",
    "        return ccc\n",
    "    \n",
    "ccc = CCC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCCLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self,inputs,targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        # pred_v = inputs[:,0]\n",
    "        # pred_a = inputs[:,1]\n",
    "        # y_v  = targets[:,0]\n",
    "        # y_a  = targets[:,1]\n",
    "\n",
    "        return 1 - ccc(self.inputs, self.targets)\n",
    "    \n",
    "cccl = CCCLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습을 위한 train, test method 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    # data 순서: text_embeddings, wav_embeddings, temp, eda, label_emotion, label_emotion_ext, label_arousal, label_valence\n",
    "    return_loss = 0\n",
    "    for batch, (X_txt, X_wav, X_temp, X_eda, \n",
    "                    label_emotion, label_emotion_ext, label_arousal, label_valence) in enumerate(dataloader): \n",
    "\n",
    "        # 예측 오류 계산 \n",
    "        X_txt, X_wav, X_temp, X_eda, y_v= X_txt.to(device), X_wav.to(device), X_temp.to(device), X_eda.to(device),label_valence.type(torch.float32).to(device)\n",
    "        \n",
    "        X_temp = X_temp.unsqueeze(dim=-1)\n",
    "        X_eda = X_eda.unsqueeze(dim=-1)\n",
    "        y_v = y_v.unsqueeze(dim=-1)\n",
    "        \n",
    "        pred = model(X_txt, X_wav, X_temp, X_eda)\n",
    "\n",
    "        loss = loss_fn(pred, y_v)\n",
    "\n",
    "        # 역전파\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            ccc_value = ccc(pred, y_v).item()\n",
    "            loss= loss.item()\n",
    "            print(ccc_value)\n",
    "            print(f\"loss: {loss}, ccc : {ccc_value}\")\n",
    "        \n",
    "            return_loss += loss\n",
    "    \n",
    "    return return_loss,ccc_value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn, mode = 'test'):\n",
    "    \n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    ccc_value = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # data 순서: text_embeddings, wav_embeddings, temp, eda, label_emotion, label_emotion_ext, label_arousal, label_valence\n",
    "        for batch, (X_txt, X_wav, X_temp, X_eda, \n",
    "                    label_emotion, label_emotion_ext, label_arousal, label_valence) in enumerate(dataloader): \n",
    "\n",
    "            # 예측 오류 계산 \n",
    "            X_txt, X_wav, X_temp, X_eda, y_v= X_txt.to(device), X_wav.to(device), X_temp.to(device), X_eda.to(device),label_valence.type(torch.float32).to(device)\n",
    "            \n",
    "            X_temp = X_temp.unsqueeze(dim=-1)\n",
    "            X_eda = X_eda.unsqueeze(dim=-1)\n",
    "            y_v = y_v.unsqueeze(dim=-1)\n",
    "\n",
    "            pred = model(X_txt, X_wav, X_temp, X_eda)\n",
    "            \n",
    "            loss = loss_fn(pred, y_v)\n",
    "            test_loss += loss\n",
    "            ccc_value += ccc(pred, y_v).item()\n",
    "      \n",
    "    test_loss /= num_batches \n",
    "    ccc_value /= num_batches \n",
    "    \n",
    "    if mode == 'test':\n",
    "        print(f\"loss: {test_loss}, ccc : {ccc_value}\")\n",
    "    elif mode == 'val':\n",
    "        print(f\"loss: {test_loss}, ccc : {ccc_value}\")\n",
    "    return test_loss, ccc_value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 지난 저장한 모델이 있다면\n",
    "# PATH = './data/test_model.pkl'\n",
    "# model_tf_mixer = torch.load(PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start training mlp fusion mixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = CCCLoss() \n",
    "optimizer = optim.Adam(model_tf_cnn_mixer.parameters(), lr=LR) # regression\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:po3xw76d) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">opt : Adam, loss : CCCLoss, LR : 0.01</strong> at: <a href='https://wandb.ai/toez/ETRI_kyungho_regression_only_valence/runs/po3xw76d' target=\"_blank\">https://wandb.ai/toez/ETRI_kyungho_regression_only_valence/runs/po3xw76d</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230412_223604-po3xw76d/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:po3xw76d). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/arplab/project/paradeigma/multi_modal/experimental/wandb/run-20230412_223747-mh5u2pnm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/toez/ETRI_kyungho_regression_only_valence/runs/mh5u2pnm' target=\"_blank\">opt : Adam, loss : CCCLoss, LR : 0.01</a></strong> to <a href='https://wandb.ai/toez/ETRI_kyungho_regression_only_valence' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/toez/ETRI_kyungho_regression_only_valence' target=\"_blank\">https://wandb.ai/toez/ETRI_kyungho_regression_only_valence</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/toez/ETRI_kyungho_regression_only_valence/runs/mh5u2pnm' target=\"_blank\">https://wandb.ai/toez/ETRI_kyungho_regression_only_valence/runs/mh5u2pnm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/toez/ETRI_kyungho_regression_only_valence/runs/mh5u2pnm?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fe77c08a2f0>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"ETRI_kyungho_regression_only_valence\",\n",
    "    name = f\"opt : {optimizer.__class__.__name__}, loss : {criterion.__class__.__name__}, LR : {LR}\",\n",
    "    config={\n",
    "    \"dataset\": 'KEMDy19, KEMDy20',\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"learning_rate\": LR,\n",
    "    \"architecture\": \"TensorFusion\",\n",
    "    \"optimizer\": optimizer.__class__.__name__,\n",
    "    \"criterion\": criterion.__class__.__name__,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Epoch 1----------------\n",
      "Training...............\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/arplab/project/paradeigma/multi_modal/.venv/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 64, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/arplab/project/paradeigma/multi_modal/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/tmp/ipykernel_219623/4090816372.py\", line 44, in forward\n    output = self.Model_cnn_final(fusion_matrix)\n  File \"/home/arplab/project/paradeigma/multi_modal/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/tmp/ipykernel_219623/132462411.py\", line 81, in forward\n    output = self.fc2(x)\n  File \"/home/arplab/project/paradeigma/multi_modal/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/arplab/project/paradeigma/multi_modal/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n    return F.linear(input, self.weight, self.bias)\nRuntimeError: CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m---------------Epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m----------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTraining...............\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m loss_train,ccc_value_train \u001b[39m=\u001b[39m train(train_dataloader,\n\u001b[1;32m      6\u001b[0m              model_tf_cnn_mixer, \n\u001b[1;32m      7\u001b[0m              criterion, \n\u001b[1;32m      8\u001b[0m              optimizer)\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mValidation.............\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m loss_val,ccc_value_val \u001b[39m=\u001b[39m test(validation_dataloader, \n\u001b[1;32m     11\u001b[0m                 model_tf_cnn_mixer, \n\u001b[1;32m     12\u001b[0m                 criterion, \n\u001b[1;32m     13\u001b[0m                 mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[74], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     11\u001b[0m X_eda \u001b[39m=\u001b[39m X_eda\u001b[39m.\u001b[39munsqueeze(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     12\u001b[0m y_v \u001b[39m=\u001b[39m y_v\u001b[39m.\u001b[39munsqueeze(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m pred \u001b[39m=\u001b[39m model(X_txt, X_wav, X_temp, X_eda)\n\u001b[1;32m     16\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(pred, y_v)\n\u001b[1;32m     18\u001b[0m \u001b[39m# 역전파\u001b[39;00m\n",
      "File \u001b[0;32m~/project/paradeigma/multi_modal/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/project/paradeigma/multi_modal/.venv/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:171\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule(\u001b[39m*\u001b[39minputs[\u001b[39m0\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs[\u001b[39m0\u001b[39m])\n\u001b[1;32m    170\u001b[0m replicas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplicate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids[:\u001b[39mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 171\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparallel_apply(replicas, inputs, kwargs)\n\u001b[1;32m    172\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgather(outputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/project/paradeigma/multi_modal/.venv/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:181\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparallel_apply\u001b[39m(\u001b[39mself\u001b[39m, replicas, inputs, kwargs):\n\u001b[0;32m--> 181\u001b[0m     \u001b[39mreturn\u001b[39;00m parallel_apply(replicas, inputs, kwargs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice_ids[:\u001b[39mlen\u001b[39;49m(replicas)])\n",
      "File \u001b[0;32m~/project/paradeigma/multi_modal/.venv/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:89\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     87\u001b[0m     output \u001b[39m=\u001b[39m results[i]\n\u001b[1;32m     88\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m---> 89\u001b[0m         output\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m     90\u001b[0m     outputs\u001b[39m.\u001b[39mappend(output)\n\u001b[1;32m     91\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/project/paradeigma/multi_modal/.venv/lib/python3.10/site-packages/torch/_utils.py:543\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    540\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    541\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    542\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 543\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/arplab/project/paradeigma/multi_modal/.venv/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 64, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/arplab/project/paradeigma/multi_modal/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/tmp/ipykernel_219623/4090816372.py\", line 44, in forward\n    output = self.Model_cnn_final(fusion_matrix)\n  File \"/home/arplab/project/paradeigma/multi_modal/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/tmp/ipykernel_219623/132462411.py\", line 81, in forward\n    output = self.fc2(x)\n  File \"/home/arplab/project/paradeigma/multi_modal/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/arplab/project/paradeigma/multi_modal/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n    return F.linear(input, self.weight, self.bias)\nRuntimeError: CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print(f\"---------------Epoch {epoch+1}----------------\")\n",
    "    print(\"Training...............\")\n",
    "    \n",
    "    loss_train,ccc_value_train = train(train_dataloader,\n",
    "                 model_tf_cnn_mixer, \n",
    "                 criterion, \n",
    "                 optimizer)\n",
    "    print(\"Validation.............\")\n",
    "    loss_val,ccc_value_val = test(validation_dataloader, \n",
    "                    model_tf_cnn_mixer, \n",
    "                    criterion, \n",
    "                    mode='val')\n",
    "\n",
    "    wandb.log({\"train_loss\": loss_train, \"val_loss\": loss_val, 'train_ccc' : ccc_value_train, 'val_ccc' : ccc_value_val})\n",
    "    scheduler.step(loss_val)\n",
    "print(\"Done!\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
