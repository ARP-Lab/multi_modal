{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# working model for tensorfusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arplab/project/lou/multi_modal/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu117\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from torchmetrics import F1Score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from datasets import Dataset\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "import random\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wav, text데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../../paradeigma/multi_modal/model/data/paradeigma_KEMDY19_annotation_nonmissing.pkl',\n",
       " '../../../paradeigma/multi_modal/model/data/paradeigma_KEMDY19_embedding_for_dataset.pkl',\n",
       " '../../../paradeigma/multi_modal/model/data/paradeigma_KEMDY20_annotation_nonmissing.pkl',\n",
       " '../../../paradeigma/multi_modal/model/data/paradeigma_KEMDY20_embedding_for_dataset.pkl']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wav and text data load\n",
    "dataset_file_lst = ['../../../paradeigma/multi_modal/model/data/paradeigma_KEMDY19_annotation_nonmissing.pkl',\n",
    "                    '../../../paradeigma/multi_modal/model/data/paradeigma_KEMDY19_embedding_for_dataset.pkl',\n",
    "                    '../../../paradeigma/multi_modal/model/data/paradeigma_KEMDY20_annotation_nonmissing.pkl',\n",
    "                    '../../../paradeigma/multi_modal/model/data/paradeigma_KEMDY20_embedding_for_dataset.pkl']\n",
    "dataset_file_lst = sorted(dataset_file_lst)\n",
    "dataset_file_lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../paradeigma/multi_modal/model/data/paradeigma_KEMDY19_annotation_nonmissing.pkl kemdy19_annot\n",
      "../../../paradeigma/multi_modal/model/data/paradeigma_KEMDY19_embedding_for_dataset.pkl kemdy19_emb\n",
      "../../../paradeigma/multi_modal/model/data/paradeigma_KEMDY20_annotation_nonmissing.pkl kemdy20_annot\n",
      "../../../paradeigma/multi_modal/model/data/paradeigma_KEMDY20_embedding_for_dataset.pkl kemdy20_emb\n"
     ]
    }
   ],
   "source": [
    "\n",
    "var_names = ['kemdy19_annot', 'kemdy19_emb', 'kemdy20_annot', 'kemdy20_emb']\n",
    "for file, var_name in zip(dataset_file_lst, var_names):\n",
    "    print(file, var_name)\n",
    "    with open(file, 'rb') as f:\n",
    "        globals()[var_name] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  paradeigma가 섞어놓은 label들..되돌리기\n",
    "# {'neutral': 0, 'happy': 0, 'surprise':0, 'disgust': 0, 'angry': 0, 'sad':0, 'fear': 0}\n",
    "# {'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprise': 6}\n",
    "\n",
    "def change_order(lst, new_order = [4, 3, 6, 1, 0, 5, 2]):\n",
    "    return [lst[i] for i in new_order]\n",
    "\n",
    "kemdy20_annot['emotion_vector'] = kemdy20_annot['emotion_vector'].apply(change_order)\n",
    "kemdy19_annot['emotion_vector'] = kemdy19_annot['emotion_vector'].apply(change_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Segment ID</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Valence</th>\n",
       "      <th>Arousal</th>\n",
       "      <th>emotion_vector</th>\n",
       "      <th>valence_vector</th>\n",
       "      <th>arousal_vector</th>\n",
       "      <th>EDA</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>EDA length</th>\n",
       "      <th>TEMP length</th>\n",
       "      <th>Scaled EDA</th>\n",
       "      <th>Scaled TEMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sess01_script01_M001</td>\n",
       "      <td>surprise</td>\n",
       "      <td>1.7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 9]</td>\n",
       "      <td>[4, 5, 1, 0, 0]</td>\n",
       "      <td>[1, 0, 0, 6, 3]</td>\n",
       "      <td>[4.408951, 4.403825, 4.410233, 4.421767, 4.429...</td>\n",
       "      <td>[34.66, 34.66, 34.66, 34.66, 34.66, 34.68, 34....</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>[-0.926455937246618, -0.9285481420682529, -0.9...</td>\n",
       "      <td>[1.6900874192514124, 1.6900874192514124, 1.690...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sess01_script01_F001</td>\n",
       "      <td>fear</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.6</td>\n",
       "      <td>[0, 0, 7, 0, 1, 0, 2]</td>\n",
       "      <td>[5, 5, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 2, 6, 1]</td>\n",
       "      <td>[0.145914, 0.140794, 0.144634, 0.145914, 0.140...</td>\n",
       "      <td>[30.69, 30.69, 30.69, 30.69, 30.71, 30.71, 30....</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>[-1.407155706494133, -1.4312942323175069, -1.4...</td>\n",
       "      <td>[-1.1135528407658513, -1.1135528407658513, -1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sess01_script01_M002</td>\n",
       "      <td>angry</td>\n",
       "      <td>1.3</td>\n",
       "      <td>4.3</td>\n",
       "      <td>[10, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[7, 3, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 0, 3, 6]</td>\n",
       "      <td>[4.478828, 4.396809, 4.334012, 4.322478, 4.346...</td>\n",
       "      <td>[34.61, 34.61, 34.61, 34.61, 34.61, 34.61, 34....</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>[-0.8979352590723317, -0.9314117606848334, -0....</td>\n",
       "      <td>[1.6164861115016742, 1.6164861115016742, 1.616...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Segment ID   Emotion  Valence  Arousal          emotion_vector  \\\n",
       "0  Sess01_script01_M001  surprise      1.7      4.0   [0, 0, 1, 0, 0, 0, 9]   \n",
       "1  Sess01_script01_F001      fear      1.5      3.6   [0, 0, 7, 0, 1, 0, 2]   \n",
       "2  Sess01_script01_M002     angry      1.3      4.3  [10, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "    valence_vector   arousal_vector  \\\n",
       "0  [4, 5, 1, 0, 0]  [1, 0, 0, 6, 3]   \n",
       "1  [5, 5, 0, 0, 0]  [1, 0, 2, 6, 1]   \n",
       "2  [7, 3, 0, 0, 0]  [1, 0, 0, 3, 6]   \n",
       "\n",
       "                                                 EDA  \\\n",
       "0  [4.408951, 4.403825, 4.410233, 4.421767, 4.429...   \n",
       "1  [0.145914, 0.140794, 0.144634, 0.145914, 0.140...   \n",
       "2  [4.478828, 4.396809, 4.334012, 4.322478, 4.346...   \n",
       "\n",
       "                                                TEMP  EDA length  TEMP length  \\\n",
       "0  [34.66, 34.66, 34.66, 34.66, 34.66, 34.68, 34....          33           33   \n",
       "1  [30.69, 30.69, 30.69, 30.69, 30.71, 30.71, 30....          17           17   \n",
       "2  [34.61, 34.61, 34.61, 34.61, 34.61, 34.61, 34....          27           27   \n",
       "\n",
       "                                          Scaled EDA  \\\n",
       "0  [-0.926455937246618, -0.9285481420682529, -0.9...   \n",
       "1  [-1.407155706494133, -1.4312942323175069, -1.4...   \n",
       "2  [-0.8979352590723317, -0.9314117606848334, -0....   \n",
       "\n",
       "                                         Scaled TEMP  \n",
       "0  [1.6900874192514124, 1.6900874192514124, 1.690...  \n",
       "1  [-1.1135528407658513, -1.1135528407658513, -1....  \n",
       "2  [1.6164861115016742, 1.6164861115016742, 1.616...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kemdy19_annot.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<2020년>\n",
    "'angry;disqust',  'angry;disqust;fear;neutral;sad',  'angry;disqust;neutral', 'angry;happy;neutral', 'angry;neutral', 'disqust;happy;neutral', 'disqust;neutral', 'disqust;neutral;sad', 'fear;happy', 'fear;happy;neutral', 'fear;neutral', 'happy;neutral', 'happy;neutral;surprise', 'happy;sad', 'happy;surprise', 'neutral;sad', 'neutral;surprise'\n",
    "\n",
    "<2019년>\n",
    "'angry;disgust;fear;neutral;surprise', 'angry;fear', 'angry;fear;neutral', 'angry;fear;surprise', 'angry;happy', 'angry;neutral;surprise', 'angry;sad', 'angry;surprise', 'disgust;fear', 'disgust;happy','disgust;neutral;surprise', 'disgust;sad', 'disgust;surprise', 'fear;neutral;surprise', 'fear;sad', 'fear;surprise', 'happy;neutral;sad', 'neutral;sad;surprise', 'sad;surprise'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_dict = {'angry':0, 'disgust':1, 'fear':2,'happy':3,'neutral':4, 'sad':5, 'surprise':6,  \n",
    "               'neutral;surprise': 450, 'neutral;sad': 460, 'happy;neutral': 340, \n",
    "               'angry;neutral': 40, 'disgust;neutral': 140, 'fear;neutral': 240, \n",
    "               'happy;surprise': 350, 'angry;happy;neutral': 7340, 'angry;disgust': 10, \n",
    "               'happy;neutral;surprise': 3450, 'fear;happy': 230,'fear;happy;neutral': 2340,\n",
    "               'angry;disgust;neutral': 7140, 'disgust;neutral;sad': 1460, \n",
    "               'happy;sad': 360, 'disgust;happy;neutral': 3410, 'angry;fear': 20, 'angry;fear;neutral':7240,\n",
    "               'angry;fear;surprise': 7250, 'angry;happy': 730, 'angry;neutral;surprise':7450, \n",
    "               'angry;sad': 60, 'angry;surprise': 50, 'disgust;fear':120, 'disgust;happy': 130,\n",
    "               'disgust;neutral;surprise':1450, 'disgust;sad': 160, 'disgust;surprise':150, \n",
    "               'fear;neutral;surprise':2450, 'fear;sad': 260, 'fear;surprise':250, 'happy;neutral;sad':3460,\n",
    "               'neutral;sad;surprise':4560, 'sad;surprise': 560,\n",
    "               'angry;disgust;fear;neutral;sad': 10000,'angry;disgust;fear;neutral;surprise':20000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprise': 6, 'angry;disgust': 10, 'angry;fear': 20, 'angry;neutral': 40, 'angry;surprise': 50, 'angry;sad': 60, 'disgust;fear': 120, 'disgust;happy': 130, 'disgust;neutral': 140, 'disgust;surprise': 150, 'disgust;sad': 160, 'fear;happy': 230, 'fear;neutral': 240, 'fear;surprise': 250, 'fear;sad': 260, 'happy;neutral': 340, 'happy;surprise': 350, 'happy;sad': 360, 'neutral;surprise': 450, 'neutral;sad': 460, 'sad;surprise': 560, 'angry;happy': 730, 'disgust;neutral;surprise': 1450, 'disgust;neutral;sad': 1460, 'fear;happy;neutral': 2340, 'fear;neutral;surprise': 2450, 'disgust;happy;neutral': 3410, 'happy;neutral;surprise': 3450, 'happy;neutral;sad': 3460, 'neutral;sad;surprise': 4560, 'angry;disgust;neutral': 7140, 'angry;fear;neutral': 7240, 'angry;fear;surprise': 7250, 'angry;happy;neutral': 7340, 'angry;neutral;surprise': 7450, 'angry;disgust;fear;neutral;sad': 10000, 'angry;disgust;fear;neutral;surprise': 20000}\n"
     ]
    }
   ],
   "source": [
    "encode_dict = {k: v for k, v in sorted(encode_dict.items(), key=lambda item: item[1])}\n",
    "print(encode_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprise': 6, 'angry;disgust': 10, 'angry;fear': 20, 'angry;neutral': 40, 'angry;surprise': 50, 'angry;sad': 60, 'disgust;fear': 120, 'disgust;happy': 130, 'disgust;neutral': 140, 'disgust;surprise': 150, 'disgust;sad': 160, 'fear;happy': 230, 'fear;neutral': 240, 'fear;surprise': 250, 'fear;sad': 260, 'happy;neutral': 340, 'happy;surprise': 350, 'happy;sad': 360, 'neutral;surprise': 450, 'neutral;sad': 460, 'sad;surprise': 560, 'angry;happy': 730, 'disgust;neutral;surprise': 1450, 'disgust;neutral;sad': 1460, 'fear;happy;neutral': 2340, 'fear;neutral;surprise': 2450, 'disgust;happy;neutral': 3410, 'happy;neutral;surprise': 3450, 'happy;neutral;sad': 3460, 'neutral;sad;surprise': 4560, 'angry;disgust;neutral': 7140, 'angry;fear;neutral': 7240, 'angry;fear;surprise': 7250, 'angry;happy;neutral': 7340, 'angry;neutral;surprise': 7450, 'angry;disgust;fear;neutral;sad': 10000, 'angry;disgust;fear;neutral;surprise': 20000} \n",
      " {0: 'angry', 1: 'disgust', 2: 'fear', 3: 'happy', 4: 'neutral', 5: 'sad', 6: 'surprise', 10: 'angry;disgust', 20: 'angry;fear', 40: 'angry;neutral', 50: 'angry;surprise', 60: 'angry;sad', 120: 'disgust;fear', 130: 'disgust;happy', 140: 'disgust;neutral', 150: 'disgust;surprise', 160: 'disgust;sad', 230: 'fear;happy', 240: 'fear;neutral', 250: 'fear;surprise', 260: 'fear;sad', 340: 'happy;neutral', 350: 'happy;surprise', 360: 'happy;sad', 450: 'neutral;surprise', 460: 'neutral;sad', 560: 'sad;surprise', 730: 'angry;happy', 1450: 'disgust;neutral;surprise', 1460: 'disgust;neutral;sad', 2340: 'fear;happy;neutral', 2450: 'fear;neutral;surprise', 3410: 'disgust;happy;neutral', 3450: 'happy;neutral;surprise', 3460: 'happy;neutral;sad', 4560: 'neutral;sad;surprise', 7140: 'angry;disgust;neutral', 7240: 'angry;fear;neutral', 7250: 'angry;fear;surprise', 7340: 'angry;happy;neutral', 7450: 'angry;neutral;surprise', 10000: 'angry;disgust;fear;neutral;sad', 20000: 'angry;disgust;fear;neutral;surprise'}\n"
     ]
    }
   ],
   "source": [
    "# encoding Emotion for whole data\n",
    "# 사전에 실제로 encoding한 끝 수가 마지막 linear layer의 끝자리랑 맞아야 합니다. 아니면 CUDA error: CUBLAS_STATUS_EXECUTION_FAILED가 나는 것 같아요.\n",
    "# 예를 들어, label이 0~9, 11,13이렇게 12개가 되었어도, 0~13은 14개니까 마지막 레이어에서 14개 unit을 받아야 multiclass classification이 에러없이 진행됩니다!\n",
    "# 데이터에서 정답 라벨 인코딩: ['neutral', 'happy', 'surprise', 'disgust', 'angry', 'sad', 'fear']\n",
    "# 이 순서를 지켜서 라벨링을 해야함\n",
    "decode_dict = {b:i for i, b in encode_dict.items()}\n",
    "print(encode_dict, '\\n', decode_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    6\n",
       " 1    2\n",
       " 2    0\n",
       " Name: Emotion, dtype: int64,\n",
       " 0    4\n",
       " 1    4\n",
       " 2    4\n",
       " Name: Emotion, dtype: int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kemdy19_annot.Emotion = list(kemdy19_annot.Emotion.map(encode_dict))\n",
    "kemdy20_annot.Emotion = list(kemdy20_annot.Emotion.map(encode_dict))\n",
    "kemdy19_annot.Emotion[:3], kemdy20_annot.Emotion[:3]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA, TEMP Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def add_padding(pd_series, length = 50):\n",
    "    if isinstance(pd_series, float):\n",
    "        if math.isnan(pd_series):\n",
    "            return np.zeros(10)\n",
    "    if len(pd_series) < length:\n",
    "        pd_series = np.concatenate([pd_series, np.zeros(length - len(pd_series))])\n",
    "        return np.array(pd_series)\n",
    "    elif len(pd_series) == length:\n",
    "        return np.array(pd_series)\n",
    "    elif len(pd_series) > length:\n",
    "        pd_series = pd_series[:length]\n",
    "        return np.array(pd_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    9008.000000\n",
       "mean       22.596470\n",
       "std        19.693268\n",
       "min         1.000000\n",
       "25%         9.000000\n",
       "50%        17.000000\n",
       "75%        30.000000\n",
       "max       314.000000\n",
       "Name: EDA, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kemdy19_annot.EDA.map(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.33301774,  0.55311825, -0.16032486, -0.10719511,  0.09773054,\n",
       "         0.10531797,  0.12809209,  0.40890977,  0.23435159, -0.29694591,\n",
       "        -0.36525051, -0.25140359, -0.46391666, -0.14514408,  0.05978157,\n",
       "         0.74286905,  0.61383542, -3.12036271,  2.16975526,  1.91929321,\n",
       "        -0.29693999, -0.19827383, -0.94967184,  0.18880927,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ]),\n",
       " array([-0.21123698, -0.18926761, -0.18665215, -0.1861293 , -0.1861293 ,\n",
       "        -0.18874476, -0.20757542, -0.25517533, -0.29074444, -0.29179054,\n",
       "        -0.30434459, -0.24105233, -0.24628284, -0.48480603, -0.52822112,\n",
       "        -0.60860162, -0.77337071, -0.85676544, -0.78562721, -0.72181169,\n",
       "        -0.65747333, -0.89756547, -0.72181169, -0.67891945, -0.63655006,\n",
       "        -0.63602721, -0.62085771, -0.58842731, -0.57796588, -0.59156562,\n",
       "        -0.60725797,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ]),\n",
       " array([-0.96177679, -0.96177679, -0.96177679, -0.96177679, -0.96177679,\n",
       "        -0.96177679, -0.96177679, -0.96177679, -0.96177679, -0.96177679,\n",
       "        -0.96177679, -0.96177679, -0.96177679, -0.96177679, -0.96177679,\n",
       "        -1.15004747, -1.15004747, -1.15004747, -1.15004747, -1.15004747,\n",
       "        -1.15004747, -1.15004747, -1.15004747, -0.96177679, -0.96177679,\n",
       "        -0.96177679, -0.96177679, -1.15004747, -1.15004747, -1.15004747,\n",
       "        -1.15004747, -1.15004747, -1.15004747, -1.15004747, -1.15004747,\n",
       "        -1.15004747, -1.15004747, -1.15004747, -1.15004747, -1.15004747,\n",
       "        -1.15004747, -1.15004747, -1.15004747, -1.33831815, -1.33831815,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ]),\n",
       " array([0.7038299 , 0.7038299 , 0.67438937, 0.67438937, 0.67438937,\n",
       "        0.67438937, 0.67438937, 0.67438937, 0.67438937, 0.67438937,\n",
       "        0.64494885, 0.64494885, 0.64494885, 0.64494885, 0.7038299 ,\n",
       "        0.7038299 , 0.7038299 , 0.7038299 , 0.71855016, 0.71855016,\n",
       "        0.71855016, 0.71855016, 0.7038299 , 0.7038299 , 0.7038299 ,\n",
       "        0.7038299 , 0.67438937, 0.67438937, 0.67438937, 0.67438937,\n",
       "        0.67438937, 0.67438937, 0.67438937, 0.67438937, 0.64494885,\n",
       "        0.64494885, 0.64494885, 0.64494885, 0.67438937, 0.67438937,\n",
       "        0.67438937, 0.67438937, 0.7038299 , 0.7038299 , 0.7038299 ,\n",
       "        0.7038299 , 0.7038299 , 0.7038299 , 0.7038299 , 0.7038299 ]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kemdy19_annot['Scaled EDA'] = kemdy19_annot['Scaled EDA'].apply(add_padding)\n",
    "kemdy20_annot['Scaled EDA'] = kemdy20_annot['Scaled EDA'].apply(add_padding)\n",
    "kemdy19_annot['Scaled TEMP'] = kemdy19_annot['Scaled TEMP'].apply(add_padding)\n",
    "kemdy20_annot['Scaled TEMP'] = kemdy20_annot['Scaled TEMP'].apply(add_padding)\n",
    "# check\n",
    "kemdy20_annot['Scaled EDA'][10], kemdy19_annot['Scaled EDA'][10], kemdy20_annot['Scaled TEMP'][3],kemdy19_annot['Scaled TEMP'][15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session 19 length: wav - 9008, txt - 9008\n",
      "session 20 length: wav - 12715, txt - 12715\n"
     ]
    }
   ],
   "source": [
    "lengt_wav = 0\n",
    "lengt_txt = 0\n",
    "for i,j in zip(kemdy19_emb[0], kemdy19_emb[1]):\n",
    "    lengt_wav += len(kemdy19_emb[0][i])\n",
    "    lengt_txt += len(kemdy19_emb[1][j])\n",
    "    # lengt += len(i)\n",
    "print(f'session 19 length: wav - {lengt_wav}, txt - {lengt_txt}')\n",
    "\n",
    "lengt_wav = 0\n",
    "lengt_txt = 0\n",
    "for i,j in zip(kemdy20_emb[0], kemdy20_emb[1]):\n",
    "    lengt_wav += len(kemdy20_emb[0][i])\n",
    "    lengt_txt += len(kemdy20_emb[1][j])\n",
    "    \n",
    "print(f'session 20 length: wav - {lengt_wav}, txt - {lengt_txt}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session pick \n",
    "- test(.2), validation(.2), train(.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choice_and_remove_list(original_list, k = 8):\n",
    "    removed_new_list = []\n",
    "    chosen_list = random.sample(original_list, k = k)\n",
    "    for session in original_list:\n",
    "        if session in chosen_list:\n",
    "            pass\n",
    "        else:\n",
    "            removed_new_list.append(session) \n",
    "    return sorted(removed_new_list), sorted(chosen_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sess02', 'Sess03', 'Sess04', 'Sess06', 'Sess07', 'Sess09', 'Sess10', 'Sess11', 'Sess13', 'Sess18', 'Sess20', 'Sess23', 'Sess25', 'Sess26', 'Sess28', 'Sess29', 'Sess30', 'Sess33', 'Sess34', 'Sess35', 'Sess36', 'Sess38']\n",
      "['Sess08', 'Sess16', 'Sess19', 'Sess21', 'Sess22', 'Sess24', 'Sess37', 'Sess39']\n",
      "['Sess01', 'Sess05', 'Sess14', 'Sess15', 'Sess27', 'Sess31', 'Sess32', 'Sess40']\n",
      "\n",
      "\n",
      "['Sess01', 'Sess02', 'Sess03', 'Sess04', 'Sess05', 'Sess06', 'Sess07', 'Sess09', 'Sess10', 'Sess11', 'Sess13', 'Sess16']\n",
      "['Sess14', 'Sess15', 'Sess17', 'Sess19']\n",
      "['Sess08', 'Sess12', 'Sess18', 'Sess20']\n"
     ]
    }
   ],
   "source": [
    "# session을 train vs test&val로 나눠줌\n",
    "session_20_lst = ['Sess0' + str(i+1) if i < 9 else 'Sess' + str(i+1) for i in range(40)]\n",
    "session_20_lst.remove('Sess12')\n",
    "session_20_lst.remove('Sess17')\n",
    "sessions_20_train_lst, sessions_20_test_lst = choice_and_remove_list(session_20_lst, k = 8)\n",
    "sessions_20_train_lst, sessions_20_val_lst = choice_and_remove_list(sessions_20_train_lst, k = 8)\n",
    "print(sessions_20_train_lst, sessions_20_test_lst, sessions_20_val_lst, sep = '\\n')\n",
    "\n",
    "session_19_lst = ['Sess0' + str(i+1) if i < 9 else 'Sess' + str(i+1) for i in range(20)]\n",
    "sessions_19_train_lst, sessions_19_test_lst = choice_and_remove_list(session_19_lst, k = 4)\n",
    "sessions_19_train_lst, sessions_19_val_lst = choice_and_remove_list(sessions_19_train_lst, k = 4)\n",
    "print('\\n', sessions_19_train_lst,sessions_19_test_lst, sessions_19_val_lst, sep = '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data reconstruct\n",
    "- dict - session - txt, wav, segment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "kemdy20_emb_new = {}\n",
    "for session in kemdy20_emb[0].keys():\n",
    "    kemdy20_emb_new[session] = {}\n",
    "    kemdy20_emb_new[session]['wav'] = kemdy20_emb[0][session]\n",
    "    kemdy20_emb_new[session]['txt'] = kemdy20_emb[1][session]\n",
    "    kemdy20_emb_new[session]['segment_id'] = kemdy20_annot['Segment ID'][kemdy20_annot['Segment ID'].str.startswith(session)]\n",
    "    # print(session, len(kemdy20_emb_new[session]['wav']), len(kemdy20_emb_new[session]['txt']), len(kemdy20_emb_new[session]['segment_id']))\n",
    "\n",
    "# print('\\n')\n",
    "kemdy19_emb_new = {}\n",
    "for session in kemdy19_emb[0].keys():\n",
    "    kemdy19_emb_new[session] = {}\n",
    "    kemdy19_emb_new[session]['wav'] = kemdy19_emb[0][session]\n",
    "    kemdy19_emb_new[session]['txt'] = kemdy19_emb[1][session]\n",
    "    kemdy19_emb_new[session]['segment_id'] = kemdy19_annot['Segment ID'][kemdy19_annot['Segment ID'].str.startswith(session)]\n",
    "    # print(session, len(kemdy19_emb_new[session]['wav']), len(kemdy19_emb_new[session]['txt']), len(kemdy19_emb_new[session]['segment_id']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나눠준 세션을 emb(wav, text순), annot set에 적용\n",
    "def get_data_by_session(data, session_lst):\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        print('dataframe')\n",
    "        for idx, session in enumerate(session_lst):\n",
    "            if idx == 0:\n",
    "                dataframe = data[data['Segment ID'].str.startswith(session)]\n",
    "            else:\n",
    "                dataframe = pd.concat([dataframe, data[data['Segment ID'].str.startswith(session)]])\n",
    "        return dataframe\n",
    "    \n",
    "    elif isinstance(data, dict):\n",
    "        print('dict')\n",
    "        emb_data = {}\n",
    "        emb_data['wav'] = []\n",
    "        emb_data['txt'] = []\n",
    "        emb_data['segment_id'] = []\n",
    "        for session in session_lst:\n",
    "            emb_data['wav'].extend(data[session]['wav'])\n",
    "            emb_data['txt'].extend(data[session]['txt'])\n",
    "            emb_data['segment_id'].extend(data[session]['segment_id'])            \n",
    "        return emb_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe\n",
      "dataframe\n",
      "dataframe\n",
      "dataframe\n",
      "dataframe\n",
      "dataframe\n"
     ]
    }
   ],
   "source": [
    "kemdy19_annot_train = get_data_by_session(kemdy19_annot, sessions_19_train_lst)\n",
    "kemdy19_annot_test = get_data_by_session(kemdy19_annot, sessions_19_test_lst)\n",
    "kemdy19_annot_val = get_data_by_session(kemdy19_annot, sessions_19_val_lst)\n",
    "\n",
    "kemdy20_annot_train = get_data_by_session(kemdy20_annot, sessions_20_train_lst)\n",
    "kemdy20_annot_test = get_data_by_session(kemdy20_annot, sessions_20_test_lst)\n",
    "kemdy20_annot_val = get_data_by_session(kemdy20_annot, sessions_20_val_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict\n",
      "dict\n",
      "dict\n",
      "dict\n",
      "dict\n",
      "dict\n"
     ]
    }
   ],
   "source": [
    "# session 정보가 없어집니다. \n",
    "kemdy19_emb_train = get_data_by_session(kemdy19_emb_new, sessions_19_train_lst)\n",
    "kemdy19_emb_test = get_data_by_session(kemdy19_emb_new, sessions_19_test_lst)\n",
    "kemdy19_emb_val = get_data_by_session(kemdy19_emb_new, sessions_19_val_lst)\n",
    "\n",
    "kemdy20_emb_train = get_data_by_session(kemdy20_emb_new, sessions_20_train_lst)\n",
    "kemdy20_emb_test = get_data_by_session(kemdy20_emb_new, sessions_20_test_lst)\n",
    "kemdy20_emb_val = get_data_by_session(kemdy20_emb_new, sessions_20_val_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5138 5138 5138\n",
      "1897 1897 1897\n",
      "1973 1973 1973\n",
      "9008\n"
     ]
    }
   ],
   "source": [
    "# kemdy 19 - 9008이 정상\n",
    "print(len(kemdy19_emb_train['wav']), len(kemdy19_emb_train['txt']), len(kemdy19_emb_train['segment_id']))\n",
    "print(len(kemdy19_emb_test['wav']), len(kemdy19_emb_test['txt']), len(kemdy19_emb_test['segment_id']))\n",
    "print(len(kemdy19_emb_val['wav']), len(kemdy19_emb_val['txt']), len(kemdy19_emb_val['segment_id']))\n",
    "print(len(kemdy19_emb_train['wav']) + len(kemdy19_emb_test['wav']) + len(kemdy19_emb_val['wav']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7080 7080 7080\n",
      "2740 2740 2740\n",
      "2895 2895 2895\n",
      "12715\n"
     ]
    }
   ],
   "source": [
    "# kemdy 20 - 12715가 정상\n",
    "print(len(kemdy20_emb_train['wav']), len(kemdy20_emb_train['txt']), len(kemdy20_emb_train['segment_id']))\n",
    "print(len(kemdy20_emb_test['wav']), len(kemdy20_emb_test['txt']), len(kemdy20_emb_test['segment_id']))\n",
    "print(len(kemdy20_emb_val['wav']), len(kemdy20_emb_val['txt']), len(kemdy20_emb_val['segment_id']))\n",
    "print(len(kemdy20_emb_train['wav']) + len(kemdy20_emb_test['wav'])+ len(kemdy20_emb_val['wav']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['wav', 'txt', 'segment_id'])\n",
      "dict_keys(['wav', 'txt', 'segment_id'])\n",
      "dict_keys(['wav', 'txt', 'segment_id'])\n",
      "dict_keys(['wav', 'txt', 'segment_id'])\n",
      "dict_keys(['wav', 'txt', 'segment_id'])\n",
      "dict_keys(['wav', 'txt', 'segment_id'])\n"
     ]
    }
   ],
   "source": [
    "print(kemdy19_emb_train.keys(), kemdy19_emb_test.keys(), kemdy19_emb_val.keys(),\n",
    "      kemdy20_emb_train.keys(),kemdy20_emb_test.keys(), kemdy20_emb_val.keys(),sep = '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Data neutral pick하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1241, 3759)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 데이터 셋에서 몇개 뽑아야 되는지 계산 neutral: 4\n",
    "target_neutral_num = 5000\n",
    "\n",
    "target_neutral_num_19 = int(target_neutral_num / (Counter(kemdy19_annot['Emotion'])[4] + Counter(kemdy20_annot['Emotion'])[4]) * Counter(kemdy19_annot['Emotion'])[4])\n",
    "target_neutral_num_20 = target_neutral_num - target_neutral_num_19\n",
    "target_neutral_num_19, target_neutral_num_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3115, 1241, 1196, 3759)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 train dataset에서 뽑아야 되는 갯수만큼 랜덤으로 뽑아옴\n",
    "kemdy19_annot_train_not_neut = kemdy19_annot_train[kemdy19_annot_train['Emotion'] != 4]\n",
    "kemdy20_annot_train_not_neut = kemdy20_annot_train[kemdy20_annot_train['Emotion'] != 4]\n",
    "\n",
    "kemdy19_annot_train_neut = kemdy19_annot_train[kemdy19_annot_train['Emotion'] == 4].sample(target_neutral_num_19)\n",
    "kemdy20_annot_train_neut = kemdy20_annot_train[kemdy20_annot_train['Emotion'] == 4].sample(target_neutral_num_20)\n",
    "len(kemdy19_annot_train_not_neut), len(kemdy19_annot_train_neut), len(kemdy20_annot_train_not_neut), len(kemdy20_annot_train_neut)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## embedding, Test and validation dataset 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4637 4637\n",
      "4637 4637\n",
      "4637 4637\n",
      "4637 4637 4637\n"
     ]
    }
   ],
   "source": [
    "emb_test_final = {}\n",
    "emb_test_final['wav'] = []\n",
    "emb_test_final['txt'] = []\n",
    "emb_test_final['segment_id'] = []\n",
    "\n",
    "emb_test_final['wav'] = kemdy19_emb_test['wav']\n",
    "emb_test_final['wav'].extend(kemdy20_emb_test['wav'])\n",
    "print(len(kemdy19_emb_test['wav']), len(emb_test_final['wav']))\n",
    "\n",
    "emb_test_final['txt'] = kemdy19_emb_test['txt']\n",
    "emb_test_final['txt'].extend(kemdy20_emb_test['txt'])\n",
    "print(len(kemdy19_emb_test['txt']), len(emb_test_final['txt']))\n",
    "\n",
    "emb_test_final['segment_id'] = kemdy19_emb_test['segment_id']\n",
    "emb_test_final['segment_id'].extend(kemdy20_emb_test['segment_id'])\n",
    "print(len(kemdy19_emb_test['segment_id']), len(emb_test_final['segment_id']))\n",
    "\n",
    "print(len(emb_test_final['wav']), len(emb_test_final['txt']), len(emb_test_final['segment_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4868 4868 4868\n"
     ]
    }
   ],
   "source": [
    "emb_val_final = {}\n",
    "emb_val_final['wav'] = []\n",
    "emb_val_final['txt'] = []\n",
    "emb_val_final['segment_id'] = []\n",
    "\n",
    "emb_val_final['wav'] = kemdy19_emb_val['wav']\n",
    "emb_val_final['wav'].extend(kemdy20_emb_val['wav'])\n",
    "\n",
    "emb_val_final['txt'] = kemdy19_emb_val['txt']\n",
    "emb_val_final['txt'].extend(kemdy20_emb_val['txt'])\n",
    "\n",
    "emb_val_final['segment_id'] = kemdy19_emb_val['segment_id']\n",
    "emb_val_final['segment_id'].extend(kemdy20_emb_val['segment_id'])\n",
    "\n",
    "print(len(emb_val_final['wav']), len(emb_val_final['txt']), len(emb_val_final['segment_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9311 4637 4868\n"
     ]
    }
   ],
   "source": [
    "annot_train_final = pd.concat([kemdy19_annot_train_neut, kemdy20_annot_train_neut, kemdy19_annot_train_not_neut, kemdy20_annot_train_not_neut])\n",
    "annot_test_final = pd.concat([kemdy19_annot_test, kemdy20_annot_test])\n",
    "annot_val_final = pd.concat([kemdy19_annot_val, kemdy20_annot_val])\n",
    "\n",
    "annot_train_final.reset_index(drop=True, inplace=True)\n",
    "annot_test_final.reset_index(drop=True, inplace=True)\n",
    "annot_val_final.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(len(annot_train_final), len(annot_test_final), len(annot_val_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9311 9311 9311\n"
     ]
    }
   ],
   "source": [
    "# train dataset neutral 4000개로 랜덤 뽑은 것 생성\n",
    "emb_train_final = {}\n",
    "emb_train_final['wav'] = []\n",
    "emb_train_final['txt'] = []\n",
    "emb_train_final['segment_id'] = []\n",
    "for segment_annot_id in kemdy19_annot_train_neut['Segment ID']:\n",
    "    for wav, txt, segment_emb_id in zip(kemdy19_emb_train['wav'], kemdy19_emb_train['txt'], kemdy19_emb_train['segment_id']):\n",
    "        if segment_annot_id == segment_emb_id:\n",
    "            emb_train_final['wav'].append(wav)\n",
    "            emb_train_final['txt'].append(txt)\n",
    "            emb_train_final['segment_id'].append(segment_emb_id)\n",
    "            \n",
    "for segment_annot_id in kemdy19_annot_train_not_neut['Segment ID']:\n",
    "    for wav, txt, segment_emb_id in zip(kemdy19_emb_train['wav'], kemdy19_emb_train['txt'], kemdy19_emb_train['segment_id']):\n",
    "        if segment_annot_id == segment_emb_id:\n",
    "            emb_train_final['wav'].append(wav)\n",
    "            emb_train_final['txt'].append(txt)\n",
    "            emb_train_final['segment_id'].append(segment_emb_id)\n",
    "        \n",
    "\n",
    "for segment_annot_id in kemdy20_annot_train_neut['Segment ID']:\n",
    "    for wav, txt, segment_emb_id in zip(kemdy20_emb_train['wav'], kemdy20_emb_train['txt'], kemdy20_emb_train['segment_id']):\n",
    "        if segment_emb_id == segment_annot_id:\n",
    "            emb_train_final['wav'].append(wav)\n",
    "            emb_train_final['txt'].append(txt)\n",
    "            emb_train_final['segment_id'].append(segment_emb_id)\n",
    "        \n",
    "for segment_annot_id in kemdy20_annot_train_not_neut['Segment ID']:\n",
    "    for wav, txt, segment_emb_id in zip(kemdy20_emb_train['wav'], kemdy20_emb_train['txt'], kemdy20_emb_train['segment_id']):\n",
    "        if segment_annot_id == segment_emb_id:\n",
    "            emb_train_final['wav'].append(wav)\n",
    "            emb_train_final['txt'].append(txt)\n",
    "            emb_train_final['segment_id'].append(segment_emb_id)\n",
    "            \n",
    "print(len(emb_train_final['wav']), len(emb_train_final['txt']), len(emb_train_final['segment_id']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch dataset 만들기\n",
    "- 참고: https://tutorials.pytorch.kr/beginner/basics/data_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "torch.set_default_dtype(torch.float32)\n",
    "print(torch.get_default_dtype())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EtriDataset(Dataset):\n",
    "    def __init__(self, file_names, \n",
    "                 text_embeddings, \n",
    "                 wav_embeddings, \n",
    "                 Temp,\n",
    "                 EDA,\n",
    "                 Emotion,\n",
    "                 Emotion_vec, \n",
    "                 Arousal, \n",
    "                 Valence):\n",
    "        self.file_names = file_names\n",
    "        self.text_embeddings = text_embeddings\n",
    "        self.wav_embeddings = wav_embeddings\n",
    "        self.temp = Temp\n",
    "        self.eda = EDA\n",
    "        self.label_emotion = Emotion\n",
    "        self.label_emotion_vec = Emotion_vec\n",
    "        self.label_arousal = Arousal\n",
    "        self.label_valence = Valence\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text_embeddings = self.text_embeddings[idx]\n",
    "        wav_embeddings = self.wav_embeddings[idx]\n",
    "        temp = self.temp[idx]\n",
    "        eda = self.eda[idx]\n",
    "        label_emotion = self.label_emotion[idx]\n",
    "        label_emotion_ext = self.label_emotion_vec[idx]\n",
    "        label_arousal = self.label_arousal[idx]\n",
    "        label_valence = self.label_valence[idx]\n",
    "        return text_embeddings, wav_embeddings, temp, eda, label_emotion, label_emotion_ext, label_arousal, label_valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Segment ID', 'Emotion', 'Valence', 'Arousal', 'emotion_vector',\n",
       "       'valence_vector', 'arousal_vector', 'EDA', 'TEMP', 'EDA length',\n",
       "       'TEMP length', 'Scaled EDA', 'Scaled TEMP'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annot_train_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [-1.9738758426167797, -1.9116486245641493, -1....\n",
       "1       [-0.4630346865653657, -0.10242922491996066, -0...\n",
       "2       [-0.6250026128208589, -0.6098925796417516, -0....\n",
       "3       [1.1610535088678586, 1.1465380430005419, 1.161...\n",
       "4       [0.17325996510114683, -0.2880033940502193, -0....\n",
       "                              ...                        \n",
       "9306    [1.0248824293032237, -0.8716879365334415, -0.1...\n",
       "9307    [0.9738374515615607, -0.011967660642870578, -0...\n",
       "9308    [-0.011967660642870578, -0.011967660642870578,...\n",
       "9309    [-0.011967660642870578, -0.011967660642870578,...\n",
       "9310    [-0.16042779153088987, -0.16061282278609879, 0...\n",
       "Name: Scaled EDA, Length: 9311, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annot_train_final['Scaled EDA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_362029/245938389.py:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  EDA = torch.Tensor(annot_train_final['Scaled EDA']),\n"
     ]
    }
   ],
   "source": [
    "# data load 및 나누기: https://076923.github.io/posts/Python-pytorch-11/\n",
    "\n",
    "# annot_train_final, annot_test_final, annot_val_final\n",
    "# emb_train_final, emb_test_final, emb_val_final\n",
    "\n",
    "# session을 통합시킨 데이터 셋을 만들었을 때\n",
    "dataset_train = EtriDataset(file_names = annot_train_final['Segment ID'],\n",
    "                      text_embeddings = torch.stack(emb_train_final['txt']),\n",
    "                      wav_embeddings = torch.stack(emb_train_final['wav']),\n",
    "                      Emotion = annot_train_final['Emotion'],\n",
    "                      Arousal = annot_train_final['Arousal'],\n",
    "                      Valence = annot_train_final['Valence'],\n",
    "                      EDA = torch.Tensor(annot_train_final['Scaled EDA']), \n",
    "                      Temp = torch.Tensor(annot_train_final['Scaled TEMP']), \n",
    "                      Emotion_vec = torch.Tensor(annot_train_final['emotion_vector'])) \n",
    "\n",
    "\n",
    "dataset_test = EtriDataset(file_names = annot_test_final['Segment ID'],\n",
    "                      text_embeddings = torch.stack(emb_test_final['txt']),\n",
    "                      wav_embeddings = torch.stack(emb_test_final['wav']),\n",
    "                      Emotion = annot_test_final['Emotion'],\n",
    "                      Arousal = annot_test_final['Arousal'],\n",
    "                      Valence = annot_test_final['Valence'],\n",
    "                      EDA = torch.Tensor(annot_test_final['Scaled EDA']), \n",
    "                      Temp = torch.Tensor(annot_test_final['Scaled TEMP']), \n",
    "                      Emotion_vec = torch.Tensor(annot_test_final['emotion_vector']))\n",
    "\n",
    "dataset_val = EtriDataset(file_names = annot_val_final['Segment ID'],\n",
    "                      text_embeddings = torch.stack(emb_val_final['txt']),\n",
    "                      wav_embeddings = torch.stack(emb_val_final['wav']),\n",
    "                      Emotion = annot_val_final['Emotion'],\n",
    "                      Arousal = annot_val_final['Arousal'],\n",
    "                      Valence = annot_val_final['Valence'],\n",
    "                      EDA = torch.Tensor(annot_val_final['Scaled EDA']), \n",
    "                      Temp = torch.Tensor(annot_val_final['Scaled TEMP']), \n",
    "                      Emotion_vec = torch.Tensor(annot_val_final['emotion_vector'])) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Size : 9311\n",
      "Validation Data Size : 4868\n",
      "Testing Data Size : 4637\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training Data Size : {len(dataset_train)}\")\n",
    "print(f\"Validation Data Size : {len(dataset_val)}\")\n",
    "print(f\"Testing Data Size : {len(dataset_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset_train, batch_size=512, shuffle=True, drop_last=True)\n",
    "validation_dataloader = DataLoader(dataset_val, batch_size=128, shuffle=True, drop_last=True)\n",
    "test_dataloader = DataLoader(dataset_test, batch_size=128, shuffle=True, drop_last=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NetWork 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNetwork_pre(nn.Module):\n",
    "    def __init__(self, input_length, input_width):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_length*input_width, 768)\n",
    "        self.gelu1 = nn.GELU()\n",
    "        self.bn1 = nn.BatchNorm1d(768)\n",
    "        self.fc2 = nn.Linear(768, 512)\n",
    "        self.gelu2 = nn.GELU()\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.fc3 = nn.Linear(512, 32)\n",
    "        self.gelu3 = nn.GELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.gelu1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.gelu2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.fc3(x)\n",
    "        output = self.gelu3(x)\n",
    "        return output\n",
    "    \n",
    "class ConvNetwork_pre(nn.Module):\n",
    "    def __init__(self, input_channel):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels = input_channel, out_channels= 32, kernel_size = 3, padding = 1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(in_channels = 32, out_channels = 20, kernel_size = 3, padding = 1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        output = self.relu2(x)\n",
    "        return output\n",
    "\n",
    "class ConvNetwork_final(nn.Module):\n",
    "    def __init__(self, input_channel):\n",
    "        super().__init__()\n",
    "        self.conv2d_1 = nn.Conv2d(in_channels = input_channel, out_channels = 64, kernel_size=2)\n",
    "        self.leakyrelu_1 = nn.LeakyReLU()\n",
    "        self.maxpool2d_1 = nn.MaxPool2d(2)\n",
    "        self.conv2d_2 = nn.Conv2d(in_channels = 64, out_channels = 32, kernel_size=2)\n",
    "        self.leakyrelu_2 = nn.LeakyReLU()\n",
    "        self.maxpool2d_2 = nn.MaxPool2d(2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(896, 64)\n",
    "        self.leakyrelu_3 = nn.LeakyReLU()\n",
    "        self.batchnorm = nn.BatchNorm1d(64)\n",
    "        self.drop = nn.Dropout(p=0.25)\n",
    "        self.fc2 = nn.Linear(64, 7)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv2d_1(x)\n",
    "        x = self.leakyrelu_1(x)\n",
    "        x = self.maxpool2d_1(x)\n",
    "        x = self.conv2d_2(x)\n",
    "        x = self.leakyrelu_2(x)\n",
    "        x = self.maxpool2d_2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.leakyrelu_3(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.drop(x)\n",
    "        output = self.fc2(x)  \n",
    "        return output\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorFusionMixer(nn.Module):\n",
    "    def __init__(self, ModelA, ModelB, ModelC, ModelD, ModelE):\n",
    "        super().__init__()\n",
    "        self.ModelA = ModelA\n",
    "        self.ModelB = ModelB\n",
    "        self.ModelC = ModelC\n",
    "        self.ModelD = ModelD\n",
    "        self.Model_cnn_final = ModelE\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "        # self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def tensor_fusion(self, batch_arr1, batch_arr2, batch_arr3):\n",
    "        fusion_matrix_lst = []\n",
    "        for i, (arr1, arr2, arr3) in enumerate(zip(batch_arr1, batch_arr2, batch_arr3)):\n",
    "            arr1 = arr1.unsqueeze(-1).unsqueeze(-1)\n",
    "            arr2 = arr2.unsqueeze(0).unsqueeze(-1)\n",
    "            arr3 = arr3.squeeze().unsqueeze(0).unsqueeze(0)\n",
    "            \n",
    "            # outer_matrix = torch.einsum('i,j,kp->ijk', arr1, arr2, arr3)\n",
    "            outer_matrix = torch.kron(torch.kron(arr1,arr2), arr3)\n",
    "            l, w, d = outer_matrix.shape\n",
    "            \n",
    "            outer_matrix = outer_matrix.view(-1, l, w, d)\n",
    "            fusion_matrix_lst.append(outer_matrix)\n",
    "            \n",
    "        fusion_matrix = torch.concat(fusion_matrix_lst)\n",
    "        \n",
    "        return fusion_matrix\n",
    "        \n",
    "    def forward(self, x1, x2, x3, x4):\n",
    "        x1 = self.ModelA(x1)\n",
    "        x2 = self.ModelB(x2)\n",
    "        x3 = self.ModelC(x3)\n",
    "        x4 = self.ModelD(x4)\n",
    "        x5 = torch.cat([x3,x4], dim=0)\n",
    "        \n",
    "        fusion_matrix = self.tensor_fusion(x1, x2, x5) \n",
    "        x = self.Model_cnn_final(fusion_matrix) # 새로운 emotion사용\n",
    "        output = self.softmax(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [0, 0, 0, 1, 9, 0, 0]\n",
       "1     [0, 0, 0, 0, 9, 1, 0]\n",
       "2     [0, 0, 0, 1, 9, 0, 0]\n",
       "3    [0, 0, 0, 0, 10, 0, 0]\n",
       "4     [0, 0, 0, 0, 8, 2, 0]\n",
       "5     [0, 0, 0, 0, 9, 1, 0]\n",
       "6     [0, 4, 0, 0, 5, 0, 1]\n",
       "7     [0, 0, 2, 0, 4, 2, 2]\n",
       "8     [0, 0, 0, 0, 8, 0, 2]\n",
       "9     [0, 0, 0, 2, 8, 0, 0]\n",
       "Name: emotion_vector, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annot_train_final['emotion_vector'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 768 149 1024\n"
     ]
    }
   ],
   "source": [
    "# txt_input_length, txt_input_width = raw_dataset[session]['text_embeddings'][0].shape | 마지막엔 지울 것\n",
    "# _, wav_input_length, wav_input_width = raw_dataset[session]['wav_embeddings'][0].shape\n",
    "txt_input_length, txt_input_width = torch.Tensor(emb_train_final['txt'][0]).shape\n",
    "wav_input_length, wav_input_width = torch.Tensor(emb_train_final['wav'][0]).shape\n",
    "temp_input_length = annot_train_final['Scaled EDA'][0].shape[0]\n",
    "eda_input_length = annot_train_final['Scaled TEMP'][0].shape[0]\n",
    "\n",
    "print(txt_input_length, txt_input_width, wav_input_length, wav_input_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 4 GPUs!\n",
      "DataParallel(\n",
      "  (module): TensorFusionMixer(\n",
      "    (ModelA): MLPNetwork_pre(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=61440, out_features=768, bias=True)\n",
      "      (gelu1): GELU(approximate='none')\n",
      "      (bn1): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc2): Linear(in_features=768, out_features=512, bias=True)\n",
      "      (gelu2): GELU(approximate='none')\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc3): Linear(in_features=512, out_features=32, bias=True)\n",
      "      (gelu3): GELU(approximate='none')\n",
      "    )\n",
      "    (ModelB): MLPNetwork_pre(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=152576, out_features=768, bias=True)\n",
      "      (gelu1): GELU(approximate='none')\n",
      "      (bn1): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc2): Linear(in_features=768, out_features=512, bias=True)\n",
      "      (gelu2): GELU(approximate='none')\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc3): Linear(in_features=512, out_features=32, bias=True)\n",
      "      (gelu3): GELU(approximate='none')\n",
      "    )\n",
      "    (ModelC): ConvNetwork_pre(\n",
      "      (conv1): Conv1d(50, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (relu1): ReLU()\n",
      "      (conv2): Conv1d(32, 20, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (relu2): ReLU()\n",
      "    )\n",
      "    (ModelD): ConvNetwork_pre(\n",
      "      (conv1): Conv1d(50, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (relu1): ReLU()\n",
      "      (conv2): Conv1d(32, 20, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (relu2): ReLU()\n",
      "    )\n",
      "    (Model_cnn_final): ConvNetwork_final(\n",
      "      (conv2d_1): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "      (leakyrelu_1): LeakyReLU(negative_slope=0.01)\n",
      "      (maxpool2d_1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (conv2d_2): Conv2d(64, 32, kernel_size=(2, 2), stride=(1, 1))\n",
      "      (leakyrelu_2): LeakyReLU(negative_slope=0.01)\n",
      "      (maxpool2d_2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=896, out_features=64, bias=True)\n",
      "      (leakyrelu_3): LeakyReLU(negative_slope=0.01)\n",
      "      (batchnorm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop): Dropout(p=0.25, inplace=False)\n",
      "      (fc2): Linear(in_features=64, out_features=7, bias=True)\n",
      "    )\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# tf_mixer에 들어갈 wav mlp, txt mlp 선언\n",
    "model_mlp_txt = MLPNetwork_pre(txt_input_length,txt_input_width).to(device)\n",
    "model_mlp_wav = MLPNetwork_pre(wav_input_length,wav_input_width).to(device)\n",
    "model_conv_temp = ConvNetwork_pre(temp_input_length).to(device)\n",
    "model_conv_eda = ConvNetwork_pre(eda_input_length).to(device)\n",
    "\n",
    "model_cnn_final = ConvNetwork_final(32).to(device)\n",
    "\n",
    "# 최종 모델 선언\n",
    "model_tf_cnn_mixer = TensorFusionMixer(ModelA = model_mlp_txt, \n",
    "                                   ModelB = model_mlp_wav,\n",
    "                                   ModelC = model_conv_temp,\n",
    "                                   ModelD = model_conv_eda,\n",
    "                                   ModelE = model_cnn_final).to(device)\n",
    "\n",
    "# model 병렬 학습 처리\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model_mlp_txt = nn.DataParallel(model_mlp_txt).to(device)\n",
    "    model_mlp_wav = nn.DataParallel(model_mlp_wav).to(device)\n",
    "    model_conv_temp = nn.DataParallel(model_conv_temp).to(device)\n",
    "    model_conv_eda = nn.DataParallel(model_conv_eda).to(device)\n",
    "    model_tf_cnn_mixer = nn.DataParallel(model_tf_cnn_mixer).to(device)\n",
    "print(model_tf_cnn_mixer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습을 위한 train, test method 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)    \n",
    "    # data 순서: text_embeddings, wav_embeddings, temp, eda, label_emotion, label_emotion_ext, label_arousal, label_valence\n",
    "    for batch, (X_txt, X_wav, X_temp, X_eda, \n",
    "                    label_emotion, label_emotion_vec, label_arousal, label_valence) in enumerate(dataloader): \n",
    "        y = label_emotion_vec # 라벨을 변경하고자 하면 이 변수만 바꿔주면 나머지는 y로 적용\n",
    "        # 예측 오류 계산 \n",
    "        X_txt, X_wav, X_temp, X_eda, y= X_txt.to(device), X_wav.to(device), X_temp.to(device), X_eda.to(device),y.type(torch.float16).to(device)\n",
    "        \n",
    "        X_temp = X_temp.unsqueeze(dim=-1)\n",
    "        X_eda = X_eda.unsqueeze(dim=-1)\n",
    "        \n",
    "        pred = model(X_txt, X_wav, X_temp, X_eda)\n",
    "        y = F.softmax(y, dim = 1)\n",
    "        \n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # 역전파\n",
    "        optimizer.zero_grad()\n",
    "        loss.mean().backward() # weighted MSE를 사용할 경우 중간에 sum() or mean()을 넣어줌\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.mean().item(), batch * len(X_txt) # weighted MSE를 사용할 경우 중간에 sum() or mean()을 넣어줌\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn, mode = 'test'):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    f1 = F1Score(task= 'multiclass', num_classes=7).to(device)   # 바로 multiclassification할 경우\n",
    "    preds = []\n",
    "    targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # data 순서: text_embeddings, wav_embeddings, temp, eda, label_emotion, label_emotion_ext, label_arousal, label_valence\n",
    "        for batch, (X_txt, X_wav, X_temp, X_eda, \n",
    "                        label_emotion, label_emotion_vec, label_arousal, label_valence) in enumerate(dataloader): \n",
    "            y = label_emotion_vec # 라벨을 변경하고자 하면 이 변수만 바꿔주면 나머지는 y로 적용\n",
    "            # 예측 오류 계산\n",
    "            X_txt, X_wav, X_temp, X_eda, y= X_txt.to(device), X_wav.to(device), X_temp.to(device), X_eda.to(device),y.type(torch.float16).to(device)\n",
    "            \n",
    "            X_temp = X_temp.unsqueeze(dim=-1)\n",
    "            X_eda = X_eda.unsqueeze(dim=-1)\n",
    "            \n",
    "            pred = model(X_txt, X_wav, X_temp, X_eda)\n",
    "            y = F.softmax(y, dim = 1)\n",
    "            # pred.topk(2)\n",
    "            preds.append(pred.argmax(1)) # multi regression후 classification으로 변환할 경우\n",
    "            targets.append(label_emotion) # classification을 할 경우 언제나 사용\n",
    "            # print('예측라벨분포:',pred[:2], '정답라벨 분포:', label_emotion_ext[:2], '예측정답:', pred.argmax(1)[:2],'정답:', label_emotion[:2])\n",
    "            # print('예측:', pred.argmax(1).tolist()[:2],'\\n', '정답:', label_emotion.tolist()[:2])\n",
    "            # https://discuss.pytorch.org/t/loss-backward-raises-error-grad-can-be-implicitly-created-only-for-scalar-outputs/12152/6\n",
    "            test_loss += loss_fn(pred, y).mean().item()# weighted MSE를 사용할 경우 중간에 sum() or mean()을 넣어줌 \n",
    "            \n",
    "            correct += (pred.argmax(1) == label_emotion.to(device)).type(torch.float).sum().item()\n",
    "            \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    if mode == 'test':\n",
    "        print(torch.cat(preds), torch.cat(preds).shape)\n",
    "        print(\"f1 score: \", f1(torch.cat(preds).to(device), torch.cat(targets).to(device)))\n",
    "        print(f\"Test Error: Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\\n\")\n",
    "    elif mode == 'val':\n",
    "        print(f\"Validation Error: Accuracy: {(100*correct):>0.1f}%, Avg val loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total obs:  8671\n",
      "Counter({4: 5000, 3: 1232, 0: 954, 6: 596, 5: 437, 2: 269, 340: 257, 1: 183, 40: 91, 450: 66, 460: 58, 140: 48, 10: 21, 350: 15, 250: 14, 240: 13, 50: 9, 20: 6, 60: 6, 560: 5, 230: 4, 260: 3, 7140: 3, 150: 2, 1450: 2, 3450: 2, 7340: 1, 7250: 1, 730: 1, 7240: 1, 7450: 1, 4560: 1, 2450: 1, 3460: 1, 130: 1, 20000: 1, 1460: 1, 10000: 1, 360: 1, 3410: 1, 2340: 1})\n",
      "0 is in single emotion\n",
      "1 is in single emotion\n",
      "2 is in single emotion\n",
      "3 is in single emotion\n",
      "4 is in single emotion\n",
      "5 is in single emotion\n",
      "6 is in single emotion\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.8901, 0.9790, 0.9688, 0.8579, 0.4233, 0.9497, 0.9312],\n",
       "       dtype=torch.float16)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weigted loss for imbalance data: https://naadispeaks.wordpress.com/2021/07/31/handling-imbalanced-classes-with-weighted-loss-in-pytorch/\n",
    "# weight 계산\n",
    "single_emotion = [0,1,2,3,4,5,6]\n",
    "total_obs = 0\n",
    "for i in single_emotion:\n",
    "    total_obs += Counter(annot_train_final['Emotion'])[i]\n",
    "print('total obs: ', total_obs)\n",
    "\n",
    "weight_for_class = []\n",
    "print(Counter(annot_train_final['Emotion']))\n",
    "for key, value in sorted(Counter(annot_train_final['Emotion']).items()):\n",
    "    if key in single_emotion:\n",
    "        print(f'{key} is in single emotion')\n",
    "        if key == 'neutral':\n",
    "            weight_for_class.append(1 - (1440/total_obs))\n",
    "        else:\n",
    "            weight_for_class.append(1 - (value/total_obs))\n",
    "weight_for_class = torch.Tensor(weight_for_class).type(torch.float16)\n",
    "weight_for_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class weighted_MSELoss(nn.Module):\n",
    "    def __init__(self, weight):\n",
    "        super().__init__()\n",
    "        self.weight = weight.to(device)\n",
    "    def forward(self,inputs,targets):\n",
    "        return ((inputs - targets)**2) * self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 지난 저장한 모델이 있다면\n",
    "# PATH = './data/test_model.pkl'\n",
    "# model_tf_mixer = torch.load(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = nn.CrossEntropyLoss(weight=weight_for_class).to(device)\n",
    "# loss_fn = nn.CrossEntropyLoss().to(device) # weigth를 주기위해 위의 loss로 임시 변경\n",
    "loss_fn = weighted_MSELoss(weight = weight_for_class).to(device) # multi target regression(감정별로 count 한 타겟)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "\n",
    "# optimizer = optim.SGD(model_tf_mixer.parameters(), lr=lr) # classification\n",
    "optimizer = optim.Adam(model_tf_cnn_mixer.parameters(), lr=lr) # regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start training mlp fusion mixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Epoch 1----------------\n",
      "loss: 0.055926  [    0/ 9311]\n",
      "Validation Error: Accuracy: 25.1%, Avg val loss: 0.067268 \n",
      "\n",
      "---------------Epoch 2----------------\n",
      "loss: 0.067558  [    0/ 9311]\n",
      "Validation Error: Accuracy: 52.6%, Avg val loss: 0.055508 \n",
      "\n",
      "---------------Epoch 3----------------\n",
      "loss: 0.047290  [    0/ 9311]\n",
      "Validation Error: Accuracy: 58.6%, Avg val loss: 0.054345 \n",
      "\n",
      "---------------Epoch 4----------------\n",
      "loss: 0.040393  [    0/ 9311]\n",
      "Validation Error: Accuracy: 48.9%, Avg val loss: 0.059643 \n",
      "\n",
      "---------------Epoch 5----------------\n",
      "loss: 0.039254  [    0/ 9311]\n",
      "Validation Error: Accuracy: 42.9%, Avg val loss: 0.064502 \n",
      "\n",
      "---------------Epoch 6----------------\n",
      "loss: 0.029827  [    0/ 9311]\n",
      "Validation Error: Accuracy: 48.7%, Avg val loss: 0.063351 \n",
      "\n",
      "---------------Epoch 7----------------\n",
      "loss: 0.026060  [    0/ 9311]\n",
      "Validation Error: Accuracy: 39.8%, Avg val loss: 0.069778 \n",
      "\n",
      "---------------Epoch 8----------------\n",
      "loss: 0.027826  [    0/ 9311]\n",
      "Validation Error: Accuracy: 45.4%, Avg val loss: 0.067326 \n",
      "\n",
      "---------------Epoch 9----------------\n",
      "loss: 0.022440  [    0/ 9311]\n",
      "Validation Error: Accuracy: 40.3%, Avg val loss: 0.070940 \n",
      "\n",
      "---------------Epoch 10----------------\n",
      "loss: 0.018129  [    0/ 9311]\n",
      "Validation Error: Accuracy: 44.9%, Avg val loss: 0.070532 \n",
      "\n",
      "---------------Epoch 11----------------\n",
      "loss: 0.015861  [    0/ 9311]\n",
      "Validation Error: Accuracy: 39.1%, Avg val loss: 0.074449 \n",
      "\n",
      "---------------Epoch 12----------------\n",
      "loss: 0.016391  [    0/ 9311]\n",
      "Validation Error: Accuracy: 44.6%, Avg val loss: 0.070414 \n",
      "\n",
      "---------------Epoch 13----------------\n",
      "loss: 0.012457  [    0/ 9311]\n",
      "Validation Error: Accuracy: 42.0%, Avg val loss: 0.073341 \n",
      "\n",
      "---------------Epoch 14----------------\n",
      "loss: 0.011251  [    0/ 9311]\n",
      "Validation Error: Accuracy: 33.4%, Avg val loss: 0.084202 \n",
      "\n",
      "---------------Epoch 15----------------\n",
      "loss: 0.009873  [    0/ 9311]\n",
      "Validation Error: Accuracy: 43.3%, Avg val loss: 0.072166 \n",
      "\n",
      "---------------Epoch 16----------------\n",
      "loss: 0.010268  [    0/ 9311]\n",
      "Validation Error: Accuracy: 42.1%, Avg val loss: 0.073459 \n",
      "\n",
      "---------------Epoch 17----------------\n",
      "loss: 0.007617  [    0/ 9311]\n",
      "Validation Error: Accuracy: 33.8%, Avg val loss: 0.083324 \n",
      "\n",
      "---------------Epoch 18----------------\n",
      "loss: 0.006535  [    0/ 9311]\n",
      "Validation Error: Accuracy: 36.3%, Avg val loss: 0.080211 \n",
      "\n",
      "---------------Epoch 19----------------\n",
      "loss: 0.006724  [    0/ 9311]\n",
      "Validation Error: Accuracy: 42.7%, Avg val loss: 0.074422 \n",
      "\n",
      "---------------Epoch 20----------------\n",
      "loss: 0.006601  [    0/ 9311]\n",
      "Validation Error: Accuracy: 37.0%, Avg val loss: 0.078766 \n",
      "\n",
      "---------------Epoch 21----------------\n",
      "loss: 0.005211  [    0/ 9311]\n",
      "Validation Error: Accuracy: 44.3%, Avg val loss: 0.072101 \n",
      "\n",
      "---------------Epoch 22----------------\n",
      "loss: 0.005283  [    0/ 9311]\n",
      "Validation Error: Accuracy: 40.8%, Avg val loss: 0.074989 \n",
      "\n",
      "---------------Epoch 23----------------\n",
      "loss: 0.004154  [    0/ 9311]\n",
      "Validation Error: Accuracy: 39.1%, Avg val loss: 0.076100 \n",
      "\n",
      "---------------Epoch 24----------------\n",
      "loss: 0.002980  [    0/ 9311]\n",
      "Validation Error: Accuracy: 37.8%, Avg val loss: 0.077053 \n",
      "\n",
      "---------------Epoch 25----------------\n",
      "loss: 0.003325  [    0/ 9311]\n",
      "Validation Error: Accuracy: 41.4%, Avg val loss: 0.072212 \n",
      "\n",
      "---------------Epoch 26----------------\n",
      "loss: 0.003218  [    0/ 9311]\n",
      "Validation Error: Accuracy: 44.6%, Avg val loss: 0.068933 \n",
      "\n",
      "---------------Epoch 27----------------\n",
      "loss: 0.002345  [    0/ 9311]\n",
      "Validation Error: Accuracy: 42.7%, Avg val loss: 0.070518 \n",
      "\n",
      "---------------Epoch 28----------------\n",
      "loss: 0.003181  [    0/ 9311]\n",
      "Validation Error: Accuracy: 44.7%, Avg val loss: 0.069532 \n",
      "\n",
      "---------------Epoch 29----------------\n",
      "loss: 0.002365  [    0/ 9311]\n",
      "Validation Error: Accuracy: 43.2%, Avg val loss: 0.069826 \n",
      "\n",
      "---------------Epoch 30----------------\n",
      "loss: 0.002920  [    0/ 9311]\n",
      "Validation Error: Accuracy: 42.6%, Avg val loss: 0.070093 \n",
      "\n",
      "---------------Epoch 31----------------\n",
      "loss: 0.001692  [    0/ 9311]\n",
      "Validation Error: Accuracy: 44.2%, Avg val loss: 0.067807 \n",
      "\n",
      "---------------Epoch 32----------------\n",
      "loss: 0.001744  [    0/ 9311]\n",
      "Validation Error: Accuracy: 47.0%, Avg val loss: 0.065899 \n",
      "\n",
      "---------------Epoch 33----------------\n",
      "loss: 0.002117  [    0/ 9311]\n",
      "Validation Error: Accuracy: 41.0%, Avg val loss: 0.070004 \n",
      "\n",
      "---------------Epoch 34----------------\n",
      "loss: 0.001366  [    0/ 9311]\n",
      "Validation Error: Accuracy: 45.8%, Avg val loss: 0.066418 \n",
      "\n",
      "---------------Epoch 35----------------\n",
      "loss: 0.001477  [    0/ 9311]\n",
      "Validation Error: Accuracy: 48.0%, Avg val loss: 0.064918 \n",
      "\n",
      "---------------Epoch 36----------------\n",
      "loss: 0.001072  [    0/ 9311]\n",
      "Validation Error: Accuracy: 46.5%, Avg val loss: 0.066085 \n",
      "\n",
      "---------------Epoch 37----------------\n",
      "loss: 0.001453  [    0/ 9311]\n",
      "Validation Error: Accuracy: 43.0%, Avg val loss: 0.068203 \n",
      "\n",
      "---------------Epoch 38----------------\n",
      "loss: 0.001513  [    0/ 9311]\n",
      "Validation Error: Accuracy: 46.0%, Avg val loss: 0.065760 \n",
      "\n",
      "---------------Epoch 39----------------\n",
      "loss: 0.001100  [    0/ 9311]\n",
      "Validation Error: Accuracy: 46.8%, Avg val loss: 0.066079 \n",
      "\n",
      "---------------Epoch 40----------------\n",
      "loss: 0.001400  [    0/ 9311]\n",
      "Validation Error: Accuracy: 46.3%, Avg val loss: 0.066081 \n",
      "\n",
      "---------------Epoch 41----------------\n",
      "loss: 0.002161  [    0/ 9311]\n",
      "Validation Error: Accuracy: 45.4%, Avg val loss: 0.066820 \n",
      "\n",
      "---------------Epoch 42----------------\n",
      "loss: 0.001122  [    0/ 9311]\n",
      "Validation Error: Accuracy: 45.9%, Avg val loss: 0.066489 \n",
      "\n",
      "---------------Epoch 43----------------\n",
      "loss: 0.001259  [    0/ 9311]\n",
      "Validation Error: Accuracy: 46.6%, Avg val loss: 0.065417 \n",
      "\n",
      "---------------Epoch 44----------------\n",
      "loss: 0.001156  [    0/ 9311]\n",
      "Validation Error: Accuracy: 45.8%, Avg val loss: 0.065991 \n",
      "\n",
      "---------------Epoch 45----------------\n",
      "loss: 0.001207  [    0/ 9311]\n",
      "Validation Error: Accuracy: 47.4%, Avg val loss: 0.064156 \n",
      "\n",
      "---------------Epoch 46----------------\n",
      "loss: 0.001201  [    0/ 9311]\n",
      "Validation Error: Accuracy: 47.7%, Avg val loss: 0.064204 \n",
      "\n",
      "---------------Epoch 47----------------\n",
      "loss: 0.000903  [    0/ 9311]\n",
      "Validation Error: Accuracy: 49.5%, Avg val loss: 0.063309 \n",
      "\n",
      "---------------Epoch 48----------------\n",
      "loss: 0.000660  [    0/ 9311]\n",
      "Validation Error: Accuracy: 44.8%, Avg val loss: 0.066827 \n",
      "\n",
      "---------------Epoch 49----------------\n",
      "loss: 0.001100  [    0/ 9311]\n",
      "Validation Error: Accuracy: 48.2%, Avg val loss: 0.064007 \n",
      "\n",
      "---------------Epoch 50----------------\n",
      "loss: 0.000924  [    0/ 9311]\n",
      "Validation Error: Accuracy: 46.9%, Avg val loss: 0.064729 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Set the Training Parameters\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    print(f\"---------------Epoch {epoch+1}----------------\")\n",
    "    train(train_dataloader, model_tf_cnn_mixer, loss_fn, optimizer)\n",
    "    test(validation_dataloader, model_tf_cnn_mixer, loss_fn, mode = 'val')\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실험을 위해 모델 저장\n",
    "PATH = './data/model_multilabel_CNN.pkl'\n",
    "torch.save(model_tf_cnn_mixer, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test(test_dataloader, model_tf_cnn_mixer, loss_fn, mode \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[29], line 18\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(dataloader, model, loss_fn, mode)\u001b[0m\n\u001b[1;32m     16\u001b[0m y \u001b[39m=\u001b[39m label_emotion_ext \u001b[39m# 라벨을 변경하고자 하면 이 변수만 바꿔주면 나머지는 y로 적용\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39m# 예측 오류 계산\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m X_txt, X_wav, X_temp, X_eda, y\u001b[39m=\u001b[39m X_txt\u001b[39m.\u001b[39;49mto(device), X_wav\u001b[39m.\u001b[39mto(device), X_temp\u001b[39m.\u001b[39mto(device), X_eda\u001b[39m.\u001b[39mto(device),y\u001b[39m.\u001b[39mtype(torch\u001b[39m.\u001b[39mfloat16)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     20\u001b[0m X_temp \u001b[39m=\u001b[39m X_temp\u001b[39m.\u001b[39munsqueeze(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     21\u001b[0m X_eda \u001b[39m=\u001b[39m X_eda\u001b[39m.\u001b[39munsqueeze(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "test(test_dataloader, model_tf_cnn_mixer, loss_fn, mode = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0372, -0.1859,  0.3214,  ..., -0.5023, -0.2907, -0.5705],\n",
      "         [-0.3403, -0.6411,  0.2710,  ..., -0.4994,  0.1688, -0.6985],\n",
      "         [-0.2038,  0.1151,  0.3303,  ...,  0.0532,  0.0731, -0.7255],\n",
      "         ...,\n",
      "         [ 0.0387,  0.1966,  0.1149,  ..., -0.5742, -0.1841, -0.6823],\n",
      "         [ 0.3737,  0.0196, -0.0652,  ..., -0.3645,  0.1043, -0.9926],\n",
      "         [ 0.4129, -0.1367,  0.0095,  ..., -0.4642,  0.2714, -0.9962]],\n",
      "\n",
      "        [[ 1.3079,  0.3727,  1.6503,  ...,  0.9950, -0.2278,  1.1599],\n",
      "         [ 1.6657, -0.2183,  2.2059,  ...,  0.7725,  1.6774,  0.4149],\n",
      "         [-0.3042, -1.3660,  1.2436,  ...,  1.1886, -0.1148,  1.5758],\n",
      "         ...,\n",
      "         [ 0.9146, -0.8450,  1.8701,  ...,  0.0545, -0.1092,  1.0398],\n",
      "         [ 0.9892, -0.2653,  1.9712,  ...,  0.7016,  0.0530,  0.9379],\n",
      "         [ 0.9061, -0.3329,  1.8953,  ...,  0.7556,  0.1093,  0.9114]],\n",
      "\n",
      "        [[-0.3704, -0.2984,  0.9155,  ..., -1.2681, -0.0433,  0.2413],\n",
      "         [ 0.1247, -0.0254,  0.4202,  ..., -0.4393,  0.1123,  0.1968],\n",
      "         [-1.0448, -0.5985,  0.6934,  ..., -0.5695,  1.1614, -0.0849],\n",
      "         ...,\n",
      "         [-0.6295, -0.2794,  0.1143,  ..., -0.8961,  0.5165,  0.2536],\n",
      "         [-0.2717, -0.5565,  0.5098,  ..., -1.0587, -0.0360, -0.1115],\n",
      "         [-0.4691, -0.6067,  0.6975,  ..., -0.9010, -0.1648,  0.0236]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.3711,  0.6465,  0.2282,  ...,  0.1984,  0.3917,  2.0942],\n",
      "         [-0.3295, -0.2967,  0.2349,  ..., -0.5635,  0.5554,  1.1205],\n",
      "         [-0.4232,  0.6536,  0.8567,  ..., -0.2254,  1.1812,  1.4104],\n",
      "         ...,\n",
      "         [ 0.4545,  0.4273,  0.3971,  ..., -0.2936,  0.3598,  2.0109],\n",
      "         [ 0.3698,  1.0537,  0.3064,  ...,  0.0722,  0.5218,  1.6172],\n",
      "         [-0.0170,  1.2301,  0.0804,  ..., -0.0447,  0.2919,  1.2817]],\n",
      "\n",
      "        [[-1.5291, -0.8282,  0.1631,  ..., -0.3010, -0.1365,  1.0752],\n",
      "         [-1.4284, -0.3882,  0.3067,  ..., -0.3796, -0.1911,  0.2225],\n",
      "         [-2.6898, -0.8852,  0.2476,  ...,  0.5852, -0.2555,  1.3906],\n",
      "         ...,\n",
      "         [-1.6336, -0.6777,  0.6971,  ..., -0.2678, -0.1649,  0.3367],\n",
      "         [-1.4160, -0.6439,  0.5943,  ..., -0.2081, -0.0875,  0.2894],\n",
      "         [-1.4706, -0.3374,  0.6136,  ..., -0.3026, -0.0718,  0.1942]],\n",
      "\n",
      "        [[ 1.4557, -2.8615,  0.1629,  ..., -0.6511, -0.0127,  0.0118],\n",
      "         [ 0.2037, -1.1833,  0.7604,  ..., -1.3256,  0.9924,  0.2238],\n",
      "         [-0.9512, -1.3324, -0.5775,  ...,  0.0492,  0.3346,  1.1628],\n",
      "         ...,\n",
      "         [ 1.1983, -2.8069,  0.1824,  ..., -0.8388,  0.0659, -0.0033],\n",
      "         [ 1.0988, -2.6560,  0.0392,  ..., -0.5585,  0.1897, -0.2910],\n",
      "         [ 1.4445, -2.5254, -0.0109,  ..., -1.0336, -0.2785, -0.1369]]],\n",
      "       grad_fn=<StackBackward0>) tensor([[[[-9.0357e-01,  5.2524e-02, -6.6244e-02,  ...,  1.1921e-01,\n",
      "            1.1450e-01,  7.6236e-02],\n",
      "          [-9.0357e-01,  5.2524e-02, -6.6244e-02,  ...,  1.1921e-01,\n",
      "            1.1450e-01,  7.6236e-02],\n",
      "          [-9.0357e-01,  5.2524e-02, -6.6244e-02,  ...,  1.1921e-01,\n",
      "            1.1450e-01,  7.6236e-02],\n",
      "          ...,\n",
      "          [-1.1249e+00,  8.3771e-02,  1.0080e-01,  ..., -2.2124e-02,\n",
      "           -1.3009e-01,  1.4258e-01],\n",
      "          [-1.0617e+00,  8.3941e-02,  1.0207e-01,  ..., -4.3921e-02,\n",
      "           -1.2729e-01,  1.1116e-01],\n",
      "          [-1.0476e+00,  8.9427e-02,  1.0727e-01,  ..., -4.9117e-02,\n",
      "           -1.2314e-01,  1.0688e-01]]],\n",
      "\n",
      "\n",
      "        [[[-9.1485e-01,  9.7829e-02, -2.8695e-01,  ...,  3.6580e-02,\n",
      "            9.3810e-02,  1.4225e-01],\n",
      "          [-9.1485e-01,  9.7830e-02, -2.8695e-01,  ...,  3.6580e-02,\n",
      "            9.3810e-02,  1.4225e-01],\n",
      "          [-9.1485e-01,  9.7830e-02, -2.8695e-01,  ...,  3.6580e-02,\n",
      "            9.3809e-02,  1.4225e-01],\n",
      "          ...,\n",
      "          [-1.0800e+00,  3.3230e-02,  6.2465e-02,  ..., -8.3610e-02,\n",
      "           -1.2309e-01,  1.7907e-01],\n",
      "          [-1.0278e+00,  2.8668e-02,  3.0407e-02,  ..., -8.9343e-02,\n",
      "           -1.3253e-01,  1.3251e-01],\n",
      "          [-1.0329e+00,  3.0989e-02,  2.8338e-02,  ..., -9.0294e-02,\n",
      "           -1.2358e-01,  1.3126e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.0149e+00,  2.5042e-01, -3.6062e-01,  ...,  5.4453e-02,\n",
      "            8.9558e-03,  2.1962e-01],\n",
      "          [-1.0149e+00,  2.5042e-01, -3.6062e-01,  ...,  5.4454e-02,\n",
      "            8.9560e-03,  2.1962e-01],\n",
      "          [-1.0149e+00,  2.5042e-01, -3.6062e-01,  ...,  5.4454e-02,\n",
      "            8.9559e-03,  2.1962e-01],\n",
      "          ...,\n",
      "          [-1.2202e+00,  3.7870e-02,  1.2962e-02,  ..., -7.2869e-02,\n",
      "           -1.5564e-01,  1.1701e-01],\n",
      "          [-1.2198e+00,  2.3194e-02,  1.5494e-02,  ..., -7.7052e-02,\n",
      "           -1.8145e-01,  1.0742e-01],\n",
      "          [-1.2384e+00,  6.6758e-02,  2.9554e-02,  ..., -8.2631e-02,\n",
      "           -1.8847e-01,  1.3013e-01]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-9.3850e-01,  2.2315e-01, -2.2688e-01,  ...,  1.2685e-01,\n",
      "           -6.1108e-02,  1.2668e-01],\n",
      "          [-9.3850e-01,  2.2315e-01, -2.2688e-01,  ...,  1.2685e-01,\n",
      "           -6.1108e-02,  1.2668e-01],\n",
      "          [-9.3850e-01,  2.2315e-01, -2.2688e-01,  ...,  1.2685e-01,\n",
      "           -6.1108e-02,  1.2668e-01],\n",
      "          ...,\n",
      "          [-1.0077e+00,  8.6026e-02,  7.5491e-03,  ..., -4.8786e-02,\n",
      "           -1.5376e-01,  1.5788e-01],\n",
      "          [-1.0043e+00,  8.1126e-02,  1.3322e-02,  ..., -6.0555e-02,\n",
      "           -1.3302e-01,  1.3763e-01],\n",
      "          [-1.0641e+00,  3.8397e-03,  1.6495e-02,  ...,  9.8441e-02,\n",
      "           -1.2825e-01,  1.8190e-01]]],\n",
      "\n",
      "\n",
      "        [[[-9.2240e-01,  1.7208e-01, -2.8582e-01,  ..., -2.0010e-03,\n",
      "            5.7543e-02,  1.0423e-01],\n",
      "          [-9.2240e-01,  1.7208e-01, -2.8582e-01,  ..., -2.0007e-03,\n",
      "            5.7543e-02,  1.0423e-01],\n",
      "          [-9.2240e-01,  1.7208e-01, -2.8582e-01,  ..., -2.0010e-03,\n",
      "            5.7543e-02,  1.0423e-01],\n",
      "          ...,\n",
      "          [-1.0281e+00, -1.1472e-01,  1.0541e-01,  ..., -1.1175e-02,\n",
      "           -6.2159e-02,  1.6739e-01],\n",
      "          [-9.8616e-01, -9.5252e-02,  7.5651e-02,  ..., -3.8075e-02,\n",
      "           -8.7918e-02,  9.3305e-02],\n",
      "          [-9.9209e-01, -1.0069e-01,  7.7865e-02,  ..., -3.6755e-02,\n",
      "           -9.8113e-02,  9.6975e-02]]],\n",
      "\n",
      "\n",
      "        [[[-9.5599e-01,  1.0903e-01, -2.2435e-01,  ...,  6.4858e-02,\n",
      "            8.9231e-02,  9.4291e-02],\n",
      "          [-9.5599e-01,  1.0903e-01, -2.2435e-01,  ...,  6.4858e-02,\n",
      "            8.9231e-02,  9.4292e-02],\n",
      "          [-9.5599e-01,  1.0903e-01, -2.2435e-01,  ...,  6.4858e-02,\n",
      "            8.9231e-02,  9.4292e-02],\n",
      "          ...,\n",
      "          [-1.0777e+00, -5.8531e-03,  4.7329e-02,  ..., -5.7925e-02,\n",
      "           -1.7681e-01, -2.5717e-05],\n",
      "          [-1.0827e+00, -3.8433e-03,  4.1373e-02,  ..., -5.9204e-02,\n",
      "           -1.7450e-01, -3.2594e-03],\n",
      "          [-1.1058e+00,  1.6713e-03,  2.3556e-02,  ..., -5.6343e-02,\n",
      "           -1.8394e-01,  1.1554e-03]]]]) tensor([ 6, 10,  1,  1,  1,  1,  1,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         3,  1,  0,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  6,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  0])\n",
      "tensor([[[-0.3135,  0.3598, -0.3683,  ..., -1.1105,  1.0331, -0.1598],\n",
      "         [-0.3732, -0.8260, -0.1992,  ..., -0.2585,  0.7229, -0.2742],\n",
      "         [-1.2590,  0.2766, -0.8130,  ...,  0.4777,  1.4969, -0.0415],\n",
      "         ...,\n",
      "         [-0.4296,  0.2825,  0.1939,  ..., -1.1886,  1.3916, -0.4094],\n",
      "         [-0.5759,  0.5137, -0.0092,  ..., -1.0802,  1.3445, -0.3047],\n",
      "         [-0.5025,  0.2482,  0.0948,  ..., -1.1301,  1.3666, -0.3354]],\n",
      "\n",
      "        [[-0.5878, -2.1452,  1.0545,  ..., -0.4789, -1.0487, -0.4928],\n",
      "         [-0.2062, -0.7940,  0.2777,  ..., -0.6237, -0.5193,  0.4430],\n",
      "         [-1.2222, -2.0101,  0.8862,  ..., -0.1185, -0.8109, -0.3826],\n",
      "         ...,\n",
      "         [-0.4429, -1.7486,  1.0510,  ..., -0.4025, -0.7158, -0.8259],\n",
      "         [-0.1854, -1.5451,  0.8015,  ..., -0.2343, -0.4666, -0.9025],\n",
      "         [-0.1624, -1.6228,  0.7988,  ..., -0.1428, -0.7545, -0.9342]],\n",
      "\n",
      "        [[-0.4113, -1.5791,  0.4911,  ..., -0.9297, -0.2178,  1.2849],\n",
      "         [ 0.3657, -2.1271,  1.0378,  ..., -0.1303, -0.0836,  0.6600],\n",
      "         [-0.5587, -1.1231,  1.0267,  ...,  0.0122, -0.0634,  0.9482],\n",
      "         ...,\n",
      "         [ 0.0981, -1.1373,  0.4401,  ..., -0.5581,  0.2197,  1.2133],\n",
      "         [ 0.2170, -1.0613,  0.4620,  ..., -0.5551,  0.4318,  1.1486],\n",
      "         [ 0.0779, -1.0698,  0.5305,  ..., -0.4075,  0.6547,  1.1789]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1763, -0.2627,  0.3106,  ..., -0.6048,  0.1521,  2.3133],\n",
      "         [-0.3986,  0.3017,  0.2455,  ..., -0.0210, -1.3808,  1.1792],\n",
      "         [-0.4408,  0.3925,  0.1968,  ..., -0.3023,  0.8824,  0.5384],\n",
      "         ...,\n",
      "         [ 0.3231, -0.1274,  0.3375,  ..., -0.7644,  0.5329,  2.0711],\n",
      "         [ 0.4434, -0.8133,  0.4733,  ..., -0.7896,  0.6990,  1.8203],\n",
      "         [ 0.4874, -0.6437,  0.3727,  ..., -0.7441,  0.6096,  1.8981]],\n",
      "\n",
      "        [[ 0.0809,  0.3388, -0.0343,  ..., -0.4724,  0.3061, -0.2449],\n",
      "         [-0.1504, -0.0187,  0.5355,  ..., -0.5271,  0.8820, -0.2652],\n",
      "         [-0.9447, -0.3966,  0.2720,  ...,  0.0304,  0.5730,  0.2580],\n",
      "         ...,\n",
      "         [-0.0282,  0.4224,  0.3070,  ..., -0.6286,  0.2938, -0.1323],\n",
      "         [ 0.0643,  0.5432,  0.2799,  ..., -0.3650,  0.4507, -0.3414],\n",
      "         [ 0.0622,  0.5037,  0.3131,  ..., -0.5660,  0.3684, -0.1913]],\n",
      "\n",
      "        [[ 0.2948, -0.6879, -0.8523,  ...,  0.0205, -0.0964,  2.5648],\n",
      "         [ 0.2895, -0.9104, -0.3288,  ...,  0.0568,  0.1937,  1.8813],\n",
      "         [-0.5450, -1.4383, -0.6964,  ...,  0.6062,  0.0495,  2.5453],\n",
      "         ...,\n",
      "         [ 0.5464, -0.9311, -0.6468,  ..., -0.0530,  0.2616,  2.2779],\n",
      "         [ 0.3521, -0.7418, -0.9001,  ...,  0.1439,  0.1110,  2.1081],\n",
      "         [ 0.4116, -0.4202, -0.9810,  ...,  0.0376, -0.2109,  1.8367]]],\n",
      "       grad_fn=<StackBackward0>) tensor([[[[-1.0329e+00,  2.5231e-01, -2.9089e-01,  ...,  9.0860e-02,\n",
      "            6.6954e-02,  1.4964e-01],\n",
      "          [-1.0329e+00,  2.5232e-01, -2.9089e-01,  ...,  9.0860e-02,\n",
      "            6.6954e-02,  1.4964e-01],\n",
      "          [-1.0329e+00,  2.5232e-01, -2.9089e-01,  ...,  9.0860e-02,\n",
      "            6.6953e-02,  1.4964e-01],\n",
      "          ...,\n",
      "          [-1.1227e+00,  8.8297e-02,  3.0206e-02,  ..., -5.6011e-02,\n",
      "           -1.6177e-01,  1.1444e-01],\n",
      "          [-1.1144e+00,  1.0119e-01,  1.8686e-02,  ..., -5.9961e-02,\n",
      "           -1.6070e-01,  9.5456e-02],\n",
      "          [-1.1550e+00,  1.4577e-01, -1.7564e-02,  ..., -6.0713e-02,\n",
      "           -1.3913e-01,  9.9464e-02]]],\n",
      "\n",
      "\n",
      "        [[[-9.7718e-01,  1.9968e-01, -2.2850e-01,  ...,  8.1957e-02,\n",
      "            4.5339e-02,  1.3011e-01],\n",
      "          [-9.7718e-01,  1.9968e-01, -2.2850e-01,  ...,  8.1956e-02,\n",
      "            4.5340e-02,  1.3011e-01],\n",
      "          [-9.7718e-01,  1.9968e-01, -2.2850e-01,  ...,  8.1956e-02,\n",
      "            4.5340e-02,  1.3011e-01],\n",
      "          ...,\n",
      "          [-9.4685e-01, -1.5891e-02, -2.6120e-02,  ..., -1.0307e-03,\n",
      "           -1.4277e-01,  1.0436e-01],\n",
      "          [-9.3327e-01, -1.1283e-02, -2.2786e-02,  ..., -4.9326e-03,\n",
      "           -1.4858e-01,  9.5232e-02],\n",
      "          [-9.2747e-01, -1.1818e-02, -2.2289e-02,  ..., -2.7388e-03,\n",
      "           -1.4094e-01,  9.1715e-02]]],\n",
      "\n",
      "\n",
      "        [[[-8.8776e-01,  3.2313e-02, -1.7624e-01,  ...,  4.2995e-02,\n",
      "            8.7901e-02,  1.1043e-01],\n",
      "          [-8.8776e-01,  3.2313e-02, -1.7624e-01,  ...,  4.2995e-02,\n",
      "            8.7901e-02,  1.1043e-01],\n",
      "          [-8.8776e-01,  3.2313e-02, -1.7624e-01,  ...,  4.2994e-02,\n",
      "            8.7901e-02,  1.1043e-01],\n",
      "          ...,\n",
      "          [-1.0266e+00, -2.2378e-02,  1.0031e-01,  ...,  6.0849e-03,\n",
      "           -1.1864e-01,  2.0036e-01],\n",
      "          [-9.5103e-01,  6.9663e-03,  7.7167e-02,  ..., -2.7488e-02,\n",
      "           -1.2608e-01,  1.3826e-01],\n",
      "          [-9.4713e-01,  7.2052e-03,  7.1369e-02,  ..., -3.1162e-02,\n",
      "           -1.2855e-01,  1.2912e-01]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-7.8421e-01,  1.1193e-01, -1.7644e-01,  ...,  1.2812e-01,\n",
      "            1.3950e-01,  1.7572e-01],\n",
      "          [-7.8421e-01,  1.1192e-01, -1.7644e-01,  ...,  1.2812e-01,\n",
      "            1.3950e-01,  1.7572e-01],\n",
      "          [-7.8421e-01,  1.1193e-01, -1.7644e-01,  ...,  1.2812e-01,\n",
      "            1.3950e-01,  1.7572e-01],\n",
      "          ...,\n",
      "          [-1.1580e+00,  1.3777e-01,  1.9458e-02,  ..., -3.1574e-02,\n",
      "           -7.9338e-02,  2.7576e-01],\n",
      "          [-1.0837e+00,  1.2958e-01,  5.5835e-03,  ..., -3.6117e-02,\n",
      "           -1.1474e-01,  2.0381e-01],\n",
      "          [-1.0788e+00,  1.2809e-01,  4.9062e-03,  ..., -3.5082e-02,\n",
      "           -1.1225e-01,  1.9607e-01]]],\n",
      "\n",
      "\n",
      "        [[[-7.5254e-01, -8.7008e-02, -8.7270e-03,  ...,  1.1036e-01,\n",
      "            8.8711e-02,  8.0031e-02],\n",
      "          [-7.5254e-01, -8.7008e-02, -8.7268e-03,  ...,  1.1036e-01,\n",
      "            8.8711e-02,  8.0030e-02],\n",
      "          [-7.5254e-01, -8.7009e-02, -8.7267e-03,  ...,  1.1036e-01,\n",
      "            8.8711e-02,  8.0031e-02],\n",
      "          ...,\n",
      "          [-1.0529e+00,  6.3521e-02,  9.9113e-02,  ...,  5.5668e-03,\n",
      "           -7.0969e-02,  1.1818e-01],\n",
      "          [-9.4167e-01,  3.5350e-02,  8.9927e-02,  ..., -1.1265e-04,\n",
      "           -8.4683e-02,  7.8517e-02],\n",
      "          [-1.2442e+00,  1.8088e-01,  6.0636e-02,  ...,  2.8081e-02,\n",
      "           -7.1830e-02,  1.5234e-01]]],\n",
      "\n",
      "\n",
      "        [[[-6.6984e-01, -1.0658e-01, -8.8424e-02,  ...,  1.2087e-01,\n",
      "            1.3490e-01,  2.2543e-01],\n",
      "          [-6.6984e-01, -1.0658e-01, -8.8423e-02,  ...,  1.2087e-01,\n",
      "            1.3490e-01,  2.2543e-01],\n",
      "          [-6.6984e-01, -1.0658e-01, -8.8423e-02,  ...,  1.2087e-01,\n",
      "            1.3490e-01,  2.2543e-01],\n",
      "          ...,\n",
      "          [-8.8718e-01,  1.0789e-01,  2.1709e-02,  ..., -4.2100e-02,\n",
      "           -1.7743e-01,  1.2572e-01],\n",
      "          [-8.8560e-01,  1.0437e-01,  2.0336e-02,  ..., -3.9385e-02,\n",
      "           -1.7578e-01,  1.1992e-01],\n",
      "          [-8.8841e-01,  1.0582e-01,  1.3285e-02,  ..., -3.6943e-02,\n",
      "           -1.7130e-01,  1.1547e-01]]]]) tensor([1, 1, 1, 7, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 6,\n",
      "        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1])\n",
      "tensor([[[ 0.2706, -2.0748,  0.2613,  ...,  0.9831,  1.2262,  1.1181],\n",
      "         [ 0.0071, -2.6216,  0.4937,  ...,  0.3073,  0.9557,  0.2179],\n",
      "         [ 1.1437, -1.4736,  0.3390,  ...,  0.3786,  1.3776,  0.5759],\n",
      "         ...,\n",
      "         [ 0.5352, -2.0011,  0.3887,  ...,  0.7555,  1.0884,  0.8362],\n",
      "         [ 0.6496, -1.7971,  0.2106,  ...,  0.9879,  1.1381,  0.8817],\n",
      "         [ 0.3137, -1.7771,  0.4166,  ...,  0.8690,  1.3066,  0.6907]],\n",
      "\n",
      "        [[ 0.3743, -0.6669,  0.3080,  ...,  0.3204, -0.3125, -0.5539],\n",
      "         [-0.3305, -0.8900, -0.1327,  ...,  0.5147,  0.1339,  0.2328],\n",
      "         [ 0.4230, -1.7240,  0.8676,  ...,  0.3009,  0.2500, -0.7881],\n",
      "         ...,\n",
      "         [ 0.2675, -1.2558,  0.4221,  ...,  0.4307, -0.4193, -0.7433],\n",
      "         [ 0.3531, -1.0410,  0.4476,  ...,  0.4841, -0.3353, -0.7836],\n",
      "         [ 0.3426, -1.1929,  0.3246,  ...,  0.1969, -0.2609, -0.6880]],\n",
      "\n",
      "        [[ 0.1577, -1.1772, -0.9367,  ...,  0.1546,  0.3775,  1.0188],\n",
      "         [ 0.7773,  0.1587, -0.8758,  ...,  0.2747,  0.7025,  0.4548],\n",
      "         [ 0.0934,  0.4195, -0.6326,  ...,  0.4970,  0.1487,  0.0999],\n",
      "         ...,\n",
      "         [-0.0077, -1.4304, -0.6231,  ...,  0.5612,  0.6655,  0.7615],\n",
      "         [-0.0530, -1.5044, -0.6926,  ...,  0.4903,  0.6846,  0.6522],\n",
      "         [-0.0241, -1.5696, -0.8064,  ...,  0.2045,  0.8276,  0.4099]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.8192, -0.5192,  0.6772,  ..., -0.9680,  0.2846,  1.0216],\n",
      "         [-0.8512, -0.4042,  0.7272,  ..., -1.0851, -0.5025,  0.3903],\n",
      "         [-1.6228, -0.3467,  1.5863,  ..., -0.3701,  0.3190,  0.8019],\n",
      "         ...,\n",
      "         [-0.4316, -0.5323,  0.8392,  ..., -0.9896,  1.0736,  0.6585],\n",
      "         [-0.1718, -0.2727,  0.6835,  ..., -0.8452,  1.4791,  0.5926],\n",
      "         [ 0.0358, -0.6472,  0.5381,  ..., -0.9594,  1.3403,  0.7104]],\n",
      "\n",
      "        [[ 0.0226, -0.6828,  0.9751,  ...,  0.2089, -1.2605, -0.1886],\n",
      "         [ 1.0976, -0.0049,  1.4372,  ...,  0.5795,  0.3054, -0.0175],\n",
      "         [ 0.9599,  1.9136,  0.8325,  ..., -0.1982,  0.3414,  0.3382],\n",
      "         ...,\n",
      "         [ 0.4563, -0.6131,  1.2568,  ...,  0.2645, -0.9909, -0.3038],\n",
      "         [ 0.4799, -0.5580,  1.1645,  ...,  0.1489, -0.8224, -0.4190],\n",
      "         [ 0.4545, -0.4306,  0.9884,  ...,  0.3384, -0.7370, -0.5919]],\n",
      "\n",
      "        [[ 1.6393,  0.8180,  1.3603,  ...,  0.9350, -0.5416,  0.0091],\n",
      "         [ 1.2747,  1.4548,  1.4296,  ...,  0.8422, -0.0632, -0.1956],\n",
      "         [ 0.5959,  0.7922,  0.4096,  ...,  1.1938, -1.1099,  0.0258],\n",
      "         ...,\n",
      "         [ 1.6985,  0.6637,  1.7474,  ...,  0.8837, -0.2234, -0.1947],\n",
      "         [ 1.7179,  0.7289,  1.6155,  ...,  1.0632, -0.1045, -0.0673],\n",
      "         [ 2.0439,  0.7932,  1.4941,  ...,  0.9373,  0.0464, -0.4613]]],\n",
      "       grad_fn=<StackBackward0>) tensor([[[[-0.9258,  0.0477, -0.0934,  ...,  0.1574,  0.0855,  0.1213],\n",
      "          [-0.9258,  0.0477, -0.0934,  ...,  0.1574,  0.0855,  0.1213],\n",
      "          [-0.9258,  0.0477, -0.0934,  ...,  0.1574,  0.0855,  0.1213],\n",
      "          ...,\n",
      "          [-1.1969,  0.1004,  0.0448,  ...,  0.0180, -0.1186,  0.1233],\n",
      "          [-1.1979,  0.1208,  0.0489,  ...,  0.0030, -0.1206,  0.1028],\n",
      "          [-1.2425,  0.1593,  0.0499,  ...,  0.0337, -0.1108,  0.1553]]],\n",
      "\n",
      "\n",
      "        [[[-1.2253,  0.1420, -0.1555,  ...,  0.0450,  0.0184,  0.1162],\n",
      "          [-1.2253,  0.1420, -0.1555,  ...,  0.0450,  0.0184,  0.1162],\n",
      "          [-1.2253,  0.1420, -0.1555,  ...,  0.0450,  0.0184,  0.1162],\n",
      "          ...,\n",
      "          [-1.3102,  0.1529, -0.0142,  ..., -0.1751, -0.2144,  0.0955],\n",
      "          [-1.3104,  0.1530, -0.0139,  ..., -0.1756, -0.2127,  0.0965],\n",
      "          [-1.3116,  0.1532, -0.0149,  ..., -0.1759, -0.2125,  0.0963]]],\n",
      "\n",
      "\n",
      "        [[[-0.8177,  0.1674, -0.2098,  ...,  0.1564,  0.0772,  0.1086],\n",
      "          [-0.8177,  0.1674, -0.2098,  ...,  0.1564,  0.0772,  0.1086],\n",
      "          [-0.8177,  0.1674, -0.2098,  ...,  0.1564,  0.0772,  0.1086],\n",
      "          ...,\n",
      "          [-0.8709, -0.0986,  0.0251,  ..., -0.0527, -0.1465,  0.1116],\n",
      "          [-0.8696, -0.1008,  0.0227,  ..., -0.0548, -0.1454,  0.1082],\n",
      "          [-0.8992, -0.1077,  0.0168,  ..., -0.0444, -0.1509,  0.1239]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.6363, -0.0621, -0.0562,  ...,  0.1645,  0.1488,  0.0590],\n",
      "          [-0.6363, -0.0621, -0.0562,  ...,  0.1645,  0.1488,  0.0590],\n",
      "          [-0.6363, -0.0621, -0.0562,  ...,  0.1645,  0.1488,  0.0590],\n",
      "          ...,\n",
      "          [-0.9920,  0.1409,  0.0267,  ..., -0.0613, -0.0972,  0.1257],\n",
      "          [-0.9585,  0.1190,  0.0240,  ..., -0.0635, -0.0993,  0.1077],\n",
      "          [-0.9534,  0.1134,  0.0204,  ..., -0.0633, -0.0953,  0.1015]]],\n",
      "\n",
      "\n",
      "        [[[-0.9750,  0.2988, -0.3390,  ...,  0.0466, -0.0477,  0.1502],\n",
      "          [-0.9750,  0.2988, -0.3390,  ...,  0.0466, -0.0477,  0.1502],\n",
      "          [-0.9750,  0.2988, -0.3390,  ...,  0.0466, -0.0477,  0.1502],\n",
      "          ...,\n",
      "          [-1.0560,  0.1591,  0.0230,  ..., -0.1238, -0.1589,  0.1883],\n",
      "          [-1.0549,  0.1518,  0.0205,  ..., -0.1230, -0.1753,  0.1804],\n",
      "          [-1.0502,  0.1404,  0.0174,  ..., -0.1211, -0.1989,  0.1731]]],\n",
      "\n",
      "\n",
      "        [[[-1.2709,  0.0327, -0.0804,  ...,  0.0564,  0.0618,  0.1136],\n",
      "          [-1.2709,  0.0327, -0.0804,  ...,  0.0564,  0.0618,  0.1136],\n",
      "          [-1.2709,  0.0327, -0.0804,  ...,  0.0564,  0.0618,  0.1136],\n",
      "          ...,\n",
      "          [-1.3558,  0.1184,  0.0471,  ..., -0.0375, -0.1273,  0.1439],\n",
      "          [-1.3506,  0.0862, -0.0512,  ...,  0.0142,  0.0152,  0.1259],\n",
      "          [-1.4284,  0.1153, -0.0139,  ...,  0.0112, -0.0878,  0.1349]]]]) tensor([ 1,  1,  1,  1,  1,  1,  0,  0,  1,  1,  1,  1,  1,  0,  1,  1,  1,  1,\n",
      "         1,  1,  1,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0,  1, 13,  1,\n",
      "         1,  1,  0,  1,  1,  1,  7,  1, 12,  1,  7,  1,  4,  1,  9,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  6,  4])\n"
     ]
    }
   ],
   "source": [
    "for batch, (X_txt, X_wav, X_temp, X_eda, \n",
    "                        label_emotion, label_emotion_ext, label_arousal, label_valence) in list(enumerate(test_dataloader))[:4]:\n",
    "    print(X_txt,X_wav,label_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = model_tf_mixer(X_txt.to(device), X_wav.to(device))\n",
    "for i in torch.argmax(probs, dim=1):\n",
    "    if decode_dict[int(i)] != 'neutral':\n",
    "        print(decode_dict[int(i)])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bdc1fd12ca460d5768d71e9df3d9063ef832ce64a62e55a1a523c8c99752868e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
