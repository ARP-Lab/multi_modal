{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download a Original Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* if has not dataset, you must do unlock comment and excute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!./init"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import csv\n",
    "import shutil"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set global variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = \"KEMDy20\"\n",
    "global_data_path = f\"{dir_name}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directories(\n",
    "    dir_path: str\n",
    ") -> List[str]:\n",
    "    t_list = []\n",
    "    \n",
    "    if os.path.exists(dir_path):\n",
    "        t_list = sorted(os.listdir(dir_path))\n",
    "    \n",
    "    return t_list\n",
    "\n",
    "def get_files(\n",
    "    dir_path: str,\n",
    "    extension: str\n",
    ") -> List[str]:\n",
    "    \n",
    "    f_list = []\n",
    "    \n",
    "    for l in get_directories(dir_path):\n",
    "        if l.endswith(f\".{extension}\"):\n",
    "            f_list.append(l)\n",
    "    \n",
    "    return sorted(f_list)\n",
    "\n",
    "def read_csv_info(\n",
    "    file_path: str=\"\"\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "    csv_list = []\n",
    "    with open(file_path) as f:\n",
    "        r = csv.reader(f)\n",
    "\n",
    "        for l in r:\n",
    "            csv_list.append(l)\n",
    "\n",
    "    return pd.DataFrame(csv_list)\n",
    "\n",
    "def make_merged_data(\n",
    "    target_dir_list: List[str],\n",
    "    file_list: List[str],\n",
    "    session_name: str\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    assert session_name != None\n",
    "    \n",
    "    result: pd.DataFrame = None\n",
    "        \n",
    "    for file in file_list:\n",
    "        file_datas = {}\n",
    "        \n",
    "        for target_dir_full in target_dir_list:\n",
    "            target_dir = target_dir_full.split(\"/\")\n",
    "            \n",
    "            # print(f\"{session_name}, {file}, {target_dir[1]}\")\n",
    "            # Directory path form is as like \"KEMDy20/EDA/Session01/Sess01_script01_User001F.csv\"\n",
    "            file_datas[target_dir[1]] = read_csv_info(f\"./{target_dir[0]}/{target_dir[1]}/{session_name}/{file}\")\n",
    "            # print(file_datas[target_dir[1]].to_string())\n",
    "            \n",
    "            if not file_datas[target_dir[1]].empty:\n",
    "                if target_dir[1] == \"EDA\" or target_dir[1] == \"TEMP\":\n",
    "                    file_datas[target_dir[1]] = file_datas[target_dir[1]].drop(1) \n",
    "                file_datas[target_dir[1]] = file_datas[target_dir[1]].drop(0)\n",
    "                \n",
    "                if target_dir[1] == \"IBI\":\n",
    "                    file_datas[target_dir[1]] = file_datas[target_dir[1]].drop(columns=file_datas[target_dir[1]].columns[0])\n",
    "                    # reset a column index \n",
    "                    file_datas[target_dir[1]] = file_datas[target_dir[1]].T.reset_index(drop=True).T\n",
    "                    \n",
    "                # drop the rows if has None\n",
    "                file_datas[target_dir[1]] = file_datas[target_dir[1]].dropna()\n",
    "                \n",
    "                file_datas[target_dir[1]] = file_datas[target_dir[1]].reset_index(drop=True)\n",
    "            else:\n",
    "                if target_dir[1] == \"IBI\":\n",
    "                    file_datas[target_dir[1]] = pd.DataFrame({\"ibi\": [None], \"timestamp\": [None], \"sid\": [None]})\n",
    "        \n",
    "        # ** for check data\n",
    "        # for x in target_dir_list:\n",
    "        #     t = x.split(\"/\")\n",
    "        #     print(f\"{t[1]} : \")\n",
    "        #     print(file_datas[t[1]].to_string())\n",
    "        \n",
    "        # attatching column names\n",
    "        if len(file_datas[\"EDA\"].columns) < 3:\n",
    "            file_datas[\"EDA\"] = file_datas[\"EDA\"].set_axis([\"acc\", \"timestamp\"], axis=1)\n",
    "            file_datas[\"EDA\"][\"sid\"] = np.nan\n",
    "        else:\n",
    "            file_datas[\"EDA\"] = file_datas[\"EDA\"].set_axis([\"acc\", \"timestamp\", \"sid\"], axis=1)\n",
    "        \n",
    "        if len(file_datas[\"IBI\"].columns) < 3:\n",
    "            file_datas[\"IBI\"] = file_datas[\"IBI\"].set_axis([\"ibi\", \"timestamp\"], axis=1)\n",
    "            file_datas[\"IBI\"][\"sid\"] = np.nan\n",
    "        else:\n",
    "            file_datas[\"IBI\"] = file_datas[\"IBI\"].set_axis([\"ibi\", \"timestamp\", \"sid\"], axis=1)\n",
    "        \n",
    "        if len(file_datas[\"TEMP\"].columns) < 3:\n",
    "            file_datas[\"TEMP\"] = file_datas[\"TEMP\"].set_axis([\"temp\", \"timestamp\"], axis=1)\n",
    "            file_datas[\"TEMP\"][\"sid\"] = np.nan\n",
    "        else:\n",
    "            file_datas[\"TEMP\"] = file_datas[\"TEMP\"].set_axis([\"temp\", \"timestamp\", \"sid\"], axis=1)\n",
    "\n",
    "        # Merge \"TEMP\" table to \"EDA\" \n",
    "        one_part_data_on_session = pd.merge(\n",
    "            file_datas[\"EDA\"], file_datas[\"TEMP\"],\n",
    "            left_on='timestamp', right_on='timestamp', how='outer')\n",
    "        del one_part_data_on_session[\"sid_x\"]\n",
    "        one_part_data_on_session = one_part_data_on_session.rename(columns={\"sid_y\": \"sid\"})\n",
    "\n",
    "        # Merge \"IBI\" table to merged table(\"EDA\" and \"TEMP\")\n",
    "        one_part_data_on_session = pd.merge(\n",
    "            one_part_data_on_session, file_datas[\"IBI\"],\n",
    "            left_on='timestamp', right_on='timestamp', how='outer')\n",
    "        one_part_data_on_session[\"sid_x\"] = one_part_data_on_session[\"sid_x\"].fillna(one_part_data_on_session[\"sid_y\"])\n",
    "        del one_part_data_on_session[\"sid_y\"]\n",
    "        one_part_data_on_session = one_part_data_on_session.rename(columns={\"sid_x\": \"sid\"})\n",
    "\n",
    "        # reorder following as \"timestamp\", \"sid\", \"acc\", \"temp\", \"ibi\"\n",
    "        one_part_data_on_session = one_part_data_on_session[[\"timestamp\", \"sid\", \"acc\", \"temp\", \"ibi\"]]\n",
    "        # sorting values from \"timestamp\" column\n",
    "        one_part_data_on_session = one_part_data_on_session.sort_values(\"timestamp\")\n",
    "        one_part_data_on_session = one_part_data_on_session.reset_index(drop=True)\n",
    "        \n",
    "        if result is None:\n",
    "            result = one_part_data_on_session\n",
    "        else:\n",
    "            result = pd.concat([result, one_part_data_on_session], sort=True)\n",
    "            # result = result.append(one_part_data_on_session, ignore_index=True)\n",
    "            \n",
    "    result = result[[\"timestamp\", \"sid\", \"acc\", \"temp\", \"ibi\"]]\n",
    "    result = result.sort_values(\"timestamp\")\n",
    "    result = result.reset_index(drop=True)\n",
    "    \n",
    "    # print(result.to_string())\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get organized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir_list = [\"EDA\", \"IBI\", \"TEMP\"]\n",
    "\n",
    "all_file_list = {}\n",
    "all_session_list = []\n",
    "\n",
    "for part_name in target_dir_list:\n",
    "    t_part_path = f\"{global_data_path}/{part_name}\"\n",
    "    t_session_dir_list = get_directories(f\"./{t_part_path}\")\n",
    "    \n",
    "    for session in t_session_dir_list:\n",
    "        t_session_dir_path = f\"{t_part_path}/{session}\"\n",
    "        t_file_list = get_files(f\"./{t_session_dir_path}\", \"csv\")\n",
    "        \n",
    "        if session not in all_session_list:\n",
    "            all_session_list.append(session)\n",
    "            \n",
    "        file_list_in_session = []\n",
    "        \n",
    "        for file in t_file_list:\n",
    "            if file not in file_list_in_session:\n",
    "                file_list_in_session.append(file)\n",
    "        \n",
    "        if session not in all_file_list:\n",
    "            all_file_list[session] = file_list_in_session\n",
    "            \n",
    "            \n",
    "org_dataset_path = f\"org_{global_data_path}\"\n",
    "\n",
    "for session in all_session_list:\n",
    "    l = [global_data_path + \"/\" + x for x in target_dir_list]\n",
    "    \n",
    "    session_merged_data = make_merged_data(l, all_file_list[session], session)\n",
    "    \n",
    "    t_ori_path = os.path.join(os.getcwd(), org_dataset_path)\n",
    "    \n",
    "    t_dir_list = get_directories(os.getcwd())\n",
    "    if f\"{org_dataset_path}\" not in t_dir_list:\n",
    "        os.mkdir(f\"{t_ori_path}\")\n",
    "    \n",
    "    t_dir_list = get_directories(f\"{os.getcwd()}/{org_dataset_path}\")\n",
    "    if f\"{session}\" not in t_dir_list:\n",
    "        os.mkdir(f\"{t_ori_path}/{session}\")\n",
    "    \n",
    "    t_ori_wav_path = f\"./{global_data_path}/wav\"\n",
    "    \n",
    "    t_file_list = get_files(f\"{t_ori_wav_path}/{session}\", \"wav\")\n",
    "    t_file_list += get_files(f\"{t_ori_wav_path}/{session}\", \"txt\")\n",
    "\n",
    "    for file in t_file_list:\n",
    "        shutil.move(f\"{t_ori_wav_path}/{session}/{file}\", f\"{t_ori_path}/{session}\")\n",
    "        \n",
    "    session_merged_data.to_csv(f\"{t_ori_path}/{session}/{session}.csv\", sep=\",\", na_rep=\"NaN\")\n",
    "    \n",
    "src_ann_path = os.path.join(os.getcwd(), f\"{global_data_path}/annotation\")\n",
    "dest_ann_path = os.path.join(os.getcwd(), f\"{org_dataset_path}/annotation\")\n",
    "\n",
    "shutil.copytree(src_ann_path, dest_ann_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
