{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [참고 data2vec audio:사용 안하기로함]: https://towhee.io/audio-embedding/data2vec\n",
    "- [참고 data2vec text]: https://huggingface.co/docs/datasets/nlp_load\n",
    "- [참고 data2vec load audio*]:https://huggingface.co/docs/datasets/audio_load\n",
    "- [참고 data2vec embedding]: https://huggingface.co/docs/transformers/model_doc/data2vec#transformers.Data2VecAudioModel\n",
    "- [참고 handling pytorch dataset]:https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel\n",
    "- [참고 add data to dataset]: https://pypi.org/project/datasets/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings(Text, Audio) Session 1 \n",
    "- 각세션으로 확대시켜야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arplab/project/lou/multi_modal/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, Data2VecAudioModel\n",
    "from datasets import load_dataset, Dataset, Audio, Features\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Sess01_script01_User002M_001\n",
       "1      Sess01_script01_User002M_002\n",
       "2      Sess01_script01_User002M_003\n",
       "3      Sess01_script01_User002M_004\n",
       "4      Sess01_script01_User001F_001\n",
       "                   ...             \n",
       "306    Sess01_script06_User001F_016\n",
       "307    Sess01_script06_User002M_041\n",
       "308    Sess01_script06_User002M_042\n",
       "309    Sess01_script06_User002M_043\n",
       "310    Sess01_script06_User001F_017\n",
       "Name:  .1, Length: 311, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = 'Sess01_eval.csv'\n",
    "df_annotation = pd.read_csv('org_KEMDy20/annotation/' + file_name, skiprows=1)\n",
    "# df_annotation['Emotion'], df_annotation['Valence'], df_annotation['Arousal']\n",
    "file_lst = df_annotation[' .1']\n",
    "file_lst"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./org_KEMDy20/Session01/Sess01_script01_User002M_001.wav',\n",
       " './org_KEMDy20/Session01/Sess01_script01_User002M_002.wav',\n",
       " './org_KEMDy20/Session01/Sess01_script01_User002M_003.wav']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav_file_lst = []\n",
    "for i in file_lst:\n",
    "    wav_file_lst.append('./org_KEMDy20/Session01/' + i + '.wav')\n",
    "wav_file_lst[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_names(file_lst):\n",
    "    file_name_lst = []\n",
    "    for i in file_lst:\n",
    "        i = i.split('/')[-1]\n",
    "        i = i.split('.')[-2]\n",
    "        file_name_lst.append(i)\n",
    "    return file_name_lst\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': './org_KEMDy20/Session01/Sess01_script01_User002M_001.wav',\n",
       " 'array': array([ 0.0526123 ,  0.05267334,  0.05197144, ..., -0.00387573,\n",
       "        -0.00244141, -0.00137329], dtype=float32),\n",
       " 'sampling_rate': 16000}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "dataset = Dataset.from_dict({\"wav\": wav_file_lst }).cast_column(\"wav\", Audio())\n",
    "dataset[0][\"wav\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/data2vec-audio-base-960h were not used when initializing Data2VecAudioModel: ['lm_head.bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing Data2VecAudioModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Data2VecAudioModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at facebook/data2vec-audio-base-960h were not used when initializing Data2VecAudioModel: ['lm_head.bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing Data2VecAudioModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Data2VecAudioModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "check_point_wav_model = 'facebook/data2vec-audio-base-960h'\n",
    "processor = AutoProcessor.from_pretrained(check_point_wav_model)\n",
    "model_wav_cuda = Data2VecAudioModel.from_pretrained(check_point_wav_model).cuda()\n",
    "model_wav = Data2VecAudioModel.from_pretrained(check_point_wav_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0526123 ,  0.05267334,  0.05197144, ..., -0.00387573,\n",
       "       -0.00244141, -0.00137329], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['wav']['array']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_rate = dataset['wav'][0]['sampling_rate']\n",
    "sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-29 18:24:41.684467: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-29 18:24:42.891267: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11/include:/usr/local/cuda-11/lib64::/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-03-29 18:24:42.891382: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11/include:/usr/local/cuda-11/lib64::/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-03-29 18:24:42.891394: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 410, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    input = processor(dataset[i]['wav']['array'], sampling_rate = sampling_rate, return_tensors = 'pt', padding = True, max_length = 50)\n",
    "    with torch.no_grad():\n",
    "        output = model_wav(**input)\n",
    "output['last_hidden_state'].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### audio embedding method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding: https://huggingface.co/docs/transformers/pad_truncation\n",
    "def wav_embeddings_func(wav_file_lst, processor, model):\n",
    "    dataset = Dataset.from_dict({\"wav\": wav_file_lst }).cast_column(\"wav\", Audio())\n",
    "    wav_embeddings = []\n",
    "    for i in range(len(dataset)): \n",
    "        inputs = processor(dataset[i]['wav']['array'], sampling_rate = sampling_rate, return_tensors = 'pt', padding = 'max_length', max_length= 16000, truncation = True)\n",
    "        with torch.no_grad():\n",
    "            output = model_wav(**inputs)\n",
    "        wav_embeddings.append(output['last_hidden_state'])\n",
    "    return wav_embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Embedding\n",
    "- 참고: https://huggingface.co/docs/datasets/nlp_load\n",
    "- 참고: https://huggingface.co/docs/transformers/v4.27.2/en/model_doc/data2vec#transformers.Data2VecTextModel\n",
    "- 참고 한국어 pretrained model: https://huggingface.co/Junmai/KR-Data2VecText-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast, BartModel\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import pandas as pd\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./org_KEMDy20/Session01/Sess01_script01_User002M_001.txt',\n",
       " './org_KEMDy20/Session01/Sess01_script01_User002M_002.txt',\n",
       " './org_KEMDy20/Session01/Sess01_script01_User002M_003.txt']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text data loading\n",
    "txt_file_lst = []\n",
    "for i in file_lst:\n",
    "    txt_file_lst.append('./org_KEMDy20/Session01/' + i + '.txt')\n",
    "txt_file_lst[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 참고: https://huggingface.co/gogamza/kobart-base-v2\n",
    "# 참고: https://github.com/BM-K/Sentence-Embedding-Is-All-You-Need\n",
    "\n",
    "# model = AutoModel.from_pretrained('gogamza/kobart-base-v2')  # or 'BM-K/KoSimCSE-bert-multitask'\n",
    "# tokenizer = AutoTokenizer.from_pretrained('gogamza/kobart-base-v2')  # or 'BM-K/KoSimCSE-bert-multitask'\n",
    "checkpoint = 'BM-K/KoSimCSE-bert-multitask'\n",
    "model_txt = AutoModel.from_pretrained(checkpoint)  # or 'BM-K/KoSimCSE-bert-multitask'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)  # or 'BM-K/KoSimCSE-bert-multitask'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text embedding method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_embeddings_func(txt_file_lst, tokenizer, model):\n",
    "    sentences = []\n",
    "\n",
    "    for i in txt_file_lst:    \n",
    "        f = open(i, 'r')\n",
    "        line = f.readline()\n",
    "        line = re.sub('\\n', '', line)\n",
    "        line = re.sub('  ', ' ', line)\n",
    "        line = line.rstrip().lstrip()\n",
    "        sentences.append(line)\n",
    "        f.close()\n",
    "    start_index = np.random.randint(1, high = len(txt_file_lst)) - 10\n",
    "    \n",
    "    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    txt_embeddings, _ = model_txt(**inputs, return_dict=False)\n",
    "    return txt_embeddings\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터셋 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # wav file embedding\n",
    "# wav_embeddings = wav_embeddings_func(wav_file_lst, processor, model_wav)\n",
    "# wav_embeddings[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # txt file embedding\n",
    "# txt_embeddings = text_embeddings_func(txt_file_lst,tokenizer, model_txt)\n",
    "# txt_embeddings[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name_lst = get_file_names(wav_file_lst)\n",
    "# file_name_lst[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # annotation data 가져오기\n",
    "# df_annotation = pd.read_csv('org_KEMDy20/annotation/' + file_name, skiprows=1)\n",
    "# df_annotation.Arousal[:3], df_annotation.Emotion[:3], df_annotation.Valence[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(txt_embeddings), len(wav_embeddings), len(file_name_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(file_name_lst, \n",
    "                  text_embeddings, \n",
    "                  wav_embeddings, \n",
    "                  label_emotion,\n",
    "                  label_arousal,\n",
    "                  label_valence, \n",
    "                  session_name = 'Session01'):\n",
    "    dataset = {'file_names': file_name_lst, \n",
    "         'text_embeddings': text_embeddings, \n",
    "         'wav_embeddings':wav_embeddings,\n",
    "         'Emotion':label_emotion,\n",
    "         'Arousal':label_arousal,\n",
    "         'Valence':label_valence}\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Session36', 'Session37', 'Session38', 'Session39', 'Session40'],\n",
       " ['Sess36_eval.csv',\n",
       "  'Sess37_eval.csv',\n",
       "  'Sess38_eval.csv',\n",
       "  'Sess39_eval.csv',\n",
       "  'Sess40_eval.csv'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session_num_lst = []\n",
    "annot_num_lst = []\n",
    "for i in range(40):\n",
    "    if i <= 8:\n",
    "        session_num_lst.append('Session0' + str(i+1))\n",
    "        annot_num_lst.append('Sess0' + str(i+1) + '_eval.csv')\n",
    "    else:\n",
    "        session_num_lst.append('Session' + str(i+1))\n",
    "        annot_num_lst.append('Sess' + str(i+1) + '_eval.csv')\n",
    "session_num_lst[-5:], annot_num_lst[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session01번째 작업이 완료 되었습니다.\n",
      "Session02번째 작업이 완료 되었습니다.\n",
      "Session03번째 작업이 완료 되었습니다.\n"
     ]
    }
   ],
   "source": [
    "dataset = {}\n",
    "for annot, session in zip(annot_num_lst[:3], session_num_lst[:3]):\n",
    "    \n",
    "    df_annotation = pd.read_csv('./org_KEMDy20/annotation/' + annot, skiprows=1)\n",
    "    file_name_lst = df_annotation[' .1']\n",
    "    \n",
    "    txt_file_lst = []\n",
    "    wav_file_lst = []\n",
    "    for file_name in file_name_lst:\n",
    "        txt_file_lst.append('./org_KEMDy20/' + session + '/' + file_name + '.txt')\n",
    "        wav_file_lst.append('./org_KEMDy20/' + session + '/' + file_name + '.wav')\n",
    "    \n",
    "    txt_embeddings = text_embeddings_func(txt_file_lst, tokenizer, model_txt)\n",
    "    wav_embeddings = wav_embeddings_func(wav_file_lst, processor, model_wav_cuda)\n",
    "    \n",
    "    dataset[session] = build_dataset(file_name_lst, \n",
    "                txt_embeddings, \n",
    "                wav_embeddings,  \n",
    "                df_annotation.Emotion, \n",
    "                df_annotation.Arousal, \n",
    "                df_annotation.Valence)\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    print(f'{session}번째 작업이 완료 되었습니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Session01', 'Session02', 'Session03'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311\n",
      "311\n",
      "311\n",
      "311\n",
      "311\n",
      "311\n"
     ]
    }
   ],
   "source": [
    "# length test\n",
    "for i in dataset['Session01'].keys():\n",
    "    print(len(dataset['Session01'][i]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save data set\n",
    "with open('./model/data/lou_dataset_1_3.pkl', 'wb')as f:\n",
    "    pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['file_names', 'text_embeddings', 'wav_embeddings', 'Emotion', 'Arousal', 'Valence'])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./model/data/lou_dataset.pkl', 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "dataset['Session01'].keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bdc1fd12ca460d5768d71e9df3d9063ef832ce64a62e55a1a523c8c99752868e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
