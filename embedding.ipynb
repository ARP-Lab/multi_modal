{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [참고 data2vec audio:사용 안하기로함]: https://towhee.io/audio-embedding/data2vec\n",
    "- [참고 data2vec text]: https://huggingface.co/docs/datasets/nlp_load\n",
    "- [참고 data2vec load audio*]:https://huggingface.co/docs/datasets/audio_load\n",
    "- [참고 data2vec embedding]: https://huggingface.co/docs/transformers/model_doc/data2vec#transformers.Data2VecAudioModel\n",
    "- [참고 handling pytorch dataset]:https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel\n",
    "- [참고 add data to dataset]: https://pypi.org/project/datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arplab/project/lou/multi_modal/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at facebook/data2vec-audio-base-960h were not used when initializing Data2VecAudioModel: ['lm_head.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing Data2VecAudioModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Data2VecAudioModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Wav2Vec2Processor, Data2VecAudioModel, AutoFeatureExtractor\n",
    "from datasets import load_dataset, Dataset, Audio, Features\n",
    "from scipy.io import wavfile # to be added\n",
    "from glob import glob\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/data2vec-audio-base-960h\")\n",
    "model = Data2VecAudioModel.from_pretrained(\"facebook/data2vec-audio-base-960h\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from towhee import pipe, ops, DataCollection # to be added\n",
    "# p = (pipe.input('path')\n",
    "#         .map('path', 'frame', ops.audio_decode.ffmpeg(sample_rate=16000))\n",
    "#         .map('frame', 'vecs', ops.audio_embedding.data2vec(model_name='facebook/data2vec-audio-base-960h'))\n",
    "#         .output('path', 'vecs'))\n",
    "\n",
    "# DataCollection(p('./org_KEMDy20/Session01/Sess01_script01_User001F_001.wav')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./org_KEMDy20/Session01/Sess01_script03_User002M_010.wav',\n",
       " './org_KEMDy20/Session01/Sess01_script03_User001F_014.wav',\n",
       " './org_KEMDy20/Session01/Sess01_script05_User002M_001.wav']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav_file_lst = glob('./org_KEMDy20/Session01/*.wav')\n",
    "wav_file_lst[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': './org_KEMDy20/Session01/Sess01_script03_User002M_010.wav',\n",
       " 'array': array([-0.00018311,  0.00057983,  0.00076294, ...,  0.00259399,\n",
       "         0.00115967,  0.00036621], dtype=float32),\n",
       " 'sampling_rate': 16000}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "dataset = Dataset.from_dict({\"wav\": wav_file_lst }).cast_column(\"wav\", Audio())\n",
    "dataset[0][\"wav\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dataset에 데이터 더하는 방법 찾아보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['wav'],\n",
       "    num_rows: 311\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/data2vec-audio-base-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 20:34:06.452296: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-26 20:34:07.664057: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11/include:/usr/local/cuda-11/lib64::/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-03-26 20:34:07.664177: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11/include:/usr/local/cuda-11/lib64::/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-03-26 20:34:07.664188: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([-0.00228672,  0.01021109,  0.01321056, ...,  0.04320529,\n",
       "         0.01970942,  0.0067117 ], dtype=float32),\n",
       " array([-0.13987999, -0.1284736 , -0.1221367 , ..., -0.01567697,\n",
       "        -0.01567697, -0.02454861], dtype=float32),\n",
       " array([ 0.6008278 ,  0.6016405 ,  0.61383116, ..., -0.02495759,\n",
       "        -0.02983384, -0.01926864], dtype=float32)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# audio file is decoded on the fly\n",
    "input_values = []\n",
    "for i in range(len(dataset)):\n",
    "    input = feature_extractor(dataset[i][\"wav\"][\"array\"], sampling_rate=dataset[0]['wav']['sampling_rate'], return_attention_mask=False)\n",
    "    input_values.append(input['input_values'][0])\n",
    "    \n",
    "\n",
    "input_values_feature = Features({'embedded_wav': input_values})\n",
    "input_values_feature['embedded_wav'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['wav', 'embedded_wav'],\n",
       "    num_rows: 311\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_new = dataset.add_column(name = \"embedded_wav\", column = input_values_feature['embedded_wav'])\n",
    "dataset_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "311"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_new['embedded_wav'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Embedding\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 참고: https://huggingface.co/docs/datasets/nlp_load\n",
    "- 참고: https://huggingface.co/docs/transformers/v4.27.2/en/model_doc/data2vec#transformers.Data2VecTextModel\n",
    "- 참고 한국어 pretrained model: https://huggingface.co/Junmai/KR-Data2VecText-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast, BartModel\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./org_KEMDy20/Session01/Sess01_script06_User002M_009.txt',\n",
       " './org_KEMDy20/Session01/Sess01_script01_User001F_007.txt',\n",
       " './org_KEMDy20/Session01/Sess01_script01_User001F_001.txt']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text data loading\n",
    "txt_file_lst = glob('./org_KEMDy20/Session01/*.txt')\n",
    "txt_file_lst[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 참고: https://huggingface.co/gogamza/kobart-base-v2\n",
    "# 참고: https://github.com/BM-K/Sentence-Embedding-Is-All-You-Need\n",
    "\n",
    "# model = AutoModel.from_pretrained('gogamza/kobart-base-v2')  # or 'BM-K/KoSimCSE-bert-multitask'\n",
    "# tokenizer = AutoTokenizer.from_pretrained('gogamza/kobart-base-v2')  # or 'BM-K/KoSimCSE-bert-multitask'\n",
    "checkpoint = 'BM-K/KoSimCSE-bert-multitask'\n",
    "model = AutoModel.from_pretrained(checkpoint)  # or 'BM-K/KoSimCSE-bert-multitask'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)  # or 'BM-K/KoSimCSE-bert-multitask'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['그것 그것 땜에 당 그거 땜에 당황스러웠던 적은 없었어?',\n",
       " '아니면 어 어.',\n",
       " '그니까 그런 경험이 음 솔직히 거의 없는 거 같은데 약간 명절에 시골에 내려 가면은 약간 큰 아빠?',\n",
       " '그 가영이는 뭐 주변에서 막 이렇게 보고 가영이가 그거 했던 적은 웃겼던 건 적은 없었어?',\n",
       " '근데 내가 만약에 열두시에 연락을 해서 어 저 한시간 정도 더 놀다 갈게요 했는데 그 한시까지 안오잖아?',\n",
       " '이제 그거에 대해서는 어린 나이에도 대표를 다시는 여성분들도 계시는데 굳이 그거를 비율을 맞춰야 되느냐에 대해서 조금 그런 거 같애.',\n",
       " '근데 그거는 굳이 약간 남녀 그런 문제가 아니라 누가 도우려고 하든 간에 내가 도움을 받는 걸 싫어해서 그런 거야.',\n",
       " '어 좀 비슷하게 라고는 말하기 좀 그런데 약간 버스를 타다가 넘어 그니까 되게 급 정차를 해 하는 바람에 버스가 그래서 막.',\n",
       " '내가 혼잣말하거나 전화를 막 하고 있는데 집에서 자기 혼자 네 부르셨나요 막 이러거나 되게 띠링 소리 나다가 자기 혼자 갑자기 노래를 트는 경우가 있어.',\n",
       " '글쎄 일단은 나는 그 말이 살짝 좀 이해가 안 됐어, 약간.',\n",
       " '같은 아파트 내에 있는 어른들이나 그런 사람들한텐 다 인사하고 다녔어.',\n",
       " '말도 말도 안 했는데 그냥 막 떠.',\n",
       " '혹시 여기 무슨 검사 받았 적 있었냐고 나한테 검사받았던 환자시냐고 묻는 거야.',\n",
       " '그 꼰대에 특징을 갖고 있다거나 막 그런 거?',\n",
       " '그런 면에서 되게 아버지가 뭐라 하시거나?',\n",
       " '한 여섯시까지는 들어가야 된다라는 게 있었는데 그거 그 규칙이 불문율이라고 했어야 되나 근데 그거를 오빠가 깨버렸거든.',\n",
       " '어 최근에 어디를 가면?',\n",
       " '물론 같이 공부를 했을 때 똑같은 사람들도 있겠지만.',\n",
       " '우리 집은 되게 자유로워 가지고 솔직히 아빠가 이게 어때라고 하면 본인 생 그니까 가족들 생각이랑 안 맞으면 그냥 자기 주관대로 가거든, 다들.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "sentences = []\n",
    "\n",
    "for i in txt_file_lst:    \n",
    "    f = open(i, 'r')\n",
    "    line = f.readline()\n",
    "    line = re.sub('l/\\n|c/|b/|n/|l/|u/', '', line)\n",
    "    line = re.sub('  ', ' ', line)\n",
    "    line = line.rstrip().lstrip()\n",
    "    sentences.append(line)\n",
    "    f.close()\n",
    "    \n",
    "    \n",
    "sentences[21:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "embeddings, _ = model(**inputs, return_dict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.3365e-01, -1.0966e+00,  6.5578e-01,  ...,  6.1874e-01,\n",
       "           1.2647e-01,  5.4168e-01],\n",
       "         [-1.2150e+00, -2.3270e-01,  5.2826e-01,  ..., -1.0512e-02,\n",
       "           8.8822e-01, -3.0252e-01],\n",
       "         [-2.2442e-01, -7.0670e-01,  9.5867e-01,  ...,  4.1815e-01,\n",
       "           4.2847e-01, -1.8841e-01],\n",
       "         ...,\n",
       "         [-2.0204e-01, -2.0759e-01,  7.5531e-01,  ...,  1.4437e-01,\n",
       "           1.1744e+00,  3.6902e-01],\n",
       "         [-2.5445e-01, -9.7568e-01,  5.1529e-01,  ...,  2.7664e-01,\n",
       "           7.4539e-01,  1.7495e-01],\n",
       "         [-1.0302e-01, -9.4838e-01,  3.5005e-01,  ...,  6.1108e-01,\n",
       "           1.0481e+00,  2.5768e-01]],\n",
       "\n",
       "        [[ 5.5688e-01, -5.6695e-01, -2.1437e-01,  ...,  1.6343e-01,\n",
       "          -3.6775e-01,  3.1689e+00],\n",
       "         [ 8.2378e-01, -1.3360e+00,  2.6778e-01,  ..., -3.1469e-01,\n",
       "          -4.5207e-01,  2.9279e+00],\n",
       "         [ 5.7025e-01, -5.9719e-01,  2.4914e-02,  ..., -4.1381e-01,\n",
       "          -4.7057e-01,  2.3544e+00],\n",
       "         ...,\n",
       "         [ 8.7290e-01, -1.6862e-01, -3.3052e-01,  ...,  1.9478e-01,\n",
       "          -4.0738e-01,  2.5339e+00],\n",
       "         [ 7.9770e-01, -2.9235e-01, -1.2395e-01,  ...,  7.8150e-02,\n",
       "          -4.0797e-01,  2.6303e+00],\n",
       "         [ 8.9939e-01, -2.2725e-01, -7.3478e-02,  ...,  1.9869e-03,\n",
       "          -3.0364e-01,  2.6188e+00]],\n",
       "\n",
       "        [[-8.5458e-01,  4.4587e-01,  1.1015e-01,  ..., -1.7020e-01,\n",
       "          -1.6229e-01,  1.1504e+00],\n",
       "         [-1.1264e+00, -3.5030e-01,  1.5518e-01,  ..., -4.2908e-01,\n",
       "           3.4483e-01,  5.2793e-02],\n",
       "         [-1.1061e+00,  8.7214e-01,  1.8514e-01,  ..., -3.0358e-01,\n",
       "           5.2065e-01,  6.8099e-01],\n",
       "         ...,\n",
       "         [-6.2189e-01,  1.0786e-01, -1.5025e-01,  ..., -2.1711e-02,\n",
       "           7.0260e-01,  1.0898e+00],\n",
       "         [-8.5152e-01, -7.9452e-02, -3.0936e-02,  ..., -2.5022e-01,\n",
       "           3.7069e-01,  5.0641e-01],\n",
       "         [-7.7095e-01,  1.7487e-01,  1.4307e-01,  ..., -3.0851e-01,\n",
       "           2.8123e-01,  5.7369e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-3.1900e-01, -7.1338e-01,  5.8788e-02,  ...,  8.8240e-01,\n",
       "           1.7354e-01,  6.2751e-01],\n",
       "         [-6.6767e-01,  2.0806e-01,  5.3561e-01,  ...,  5.9563e-01,\n",
       "           6.2825e-01,  7.9506e-01],\n",
       "         [-1.4016e-01, -1.6990e-01, -1.5937e-01,  ...,  4.1411e-02,\n",
       "          -6.3166e-02,  2.0637e-01],\n",
       "         ...,\n",
       "         [-5.6035e-02, -7.7154e-01,  3.3506e-01,  ...,  4.6209e-01,\n",
       "           1.4240e-01,  4.8999e-01],\n",
       "         [-3.9503e-01, -5.0755e-01,  4.6022e-01,  ...,  4.0869e-01,\n",
       "           1.4483e-02,  4.5455e-01],\n",
       "         [-1.2190e-01, -9.0727e-01,  1.2820e-01,  ...,  1.8746e-01,\n",
       "           3.4818e-01,  5.0122e-01]],\n",
       "\n",
       "        [[ 6.2557e-01, -1.6339e+00,  9.5222e-01,  ..., -5.9204e-01,\n",
       "           1.5353e+00,  1.8605e-01],\n",
       "         [-5.8873e-01, -5.0636e-01,  1.5908e-01,  ..., -3.3334e-01,\n",
       "           1.9059e+00,  3.8707e-01],\n",
       "         [ 7.1563e-01, -1.0858e+00,  6.4779e-01,  ..., -6.3089e-01,\n",
       "           4.1712e-01,  4.2900e-01],\n",
       "         ...,\n",
       "         [ 7.3785e-01, -1.4638e+00,  9.0225e-01,  ..., -5.0771e-01,\n",
       "           1.3658e+00,  8.6494e-03],\n",
       "         [ 1.0243e+00, -1.6652e+00,  6.0050e-01,  ..., -8.0530e-01,\n",
       "           1.6427e+00, -6.2832e-02],\n",
       "         [ 1.0368e+00, -1.7326e+00,  4.6587e-01,  ..., -8.1483e-01,\n",
       "           1.2980e+00, -2.4393e-01]],\n",
       "\n",
       "        [[ 1.2040e+00, -6.6251e-01,  7.9214e-01,  ...,  3.9749e-01,\n",
       "          -2.7562e-03,  4.8779e-01],\n",
       "         [ 4.0422e-01, -1.3411e-01,  5.9859e-01,  ...,  8.8835e-01,\n",
       "           6.5665e-01,  1.3181e-01],\n",
       "         [ 3.3182e-01,  3.2515e-03,  4.0354e-01,  ...,  9.5147e-01,\n",
       "           3.2886e-01,  6.3368e-01],\n",
       "         ...,\n",
       "         [ 1.4801e+00, -2.9981e-01,  6.5220e-01,  ...,  1.9490e-01,\n",
       "           4.3690e-01,  2.7688e-01],\n",
       "         [ 1.3201e+00, -4.4769e-01,  8.4931e-01,  ...,  1.1683e-01,\n",
       "           2.6495e-01,  3.7254e-01],\n",
       "         [ 1.1932e+00, -5.7769e-01,  8.1005e-01,  ...,  1.0221e-01,\n",
       "           2.0848e-01,  2.5364e-01]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['wav', 'embedded_wav'],\n",
       "    num_rows: 311\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(embeddings))\n",
    "dataset_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['wav', 'embedded_wav'],\n",
       "    num_rows: 311\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowInvalid",
     "evalue": "Could not convert tensor([[-0.2337, -1.0966,  0.6558,  ...,  0.6187,  0.1265,  0.5417],\n        [-1.2150, -0.2327,  0.5283,  ..., -0.0105,  0.8882, -0.3025],\n        [-0.2244, -0.7067,  0.9587,  ...,  0.4182,  0.4285, -0.1884],\n        ...,\n        [-0.2020, -0.2076,  0.7553,  ...,  0.1444,  1.1744,  0.3690],\n        [-0.2544, -0.9757,  0.5153,  ...,  0.2766,  0.7454,  0.1749],\n        [-0.1030, -0.9484,  0.3500,  ...,  0.6111,  1.0481,  0.2577]],\n       grad_fn=<UnbindBackward0>) with type Tensor: did not recognize Python value type when inferring an Arrow data type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset_new \u001b[39m=\u001b[39m dataset_new\u001b[39m.\u001b[39;49madd_column(name \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39membedded_text\u001b[39;49m\u001b[39m\"\u001b[39;49m, column \u001b[39m=\u001b[39;49m embeddings)\n\u001b[1;32m      2\u001b[0m dataset_new\n",
      "File \u001b[0;32m~/project/lou/multi_modal/.venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:528\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    521\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    522\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    523\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    524\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    525\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    526\u001b[0m }\n\u001b[1;32m    527\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 528\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    529\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    530\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/project/lou/multi_modal/.venv/lib/python3.10/site-packages/datasets/fingerprint.py:511\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    507\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    509\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 511\u001b[0m out \u001b[39m=\u001b[39m func(dataset, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    513\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m~/project/lou/multi_modal/.venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:5342\u001b[0m, in \u001b[0;36mDataset.add_column\u001b[0;34m(self, name, column, new_fingerprint)\u001b[0m\n\u001b[1;32m   5313\u001b[0m \u001b[39m@transmit_format\u001b[39m\n\u001b[1;32m   5314\u001b[0m \u001b[39m@fingerprint_transform\u001b[39m(inplace\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   5315\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madd_column\u001b[39m(\u001b[39mself\u001b[39m, name: \u001b[39mstr\u001b[39m, column: Union[\u001b[39mlist\u001b[39m, np\u001b[39m.\u001b[39marray], new_fingerprint: \u001b[39mstr\u001b[39m):\n\u001b[1;32m   5316\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Add column to Dataset.\u001b[39;00m\n\u001b[1;32m   5317\u001b[0m \n\u001b[1;32m   5318\u001b[0m \u001b[39m    <Added version=\"1.7\"/>\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5340\u001b[0m \u001b[39m    ```\u001b[39;00m\n\u001b[1;32m   5341\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5342\u001b[0m     column_table \u001b[39m=\u001b[39m InMemoryTable\u001b[39m.\u001b[39;49mfrom_pydict({name: column})\n\u001b[1;32m   5343\u001b[0m     _check_column_names(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data\u001b[39m.\u001b[39mcolumn_names \u001b[39m+\u001b[39m column_table\u001b[39m.\u001b[39mcolumn_names)\n\u001b[1;32m   5344\u001b[0m     dataset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflatten_indices() \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_indices \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/project/lou/multi_modal/.venv/lib/python3.10/site-packages/datasets/table.py:801\u001b[0m, in \u001b[0;36mInMemoryTable.from_pydict\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    786\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pydict\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    787\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[39m    Construct a Table from Arrow arrays or columns.\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[39m        `datasets.table.Table`\u001b[39;00m\n\u001b[1;32m    800\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 801\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(pa\u001b[39m.\u001b[39;49mTable\u001b[39m.\u001b[39;49mfrom_pydict(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n",
      "File \u001b[0;32m~/project/lou/multi_modal/.venv/lib/python3.10/site-packages/pyarrow/table.pxi:3725\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_pydict\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/project/lou/multi_modal/.venv/lib/python3.10/site-packages/pyarrow/table.pxi:5254\u001b[0m, in \u001b[0;36mpyarrow.lib._from_pydict\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/project/lou/multi_modal/.venv/lib/python3.10/site-packages/pyarrow/array.pxi:350\u001b[0m, in \u001b[0;36mpyarrow.lib.asarray\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/project/lou/multi_modal/.venv/lib/python3.10/site-packages/pyarrow/array.pxi:320\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/project/lou/multi_modal/.venv/lib/python3.10/site-packages/pyarrow/array.pxi:39\u001b[0m, in \u001b[0;36mpyarrow.lib._sequence_to_array\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/project/lou/multi_modal/.venv/lib/python3.10/site-packages/pyarrow/error.pxi:144\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/project/lou/multi_modal/.venv/lib/python3.10/site-packages/pyarrow/error.pxi:100\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: Could not convert tensor([[-0.2337, -1.0966,  0.6558,  ...,  0.6187,  0.1265,  0.5417],\n        [-1.2150, -0.2327,  0.5283,  ..., -0.0105,  0.8882, -0.3025],\n        [-0.2244, -0.7067,  0.9587,  ...,  0.4182,  0.4285, -0.1884],\n        ...,\n        [-0.2020, -0.2076,  0.7553,  ...,  0.1444,  1.1744,  0.3690],\n        [-0.2544, -0.9757,  0.5153,  ...,  0.2766,  0.7454,  0.1749],\n        [-0.1030, -0.9484,  0.3500,  ...,  0.6111,  1.0481,  0.2577]],\n       grad_fn=<UnbindBackward0>) with type Tensor: did not recognize Python value type when inferring an Arrow data type"
     ]
    }
   ],
   "source": [
    "# dataset_new = dataset_new.add_column(name = \"embedded_text\", column = embeddings)\n",
    "# dataset_new"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bdc1fd12ca460d5768d71e9df3d9063ef832ce64a62e55a1a523c8c99752868e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
