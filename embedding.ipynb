{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [참고 data2vec audio:사용 안하기로함]: https://towhee.io/audio-embedding/data2vec\n",
    "- [참고 data2vec text]: https://huggingface.co/docs/datasets/nlp_load\n",
    "- [참고 data2vec load audio*]:https://huggingface.co/docs/datasets/audio_load\n",
    "- [참고 data2vec embedding]: https://huggingface.co/docs/transformers/model_doc/data2vec#transformers.Data2VecAudioModel\n",
    "- [참고 handling pytorch dataset]:https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel\n",
    "- [참고 add data to dataset]: https://pypi.org/project/datasets/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings(Text, Audio) Session 1 \n",
    "- 각세션으로 확대시켜야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, Data2VecAudioModel\n",
    "from datasets import load_dataset, Dataset, Audio, Features\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Sess01_script01_User002M_001\n",
       "1      Sess01_script01_User002M_002\n",
       "2      Sess01_script01_User002M_003\n",
       "3      Sess01_script01_User002M_004\n",
       "4      Sess01_script01_User001F_001\n",
       "                   ...             \n",
       "306    Sess01_script06_User001F_016\n",
       "307    Sess01_script06_User002M_041\n",
       "308    Sess01_script06_User002M_042\n",
       "309    Sess01_script06_User002M_043\n",
       "310    Sess01_script06_User001F_017\n",
       "Name:  .1, Length: 311, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = 'Sess01_eval.csv'\n",
    "df_annotation = pd.read_csv('org_KEMDy20/annotation/' + file_name, skiprows=1)\n",
    "# df_annotation['Emotion'], df_annotation['Valence'], df_annotation['Arousal']\n",
    "file_lst = df_annotation[' .1']\n",
    "file_lst"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./org_KEMDy20/Session01/Sess01_script01_User002M_001.wav',\n",
       " './org_KEMDy20/Session01/Sess01_script01_User002M_002.wav',\n",
       " './org_KEMDy20/Session01/Sess01_script01_User002M_003.wav']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav_file_lst = []\n",
    "for i in file_lst:\n",
    "    wav_file_lst.append('./org_KEMDy20/Session01/' + i + '.wav')\n",
    "wav_file_lst[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_names(file_lst):\n",
    "    file_name_lst = []\n",
    "    for i in file_lst:\n",
    "        i = i.split('/')[-1]\n",
    "        i = i.split('.')[-2]\n",
    "        file_name_lst.append(i)\n",
    "    return file_name_lst\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': './org_KEMDy20/Session01/Sess01_script01_User002M_001.wav',\n",
       " 'array': array([ 0.0526123 ,  0.05267334,  0.05197144, ..., -0.00387573,\n",
       "        -0.00244141, -0.00137329], dtype=float32),\n",
       " 'sampling_rate': 16000}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "dataset = Dataset.from_dict({\"wav\": wav_file_lst }).cast_column(\"wav\", Audio())\n",
    "dataset[0][\"wav\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/data2vec-audio-base-960h were not used when initializing Data2VecAudioModel: ['lm_head.bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing Data2VecAudioModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Data2VecAudioModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at facebook/data2vec-audio-base-960h were not used when initializing Data2VecAudioModel: ['lm_head.bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing Data2VecAudioModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Data2VecAudioModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "check_point_wav_model = 'facebook/data2vec-audio-base-960h'\n",
    "processor = AutoProcessor.from_pretrained(check_point_wav_model)\n",
    "model_wav_cuda = Data2VecAudioModel.from_pretrained(check_point_wav_model).cuda()\n",
    "model_wav = Data2VecAudioModel.from_pretrained(check_point_wav_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0526123 ,  0.05267334,  0.05197144, ..., -0.00387573,\n",
       "       -0.00244141, -0.00137329], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['wav']['array']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_rate = dataset['wav'][0]['sampling_rate']\n",
    "sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 410, 768])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    input = processor(dataset[i]['wav']['array'], sampling_rate = sampling_rate, return_tensors = 'pt', padding = True, max_length = 50)\n",
    "    with torch.no_grad():\n",
    "        output = model_wav(**input)\n",
    "output['last_hidden_state'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wav_embeddings_func(wav_file_lst, processor, model):\n",
    "    dataset = Dataset.from_dict({\"wav\": wav_file_lst }).cast_column(\"wav\", Audio())\n",
    "    wav_embeddings = []\n",
    "    for i in range(len(dataset)): \n",
    "        inputs = processor(dataset[i]['wav']['array'], sampling_rate = sampling_rate, return_tensors = 'pt', padding = True, max_length=400)\n",
    "        with torch.no_grad():\n",
    "            output = model_wav(**inputs)\n",
    "        wav_embeddings.append(output['last_hidden_state'])\n",
    "    return wav_embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Embedding\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 참고: https://huggingface.co/docs/datasets/nlp_load\n",
    "- 참고: https://huggingface.co/docs/transformers/v4.27.2/en/model_doc/data2vec#transformers.Data2VecTextModel\n",
    "- 참고 한국어 pretrained model: https://huggingface.co/Junmai/KR-Data2VecText-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast, BartModel\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import pandas as pd\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./org_KEMDy20/Session01/Sess01_script01_User002M_001.txt',\n",
       " './org_KEMDy20/Session01/Sess01_script01_User002M_002.txt',\n",
       " './org_KEMDy20/Session01/Sess01_script01_User002M_003.txt']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text data loading\n",
    "txt_file_lst = []\n",
    "for i in file_lst:\n",
    "    txt_file_lst.append('./org_KEMDy20/Session01/' + i + '.txt')\n",
    "txt_file_lst[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 참고: https://huggingface.co/gogamza/kobart-base-v2\n",
    "# 참고: https://github.com/BM-K/Sentence-Embedding-Is-All-You-Need\n",
    "\n",
    "# model = AutoModel.from_pretrained('gogamza/kobart-base-v2')  # or 'BM-K/KoSimCSE-bert-multitask'\n",
    "# tokenizer = AutoTokenizer.from_pretrained('gogamza/kobart-base-v2')  # or 'BM-K/KoSimCSE-bert-multitask'\n",
    "checkpoint = 'BM-K/KoSimCSE-bert-multitask'\n",
    "model_txt = AutoModel.from_pretrained(checkpoint)  # or 'BM-K/KoSimCSE-bert-multitask'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)  # or 'BM-K/KoSimCSE-bert-multitask'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_embeddings_func(txt_file_lst, tokenizer, model):\n",
    "    sentences = []\n",
    "\n",
    "    for i in txt_file_lst:    \n",
    "        f = open(i, 'r')\n",
    "        line = f.readline()\n",
    "        # line = re.sub('l/\\n|c/|b/|n/|l/|u/', '', line)\n",
    "        line = re.sub('\\n', '', line)\n",
    "        line = re.sub('  ', ' ', line)\n",
    "        line = line.rstrip().lstrip()\n",
    "        sentences.append(line)\n",
    "        f.close()\n",
    "    start_index = np.random.randint(1, high = len(txt_file_lst)) - 10\n",
    "    print(sentences[start_index:start_index+10])\n",
    "    \n",
    "    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    txt_embeddings, _ = model_txt(**inputs, return_dict=False)\n",
    "    return txt_embeddings\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터셋 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # wav file embedding\n",
    "# wav_embeddings = wav_embeddings_func(wav_file_lst, processor, model_wav)\n",
    "# wav_embeddings[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # txt file embedding\n",
    "# txt_embeddings = text_embeddings_func(txt_file_lst,tokenizer, model_txt)\n",
    "# txt_embeddings[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name_lst = get_file_names(wav_file_lst)\n",
    "# file_name_lst[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # annotation data 가져오기\n",
    "# df_annotation = pd.read_csv('org_KEMDy20/annotation/' + file_name, skiprows=1)\n",
    "# df_annotation.Arousal[:3], df_annotation.Emotion[:3], df_annotation.Valence[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(txt_embeddings), len(wav_embeddings), len(file_name_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(file_name_lst, \n",
    "                  text_embeddings, \n",
    "                  wav_embeddings, \n",
    "                  label_emotion,\n",
    "                  label_arousal,\n",
    "                  label_valence, \n",
    "                  session_name = 'Session01'):\n",
    "    dataset = {'file_names': file_name_lst, \n",
    "         'text_embeddings': text_embeddings, \n",
    "         'wav_embeddings':wav_embeddings,\n",
    "         'Emotion':label_emotion,\n",
    "         'Arousal':label_arousal,\n",
    "         'Valence':label_valence}\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Session36', 'Session37', 'Session38', 'Session39', 'Session40'],\n",
       " ['Sess36_eval.csv',\n",
       "  'Sess37_eval.csv',\n",
       "  'Sess38_eval.csv',\n",
       "  'Sess39_eval.csv',\n",
       "  'Sess40_eval.csv'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session_num_lst = []\n",
    "annot_num_lst = []\n",
    "for i in range(40):\n",
    "    if i <= 8:\n",
    "        session_num_lst.append('Session0' + str(i+1))\n",
    "        annot_num_lst.append('Sess0' + str(i+1) + '_eval.csv')\n",
    "    else:\n",
    "        session_num_lst.append('Session' + str(i+1))\n",
    "        annot_num_lst.append('Sess' + str(i+1) + '_eval.csv')\n",
    "session_num_lst[-5:], annot_num_lst[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_annotation = pd.read_csv('org_KEMDy20/annotation/' + annot, skiprows=1)\n",
    "# df_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c/ 기차를 타고.', 'c/ 근데 그때가 이제 처음 방을 구하러 내려온 날이었거든.', '근데 처음에 이제 기차를 서울에서 타고 오는데 친해진 옆자리 사람도 같 원광대 오는 사람인 거야.', 'c/ 바로 옆에 앉 u/ .', 'c/ 이제 그게 익산 가까이 와서 좀 알게 됐는데 그 사람도 방을 보러 왔 온다고 하더라고.', '그래서 같이 어 그러면 같이 방을 볼래요 하면서 같이 방을 보다가 그 결국은 둘이 같이 룸메를 해서 하숙집에 들어갔어.', '그래갖고 근데 지 난 그 둘이 되게 친해졌을 거라고 생각을 했거든?', 'c/ 근데 진짜 방만 공유하고 서로 아무 대화도 안 한다는 거야. l/', '심지어 같은 원룸에서 같이 지내는데.', '그냥 이불을 깔고 같이 지내는데 아무 그거를 안 얘기를 거의 안 한다고 하더라고, 어 왔어 정도? l/']\n",
      "['c/ 그래서 언니가 지금 딴 데 가있어서 그래서 엄마가 아빠 술 먹고 좀 좀 만취한 거 같으면 아예 다른 방에서 주무세요.', 'c/ 소리가 너무 커 가지고 n/ 저 방과 방 끝 끝에 있는데도 b/ 여기 끝에서 아빠가 아 오늘 아빠 코골이 많이 하네 좀 알 정 도로 많이 들리더라고요.', '아 딱 인제 제 친구가 그 정도의 소리였을 거 같애요, 그때.', 'c/ 진짜 엄청 컸거든요.', '너무 코골이가 심하고 제 친구도 코골이 진짜 심한 애가 기숙사 그니까 두 명 있었거든요.', '둘 다 기숙사 살았는데 룸메가 2주?', '진짜 길면 한 달 살다가 다 나갔어요.', '도저히 얘 코골이 너무 심해서 같이 못 살겠다고 다른 방 해달 해달라 그래가지구 b/ 중간에 탈주 엄청 하더라고요.', 'c/ 도저히 못 자겠다고.', '아 근데 진짜 코골이는 너무 심하면 좀 그래서 근데 또 l/ 신 신기한게 코고는 친구들끼리는 엄청 잘 자더라고요, 같이 l/ u/ 어 b/ 어 어 너무 u/ 하면서 옆에 있는 코 안 고는 애들 u/ 야 너희들 코 엄청 곤다고 하면서 어 그래 골았어 내가 이러면서.']\n",
      "['아 그니까 나한테는 그 지갑 값을 주는 게 아니라 뭔가 그냥 용돈 겸 이렇게 주시는데 이런 적이 많았어.', '그냥 말 없이 b/ 그냥 봉투 같은데에 막 사랑하는 아빠가 하면서 3만원 5만원 이렇게 준 적이 많았단 말야.', '어 봉투에다가?', '근데 난 그런 어 그냥 사랑하는 아빠가 딱 적어서.', '나는 그럴 때마다 진짜 막 울컥하고 그냥 액수가 중요하지 않고 그냥 너무 그 마음이 너무 잘 전달이 돼서 응 아빠한테 감동할 때가 많았지.', '너는 그 아 아버지한테 감동 받았던 적 있었어?', '나는 감동이라기보다는 딱히 b/ 없는데 일단은 나는 고등학교 졸업하기 전까지 아니 u/ 뭐 회사를 내가 퇴사하기 전까지 부모님이랑 난 되게 좀 사이가 좀 멀었다고 생각을 했거든.', '그니까 부모님이 교회를 다니면서 나한테 교회를 막 강요를 하니까 그것 때문에 트러블이 되게 심했어.', '그러면서 이제 중고등학교때는 막 주기적으로 막 뭐 나 한 달에 얼마 용돈 얼마 받는다 이런 말 하잖아.', '나는 그런 게 따로 없었고 그냥 필요할 때마다 그냥 조금씩 챙겨주는 그런 그 정도였지.']\n"
     ]
    }
   ],
   "source": [
    "dataset = {}\n",
    "for annot, session in zip(annot_num_lst[:3], session_num_lst[:3]):\n",
    "    \n",
    "    df_annotation = pd.read_csv('org_KEMDy20/annotation/' + annot, skiprows=1)\n",
    "    file_name_lst = df_annotation[' .1']\n",
    "    \n",
    "    txt_file_lst = []\n",
    "    wav_file_lst = []\n",
    "    for file_name in file_name_lst:\n",
    "        txt_file_lst.append('./org_KEMDy20/' + session + '/' + file_name + '.txt')\n",
    "        wav_file_lst.append('./org_KEMDy20/' + session + '/' + file_name + '.wav')\n",
    "    \n",
    "    txt_embeddings = text_embeddings_func(txt_file_lst, tokenizer, model_txt)\n",
    "    wav_embeddings = wav_embeddings_func(wav_file_lst, processor, model_wav_cuda)\n",
    "    \n",
    "    dataset[session] = build_dataset(file_name_lst, \n",
    "                txt_embeddings, \n",
    "                wav_embeddings,  \n",
    "                df_annotation.Arousal, \n",
    "                df_annotation.Emotion, \n",
    "                df_annotation.Valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Session01', 'Session02', 'Session03'])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311\n",
      "311\n",
      "311\n",
      "311\n",
      "311\n",
      "311\n"
     ]
    }
   ],
   "source": [
    "# length test\n",
    "for i in dataset['Session01'].keys():\n",
    "    print(len(dataset['Session01'][i]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save data set\n",
    "with open('./model/data/lou_dataset_1_3.pkl', 'wb')as f:\n",
    "    pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['file_names', 'text_embeddings', 'wav_embeddings', 'Emotion', 'Arousal', 'Valence'])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./model/data/lou_dataset.pkl', 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "dataset['Session01'].keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bdc1fd12ca460d5768d71e9df3d9063ef832ce64a62e55a1a523c8c99752868e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
