{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [참고 data2vec audio:사용 안하기로함]: https://towhee.io/audio-embedding/data2vec\n",
    "- [참고 data2vec text]: https://huggingface.co/docs/datasets/nlp_load\n",
    "- [참고 data2vec load audio*]:https://huggingface.co/docs/datasets/audio_load\n",
    "- [참고 data2vec embedding]: https://huggingface.co/docs/transformers/model_doc/data2vec#transformers.Data2VecAudioModel\n",
    "- [참고 handling pytorch dataset]:https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel\n",
    "- [참고 add data to dataset]: https://pypi.org/project/datasets/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings(Text, Audio) Session 1 \n",
    "- 각세션으로 확대시켜야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arplab/project/lou/multi_modal/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, Data2VecAudioModel\n",
    "from datasets import load_dataset, Dataset, Audio, Features\n",
    "from glob import glob\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Sess01_script01_User002M_001\n",
       "1      Sess01_script01_User002M_002\n",
       "2      Sess01_script01_User002M_003\n",
       "3      Sess01_script01_User002M_004\n",
       "4      Sess01_script01_User001F_001\n",
       "                   ...             \n",
       "306    Sess01_script06_User001F_016\n",
       "307    Sess01_script06_User002M_041\n",
       "308    Sess01_script06_User002M_042\n",
       "309    Sess01_script06_User002M_043\n",
       "310    Sess01_script06_User001F_017\n",
       "Name:  .1, Length: 311, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = 'Sess01_eval.csv'\n",
    "df_annotation = pd.read_csv('org_KEMDy20/annotation/' + file_name, skiprows=1)\n",
    "# df_annotation['Emotion'], df_annotation['Valence'], df_annotation['Arousal']\n",
    "file_lst = df_annotation[' .1']\n",
    "file_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Session01',\n",
       " 'Session02',\n",
       " 'Session03',\n",
       " 'Session04',\n",
       " 'Session05',\n",
       " 'Session06',\n",
       " 'Session07',\n",
       " 'Session08',\n",
       " 'Session09',\n",
       " 'Session10',\n",
       " 'Session11',\n",
       " 'Session12',\n",
       " 'Session13',\n",
       " 'Session14',\n",
       " 'Session15',\n",
       " 'Session16',\n",
       " 'Session17',\n",
       " 'Session18',\n",
       " 'Session19',\n",
       " 'Session20',\n",
       " 'Session21',\n",
       " 'Session22',\n",
       " 'Session23',\n",
       " 'Session24',\n",
       " 'Session25',\n",
       " 'Session26',\n",
       " 'Session27',\n",
       " 'Session28',\n",
       " 'Session29',\n",
       " 'Session30',\n",
       " 'Session31',\n",
       " 'Session32',\n",
       " 'Session33',\n",
       " 'Session34',\n",
       " 'Session35',\n",
       " 'Session36',\n",
       " 'Session37',\n",
       " 'Session38',\n",
       " 'Session39',\n",
       " 'Session40']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session_num_lst = []\n",
    "for i in range(40):\n",
    "    if i <= 8:\n",
    "        session_num_lst.append('Session0' + str(i+1))\n",
    "    else:\n",
    "        session_num_lst.append('Session' + str(i+1))\n",
    "        \n",
    "session_num_lst"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./org_KEMDy20/Session01/Sess01_script01_User002M_001.wav',\n",
       " './org_KEMDy20/Session01/Sess01_script01_User002M_002.wav',\n",
       " './org_KEMDy20/Session01/Sess01_script01_User002M_003.wav']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav_file_lst = []\n",
    "for i in file_lst:\n",
    "    wav_file_lst.append('./org_KEMDy20/Session01/' + i + '.wav')\n",
    "wav_file_lst[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_names(file_lst):\n",
    "    file_name_lst = []\n",
    "    for i in file_lst:\n",
    "        i = i.split('/')[-1]\n",
    "        i = i.split('.')[-2]\n",
    "        file_name_lst.append(i)\n",
    "    return file_name_lst\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': './org_KEMDy20/Session01/Sess01_script01_User001F_001.wav',\n",
       " 'array': array([-0.00183105, -0.00146484, -0.00140381, ..., -0.00091553,\n",
       "        -0.00076294, -0.0010376 ], dtype=float32),\n",
       " 'sampling_rate': 16000}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "dataset = Dataset.from_dict({\"wav\": wav_file_lst }).cast_column(\"wav\", Audio())\n",
    "dataset[0][\"wav\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/data2vec-audio-base-960h were not used when initializing Data2VecAudioModel: ['lm_head.bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing Data2VecAudioModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Data2VecAudioModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "check_point_wav_model = 'facebook/data2vec-audio-base-960h'\n",
    "processor = AutoProcessor.from_pretrained(check_point_wav_model)\n",
    "model_wav = Data2VecAudioModel.from_pretrained(check_point_wav_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00183105, -0.00146484, -0.00140381, ..., -0.00091553,\n",
       "       -0.00076294, -0.0010376 ], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['wav']['array']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_rate = dataset['wav'][0]['sampling_rate']\n",
    "sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wav_embeddings_func(wav_file_lst, processor, model):\n",
    "    dataset = Dataset.from_dict({\"wav\": wav_file_lst }).cast_column(\"wav\", Audio())\n",
    "    wav_embeddings = []\n",
    "    for i in range(len(dataset)): \n",
    "        inputs = processor(dataset[i]['wav']['array'], sampling_rate = sampling_rate, return_tensors = 'pt', padding = True, max_length=400)\n",
    "        with torch.no_grad():\n",
    "            output = model_wav(**inputs)\n",
    "        wav_embeddings.append(output['last_hidden_state'])\n",
    "    return wav_embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Embedding\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 참고: https://huggingface.co/docs/datasets/nlp_load\n",
    "- 참고: https://huggingface.co/docs/transformers/v4.27.2/en/model_doc/data2vec#transformers.Data2VecTextModel\n",
    "- 참고 한국어 pretrained model: https://huggingface.co/Junmai/KR-Data2VecText-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast, BartModel\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import pandas as pd\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./org_KEMDy20/Session01/Sess01_script01_User002M_001.txt',\n",
       " './org_KEMDy20/Session01/Sess01_script01_User002M_002.txt',\n",
       " './org_KEMDy20/Session01/Sess01_script01_User002M_003.txt']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text data loading\n",
    "txt_file_lst = []\n",
    "for i in file_lst:\n",
    "    txt_file_lst.append('./org_KEMDy20/Session01/' + i + '.txt')\n",
    "txt_file_lst[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 참고: https://huggingface.co/gogamza/kobart-base-v2\n",
    "# 참고: https://github.com/BM-K/Sentence-Embedding-Is-All-You-Need\n",
    "\n",
    "# model = AutoModel.from_pretrained('gogamza/kobart-base-v2')  # or 'BM-K/KoSimCSE-bert-multitask'\n",
    "# tokenizer = AutoTokenizer.from_pretrained('gogamza/kobart-base-v2')  # or 'BM-K/KoSimCSE-bert-multitask'\n",
    "checkpoint = 'BM-K/KoSimCSE-bert-multitask'\n",
    "model_txt = AutoModel.from_pretrained(checkpoint)  # or 'BM-K/KoSimCSE-bert-multitask'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)  # or 'BM-K/KoSimCSE-bert-multitask'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_embeddings_func(txt_file_lst, tokenizer, model):\n",
    "    sentences = []\n",
    "\n",
    "    for i in txt_file_lst:    \n",
    "        f = open(i, 'r')\n",
    "        line = f.readline()\n",
    "        # line = re.sub('l/\\n|c/|b/|n/|l/|u/', '', line)\n",
    "        line = re.sub('\\n', '', line)\n",
    "        line = re.sub('  ', ' ', line)\n",
    "        line = line.rstrip().lstrip()\n",
    "        sentences.append(line)\n",
    "        f.close()\n",
    "    start_index = np.random.randint(1, high = len(txt_file_lst)) - 10\n",
    "    print(sentences[start_index:start_index+10])\n",
    "    \n",
    "    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    txt_embeddings, _ = model_txt(**inputs, return_dict=False)\n",
    "    return txt_embeddings\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터셋 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-27 17:14:41.527856: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-27 17:14:42.452983: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11/include:/usr/local/cuda-11/lib64::/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-03-27 17:14:42.453067: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11/include:/usr/local/cuda-11/lib64::/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-03-27 17:14:42.453075: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[[-1.0521,  0.0236, -0.0885,  ...,  0.0592, -0.0703,  0.1332],\n",
       "          [-1.0521,  0.0236, -0.0885,  ...,  0.0592, -0.0703,  0.1332],\n",
       "          [-1.0521,  0.0236, -0.0885,  ...,  0.0592, -0.0703,  0.1332],\n",
       "          ...,\n",
       "          [-1.1249,  0.0886, -0.0205,  ..., -0.1581, -0.2275,  0.0239],\n",
       "          [-1.1264,  0.0884, -0.0214,  ..., -0.1584, -0.2291,  0.0230],\n",
       "          [-1.1284,  0.0942, -0.0229,  ..., -0.1598, -0.2316,  0.0242]]]),\n",
       " tensor([[[-1.2183,  0.2462, -0.3427,  ...,  0.0913,  0.0475,  0.2768],\n",
       "          [-1.2183,  0.2462, -0.3427,  ...,  0.0913,  0.0475,  0.2768],\n",
       "          [-1.2183,  0.2462, -0.3427,  ...,  0.0913,  0.0475,  0.2768],\n",
       "          ...,\n",
       "          [-1.1338,  0.1411, -0.0790,  ..., -0.1295, -0.0888,  0.0974],\n",
       "          [-1.1310,  0.1384, -0.0780,  ..., -0.1281, -0.0885,  0.0973],\n",
       "          [-1.1288,  0.1362, -0.0776,  ..., -0.1268, -0.0862,  0.0965]]]),\n",
       " tensor([[[-0.6320,  0.0581, -0.4626,  ...,  0.1816, -0.1539,  0.4442],\n",
       "          [-0.6320,  0.0581, -0.4626,  ...,  0.1816, -0.1539,  0.4442],\n",
       "          [-0.6320,  0.0581, -0.4626,  ...,  0.1816, -0.1539,  0.4442],\n",
       "          ...,\n",
       "          [-0.9908, -0.0683, -0.0462,  ..., -0.0752, -0.1489,  0.1926],\n",
       "          [-0.9909, -0.0674, -0.0470,  ..., -0.0756, -0.1494,  0.1926],\n",
       "          [-0.9920, -0.0675, -0.0476,  ..., -0.0764, -0.1505,  0.1920]]])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wav file embedding\n",
    "wav_embeddings = wav_embeddings_func(wav_file_lst, processor, model_wav)\n",
    "wav_embeddings[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c/ 내가 그냥 그 일상적인 전화로 대화하거나 하는데.', '그래갖고 되게 특히 친구랑 전화하는데 웃겼던 적도 있었었어.', '어 막 한참 웃으면서 얘기하고 있는데 갑자기 막 노래 노래 막 팝송 나오고 l/ 어.', '어어 l/ b/ 되게 BGM처럼 막 깔아줬던 웃겼던 적도 있었었어.', 'b/ u/ 그것 말고 또 뭐가 있었을까?', '되게 웃겼던 적은 막 인식을 진짜 잘못해서 하는 경우가 대부분인 거 같애.', 'c/ 그 영상에서 봤다시피.', '우리 집은 되게 자유로워 가지고 솔직히 아빠가 이게 어때라고 하면 본인 생 그니까 가족들 생각이랑 안 맞으면 그냥 자기 주관대로 가거든, 다들.', '그래서 좀 자유로워서 아빠가 b/ 가부장적인 면이 없지 않아 있다고는 생각을 하는데 그래도 우리가 그냥 안 듣다 보니까 그런 게 없 없는 듯이 느껴지는 거 같애.', '가부장적이다라는 거?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7713,  0.3540, -0.1561,  ..., -0.8406, -0.5942,  0.7619],\n",
       "         [-0.0500, -1.1157,  0.3815,  ..., -0.3483,  0.5291,  0.6400],\n",
       "         [-1.4624, -0.6926, -0.6904,  ..., -0.1558,  0.2706,  1.2804],\n",
       "         ...,\n",
       "         [-0.4228,  0.6478,  0.0635,  ..., -0.9161,  0.1675,  0.7280],\n",
       "         [-0.6350,  0.4763, -0.0086,  ..., -0.8950,  0.3458,  0.6184],\n",
       "         [-0.7193, -0.0301, -0.1823,  ..., -0.5610,  0.4327,  0.3804]],\n",
       "\n",
       "        [[ 0.5551, -0.4455, -0.0732,  ...,  1.3076,  0.8142,  1.6034],\n",
       "         [ 0.0394, -0.5077,  0.7435,  ...,  0.6888,  0.9015,  1.2672],\n",
       "         [-0.8009, -0.2002, -0.3190,  ...,  1.1466,  0.2687,  1.8889],\n",
       "         ...,\n",
       "         [ 0.8747, -0.5071, -0.1666,  ...,  1.4158,  1.1544,  1.0865],\n",
       "         [ 0.7265, -0.7809, -0.0251,  ...,  1.1178,  1.1242,  1.1936],\n",
       "         [ 0.6910, -0.9297, -0.2339,  ...,  0.6449,  1.0709,  1.6147]],\n",
       "\n",
       "        [[-0.4392,  0.2658,  0.3925,  ...,  0.9807,  1.5450,  1.4830],\n",
       "         [-0.5285,  0.5159,  0.2705,  ...,  0.8495,  2.5343,  1.2020],\n",
       "         [-0.1090,  1.0650, -0.0227,  ...,  1.1643,  1.9918,  1.7296],\n",
       "         ...,\n",
       "         [-0.4756, -0.1917,  0.4931,  ...,  0.4284,  1.8216,  0.8075],\n",
       "         [-0.0467,  0.1436,  0.5951,  ...,  0.3614,  1.7975,  1.2442],\n",
       "         [-0.0540,  0.3014,  0.5351,  ...,  0.3082,  1.8561,  1.2360]]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# txt file embedding\n",
    "txt_embeddings = text_embeddings_func(txt_file_lst,tokenizer, model_txt)\n",
    "txt_embeddings[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sess01_script01_User002M_001',\n",
       " 'Sess01_script01_User002M_002',\n",
       " 'Sess01_script01_User002M_003',\n",
       " 'Sess01_script01_User002M_004',\n",
       " 'Sess01_script01_User001F_001']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name_lst = get_file_names(wav_file_lst)\n",
    "file_name_lst[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    2.9\n",
       " 1    2.9\n",
       " 2    3.0\n",
       " Name: Arousal, dtype: float64,\n",
       " 0    neutral\n",
       " 1    neutral\n",
       " 2    neutral\n",
       " Name: Emotion, dtype: object,\n",
       " 0    3.4\n",
       " 1    3.1\n",
       " 2    3.1\n",
       " Name: Valence, dtype: float64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# annotation data 가져오기\n",
    "df_annotation = pd.read_csv('org_KEMDy20/annotation/' + file_name, skiprows=1)\n",
    "df_annotation.Arousal[:3], df_annotation.Emotion[:3], df_annotation.Valence[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311 311 311\n"
     ]
    }
   ],
   "source": [
    "print(len(txt_embeddings), len(wav_embeddings), len(file_name_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(file_name_lst, \n",
    "                  text_embeddings, \n",
    "                  wav_embeddings, \n",
    "                  label_emotion,\n",
    "                  label_arousal,\n",
    "                  label_valence, \n",
    "                  session_name = 'Session01'):\n",
    "    dataset = {session_name: \n",
    "        {'file_names': file_name_lst, \n",
    "         'text_embeddings': text_embeddings, \n",
    "         'wav_embeddings':wav_embeddings,\n",
    "         'Emotion':label_emotion,\n",
    "         'Arousal':label_arousal,\n",
    "         'Valence':label_valence}}\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['file_names', 'text_embeddings', 'wav_embeddings', 'Emotion', 'Arousal', 'Valence'])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = build_dataset(file_name_lst, \n",
    "                txt_embeddings, \n",
    "                wav_embeddings,  \n",
    "                df_annotation.Arousal, \n",
    "                df_annotation.Emotion, \n",
    "                df_annotation.Valence,\n",
    "                session_name = 'Session01')\n",
    "dataset['Session01'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311\n",
      "311\n",
      "311\n",
      "311\n",
      "311\n",
      "311\n"
     ]
    }
   ],
   "source": [
    "# length test\n",
    "for i in dataset['Session01'].keys():\n",
    "    print(len(dataset['Session01'][i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save data set\n",
    "with open('./model/data/lou_dataset.pkl', 'wb')as f:\n",
    "    pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['file_names', 'text_embeddings', 'wav_embeddings', 'Emotion', 'Arousal', 'Valence'])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./model/data/lou_dataset.pkl', 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "dataset['Session01'].keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bdc1fd12ca460d5768d71e9df3d9063ef832ce64a62e55a1a523c8c99752868e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
