{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download a Original Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* if has not dataset, you must do unlock comment and excute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !./init y20"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import csv\n",
    "import shutil"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set global variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = \"KEMDy20\"\n",
    "global_data_path = f\"{dir_name}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directories(\n",
    "    dir_path: str\n",
    ") -> List[str]:\n",
    "    t_list = []\n",
    "    \n",
    "    if os.path.exists(dir_path):\n",
    "        t_list = sorted(os.listdir(dir_path))\n",
    "    \n",
    "    return t_list\n",
    "\n",
    "def get_files(\n",
    "    dir_path: str,\n",
    "    extension: str\n",
    ") -> List[str]:\n",
    "    \n",
    "    f_list = []\n",
    "    \n",
    "    for l in get_directories(dir_path):\n",
    "        if l.endswith(f\".{extension}\"):\n",
    "            f_list.append(l)\n",
    "    \n",
    "    return sorted(f_list)\n",
    "\n",
    "def read_csv_info(\n",
    "    file_path: str=\"\"\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "    csv_list = []\n",
    "    with open(file_path) as f:\n",
    "        r = csv.reader(f)\n",
    "\n",
    "        for l in r:\n",
    "            csv_list.append(l)\n",
    "\n",
    "    return pd.DataFrame(csv_list)\n",
    "\n",
    "def make_merged_data(\n",
    "    target_dir_list: List[str],\n",
    "    file_list: List[str],\n",
    "    session_name: str\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    assert session_name != None\n",
    "    \n",
    "    res: pd.DataFrame = None\n",
    "        \n",
    "    for _f in file_list:\n",
    "        _fds = {}\n",
    "        \n",
    "        for tdp in target_dir_list:\n",
    "            _td = tdp.split(\"/\")\n",
    "            \n",
    "            # print(f\"{session_name}, {file}, {target_dir[1]}\")\n",
    "            # Directory path form name is as like \"KEMDy20/EDA/Session01/Sess01_script01_User001F.csv\"\n",
    "            _fds[_td[1]] = read_csv_info(f\"./{_td[0]}/{_td[1]}/{session_name}/{_f}\")\n",
    "            # print(file_datas[target_dir[1]].to_string())\n",
    "            \n",
    "            if not _fds[_td[1]].empty:\n",
    "                if _td[1] == \"EDA\" or _td[1] == \"TEMP\":\n",
    "                    _fds[_td[1]] = _fds[_td[1]].drop(1) \n",
    "                _fds[_td[1]] = _fds[_td[1]].drop(0)\n",
    "                \n",
    "                if _td[1] == \"IBI\":\n",
    "                    _fds[_td[1]] = _fds[_td[1]].drop(columns=_fds[_td[1]].columns[0])\n",
    "                    # reset a column index \n",
    "                    _fds[_td[1]] = _fds[_td[1]].T.reset_index(drop=True).T\n",
    "                    \n",
    "                # drop the rows if has None\n",
    "                _fds[_td[1]] = _fds[_td[1]].dropna()\n",
    "                \n",
    "                _fds[_td[1]] = _fds[_td[1]].reset_index(drop=True)\n",
    "            else:\n",
    "                if _td[1] == \"IBI\":\n",
    "                    _fds[_td[1]] = pd.DataFrame({\"ibi\": [None], \"timestamp\": [None], \"sid\": [None]})\n",
    "        \n",
    "        # ** for check data\n",
    "        # for x in target_dir_list:\n",
    "        #     t = x.split(\"/\")\n",
    "        #     print(f\"{t[1]} : \")\n",
    "        #     print(file_datas[t[1]].to_string())\n",
    "        \n",
    "        # attatching column names\n",
    "        if len(_fds[\"EDA\"].columns) < 3:\n",
    "            _fds[\"EDA\"] = _fds[\"EDA\"].set_axis([\"eda\", \"timestamp\"], axis=1)\n",
    "            _fds[\"EDA\"][\"sid\"] = np.nan\n",
    "        else:\n",
    "            _fds[\"EDA\"] = _fds[\"EDA\"].set_axis([\"eda\", \"timestamp\", \"sid\"], axis=1)\n",
    "        \n",
    "        if len(_fds[\"IBI\"].columns) < 3:\n",
    "            _fds[\"IBI\"] = _fds[\"IBI\"].set_axis([\"ibi\", \"timestamp\"], axis=1)\n",
    "            _fds[\"IBI\"][\"sid\"] = np.nan\n",
    "        else:\n",
    "            _fds[\"IBI\"] = _fds[\"IBI\"].set_axis([\"ibi\", \"timestamp\", \"sid\"], axis=1)\n",
    "        \n",
    "        if len(_fds[\"TEMP\"].columns) < 3:\n",
    "            _fds[\"TEMP\"] = _fds[\"TEMP\"].set_axis([\"temp\", \"timestamp\"], axis=1)\n",
    "            _fds[\"TEMP\"][\"sid\"] = np.nan\n",
    "        else:\n",
    "            _fds[\"TEMP\"] = _fds[\"TEMP\"].set_axis([\"temp\", \"timestamp\", \"sid\"], axis=1)\n",
    "\n",
    "        # Merge \"TEMP\" table to \"EDA\" \n",
    "        _opd = pd.merge(\n",
    "            _fds[\"EDA\"], _fds[\"TEMP\"],\n",
    "            left_on='timestamp', right_on='timestamp', how='outer')\n",
    "        del _opd[\"sid_x\"]\n",
    "        _opd = _opd.rename(columns={\"sid_y\": \"sid\"})\n",
    "\n",
    "        # Merge \"IBI\" table to merged table(\"EDA\" and \"TEMP\")\n",
    "        _opd = pd.merge(\n",
    "            _opd, _fds[\"IBI\"],\n",
    "            left_on='timestamp', right_on='timestamp', how='outer')\n",
    "        _opd[\"sid_x\"] = _opd[\"sid_x\"].fillna(_opd[\"sid_y\"])\n",
    "        del _opd[\"sid_y\"]\n",
    "        _opd = _opd.rename(columns={\"sid_x\": \"sid\"})\n",
    "\n",
    "        # reorder following as \"timestamp\", \"sid\", \"eda\", \"temp\", \"ibi\"\n",
    "        _opd = _opd[[\"timestamp\", \"sid\", \"eda\", \"temp\", \"ibi\"]]\n",
    "        # sorting values from \"timestamp\" column\n",
    "        _opd = _opd.sort_values(\"timestamp\")\n",
    "        _opd = _opd.reset_index(drop=True)\n",
    "        \n",
    "        if res is None:\n",
    "            res = _opd\n",
    "        else:\n",
    "            res = pd.concat([res, _opd], sort=True)\n",
    "            # result = result.append(one_part_data_on_session, ignore_index=True)\n",
    "            \n",
    "    res = res[[\"timestamp\", \"sid\", \"eda\", \"temp\", \"ibi\"]]\n",
    "    res = res.sort_values(\"timestamp\")\n",
    "    res = res.reset_index(drop=True)\n",
    "    \n",
    "    # print(result.to_string())\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get organized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir_list = [\"EDA\", \"IBI\", \"TEMP\"]\n",
    "\n",
    "all_fl = {}\n",
    "all_sl = []\n",
    "\n",
    "for part_name in target_dir_list:\n",
    "    _pp = f\"{global_data_path}/{part_name}\"\n",
    "    _sdl = get_directories(f\"./{_pp}\")\n",
    "    \n",
    "    for _sd in _sdl:\n",
    "        _sdp = f\"{_pp}/{_sd}\"\n",
    "        _fl = get_files(f\"./{_sdp}\", \"csv\")\n",
    "        \n",
    "        if _sd not in all_sl:\n",
    "            all_sl.append(_sd)\n",
    "            \n",
    "        _fl_s = []\n",
    "        \n",
    "        for _f in _fl:\n",
    "            if _f not in _fl_s:\n",
    "                _fl_s.append(_f)\n",
    "        \n",
    "        if _sd not in all_fl:\n",
    "            all_fl[_sd] = _fl_s\n",
    "            \n",
    "            \n",
    "org_dataset_path = f\"org_{global_data_path}\"\n",
    "\n",
    "for _s in all_sl:\n",
    "    l = [global_data_path + \"/\" + x for x in target_dir_list]\n",
    "    \n",
    "    _smd = make_merged_data(l, all_fl[_s], _s)\n",
    "    \n",
    "    # t_ori_path = os.path.join(os.getcwd(), org_dataset_path)\n",
    "    _op = f\"./{org_dataset_path}\"\n",
    "    \n",
    "    _dl = get_directories(os.getcwd())\n",
    "    if f\"{org_dataset_path}\" not in _dl:\n",
    "        os.mkdir(f\"{_op}\")\n",
    "    \n",
    "    _dl = get_directories(f\"{os.getcwd()}/{org_dataset_path}\")\n",
    "    if f\"{_s}\" not in _dl:\n",
    "        os.mkdir(f\"{_op}/{_s}\")\n",
    "    \n",
    "    _op_wav = f\"./{global_data_path}/wav\"\n",
    "    \n",
    "    _fl = get_files(f\"{_op_wav}/{_s}\", \"wav\")\n",
    "    _fl += get_files(f\"{_op_wav}/{_s}\", \"txt\")\n",
    "\n",
    "    for _f in _fl:\n",
    "        _tf = f\"{_op_wav}/{_s}/{_f}\"\n",
    "        _dd = f\"{_op}/{_s}\"\n",
    "        \n",
    "        if _f.endswith(\"wav\"):\n",
    "            shutil.move(f\"{_tf}\", f\"{_dd}\")\n",
    "        else:\n",
    "            # if file is text..\n",
    "            with open(f\"{_tf}\", mode=\"r\", encoding=\"euc-kr\", errors=\"ignore\") as f:\n",
    "                _nfl = []\n",
    "\n",
    "                ls = f.readlines()\n",
    "                for t_l in ls:\n",
    "                    _nfl.append(t_l.encode(\"utf-8\", \"ignore\").decode(\"utf-8\"))\n",
    "                    \n",
    "                with open(f\"{_dd}/{_f}\", mode=\"w\") as des_f:\n",
    "                    des_f.writelines(_nfl)\n",
    "        \n",
    "    _smd.to_csv(f\"{_op}/{_s}/{_s}.csv\", sep=\",\", na_rep=\"NaN\")\n",
    "    \n",
    "src_ann_path = os.path.join(os.getcwd(), f\"{global_data_path}/annotation\")\n",
    "dest_ann_path = os.path.join(os.getcwd(), f\"{org_dataset_path}/annotation\")\n",
    "\n",
    "shutil.copytree(src_ann_path, dest_ann_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bdc1fd12ca460d5768d71e9df3d9063ef832ce64a62e55a1a523c8c99752868e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
