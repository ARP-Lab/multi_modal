{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoProcessor, Data2VecAudioModel\n",
    "from transformers import AutoTokenizer, Data2VecTextModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset, Audio\n",
    "import datasets\n",
    "import glob\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_path(session_num, modal):\n",
    "# modal : wav, txt 둘 중 하나 입력\n",
    "# 이렇게 불러온 데이터는 시간 순은 아님을 유의\n",
    "    if session_num <= 9:\n",
    "        dir_path = '/home/arplab/project/paradeigma/multi_modal/org_KEMDy20/Session0' + str(session_num)\n",
    "    else:\n",
    "        dir_path = '/home/arplab/project/paradeigma/multi_modal/org_KEMDy20/Session' + str(session_num)\n",
    "    data_path = glob.glob(dir_path + '/*.' + modal)\n",
    "    data_path = sorted(data_path)\n",
    "    return data_path\n",
    "\n",
    "def make_audio_datasets(num_sessions):\n",
    "# num_sessions : from 1 to 40\n",
    "# type(return) = dict{1:audio_dataset,...,40:audio_dataset}\n",
    "    audio_datasets = {}\n",
    "    \n",
    "    for i in tqdm(range(1, num_sessions + 1)):\n",
    "        # audio_datasets[i] = datasets.Dataset.from_dict({'audio': data_path(i, 'wav')})\n",
    "        audio_datasets[i] = datasets.Dataset.from_dict({'audio': data_path(i, 'wav')}).cast_column(\"audio\", Audio())\n",
    "    return audio_datasets\n",
    "\n",
    "def make_text_datasets(num_sessions):\n",
    "# num_sessions : from 1 to 40\n",
    "# type(return) = dict{1:[paths, corpus],...,40:[paths, corpus]}\n",
    "    text_datasets = {}\n",
    "    \n",
    "    for i in tqdm(range(1, num_sessions + 1)):\n",
    "        \n",
    "        paths = []\n",
    "        corpus = []\n",
    "        \n",
    "        for path in data_path(i, 'txt'):\n",
    "            paths.append(path)\n",
    "            with open(path, \"r\") as f:\n",
    "                corpus.append(f.read())\n",
    "        \n",
    "        text_datasets[i] = [paths, corpus]\n",
    "\n",
    "    return text_datasets  \n",
    "\n",
    "def audio_embedding(audio_datasets, batch_size=32):\n",
    "# audio_datasets = {1:audio_dataset, ... , 40:audio_dataset}\n",
    "    batch_size = batch_size\n",
    "    \n",
    "    audio_processed_dict = {}\n",
    "    for session, audio_dataset in audio_datasets.items():\n",
    "        \n",
    "        processed_audio_by_session = []\n",
    "        for i in range(0, audio_dataset.num_rows, batch_size):\n",
    "            \n",
    "            audio_arrayes = []\n",
    "            for j in audio_dataset[i:i+batch_size]['audio']:\n",
    "                audio_arrayes.append(j['array'])\n",
    "        \n",
    "            inputs = processor(audio_arrayes, sampling_rate = sampling_rate,  padding=True, max_length=16000, truncation = True,return_attention_mask=True, return_tensors=\"pt\")\n",
    "            processed_audio_by_session.append(inputs)\n",
    "        \n",
    "        audio_processed_dict[session] = processed_audio_by_session\n",
    "\n",
    "    \n",
    "# audio_processed_dict : {1 : [[batch 당 processing][batch 당 processing][batch 당 processing]],2,3,}    \n",
    "    audio_embedded_dict = {}        \n",
    "    for session, val_dic_list in tqdm(audio_processed_dict.items()):\n",
    "        \n",
    "        val_dic_by_session = []\n",
    "        for val_dic in val_dic_list:\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = audio_d2v(**val_dic)\n",
    "            val_dic_by_session.append(outputs)    \n",
    "            \n",
    "        audio_embedded_dict[session] = val_dic_by_session\n",
    "# audio_embedded_dict = {1:embedded_data_by_session, ..., 40:embedded_data_by_session}    \n",
    "    \n",
    "    return audio_embedded_dict\n",
    "\n",
    "def text_embedding(text_datasets, batch_size=32):\n",
    "# text_dataset = {1:[path, corpus], ..., 40:[path, corpus]}    \n",
    "    \n",
    "    text_embedded_dict = {}\n",
    "    \n",
    "    for session, ( _ , corpus) in tqdm(text_datasets.items()):\n",
    "        \n",
    "        embedded_data_by_batch = []\n",
    "        print(len(corpus))\n",
    "        for i in range(0, len(corpus), batch_size):\n",
    "            \n",
    "            corpus_by_batch = corpus[i:i+batch_size]\n",
    "            inputs = tokenizer(corpus_by_batch, padding= True, max_length = 20, truncation = True, return_tensors=\"pt\")\n",
    "            outputs = text_d2v(**inputs)\n",
    "            embedded_data_by_batch.append(outputs)\n",
    "            \n",
    "        text_embedded_dict[session] = embedded_data_by_batch\n",
    "# text_embedded_dict = {1:embedded_data_by_session, ..., 40:embedded_data_by_session}         \n",
    "    return text_embedded_dict\n",
    "\n",
    "def make_target_dataframe(session_num):\n",
    "# session_num : from 1 to 40\n",
    "\n",
    "    if session_num <= 9:\n",
    "        target_path = '/home/arplab/project/paradeigma/multi_modal/org_KEMDy20/annotation/Sess0' + str(session_num) + '_eval.csv'\n",
    "    \n",
    "    else:\n",
    "        target_path = '/home/arplab/project/paradeigma/multi_modal/org_KEMDy20/annotation/Sess' + str(session_num) + '_eval.csv'\n",
    "\n",
    "    train = pd.read_csv(target_path)\n",
    "    train = train[['Segment ID', 'Total Evaluation',' .1',' .2']]\n",
    "    train.columns = ['segment_id','emotion','valence','arousal']\n",
    "    train = train.drop([0], axis = 0)\n",
    "    train = train.sort_values('segment_id', ascending=True)\n",
    "    train = train.reset_index(drop=True)\n",
    "    \n",
    "    return train\n",
    "\n",
    "def make_target_dict(num_sessions):\n",
    "# num_sessions : from 1 to 40\n",
    "\n",
    "    target_dict = {}\n",
    "    for i in tqdm(range(1, num_sessions + 1)):\n",
    "        target_data = {}\n",
    "        target_dataframe = make_target_dataframe(i)\n",
    "        columns = target_dataframe.columns\n",
    "        \n",
    "        for j in columns:\n",
    "            target_data[j] = target_dataframe[j]\n",
    "\n",
    "        target_dict[i] = target_data\n",
    "        \n",
    "# target_dict = {1:{segment_id:_,emotion:_,valence:_, arousal:_}, ..., 40: {segment_id:_,emotion:_,valence:_, arousal:_}}        \n",
    "    return target_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/data2vec-audio-base-960h were not used when initializing Data2VecAudioModel: ['lm_head.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing Data2VecAudioModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Data2VecAudioModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at facebook/data2vec-text-base were not used when initializing Data2VecTextModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing Data2VecTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Data2VecTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Data2VecTextModel were not initialized from the model checkpoint at facebook/data2vec-text-base and are newly initialized: ['data2vec_text.pooler.dense.weight', 'data2vec_text.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"facebook/data2vec-audio-base-960h\")\n",
    "audio_d2v = Data2VecAudioModel.from_pretrained(\"facebook/data2vec-audio-base-960h\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/data2vec-text-base\")\n",
    "text_d2v = Data2VecTextModel.from_pretrained(\"facebook/data2vec-text-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 98.65it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 124.56it/s]\n"
     ]
    }
   ],
   "source": [
    "audio_datasets = make_audio_datasets(1)\n",
    "sampling_rate = audio_datasets[1][0]['audio']['sampling_rate']\n",
    "text_datasets = make_text_datasets(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-30 12:52:31.556861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-30 12:52:32.398040: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11/include:/usr/local/cuda-11/lib64::/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-03-30 12:52:32.398115: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11/include:/usr/local/cuda-11/lib64::/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-03-30 12:52:32.398123: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "100%|██████████| 1/1 [00:23<00:00, 23.36s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.23s/it]\n"
     ]
    }
   ],
   "source": [
    "audio_embedded_dict = audio_embedding(audio_datasets,32)\n",
    "text_embedded_dict = text_embedding(text_datasets,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_audio_shape = audio_embedded_dict[1][0]['last_hidden_state'].shape\n",
    "a_b, a_l, a_e = input_audio_shape\n",
    "input_text_shape = text_embedded_dict[1][0]['last_hidden_state'].shape\n",
    "t_b, t_l, t_e = input_text_shape\n",
    "iteration = len(audio_embedded_dict[1])\n",
    "batch_size = len(audio_embedded_dict[1][0]['last_hidden_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Audio_Mlp_For_Tensor_Fusion(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Audio_Mlp_For_Tensor_Fusion, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(a_l * a_e, 768)\n",
    "        self.fc2 = nn.Linear(768, 32)\n",
    "\n",
    "    def forward(self, x):\n",
    "                          \n",
    "        x = torch.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "audio_mlp_for_tensor_fusion = Audio_Mlp_For_Tensor_Fusion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text_Mlp_For_Tensor_Fusion(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Text_Mlp_For_Tensor_Fusion, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(t_l * t_e, 768)\n",
    "        self.fc2 = nn.Linear(768, 32)\n",
    "\n",
    "    def forward(self, x):\n",
    "                          \n",
    "        x = torch.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "text_mlp_for_tensor_fusion = Text_Mlp_For_Tensor_Fusion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_922945/4256021504.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor_fusion_by_iteration[i+1] = torch.tensor(torch.concat(temp_list),requires_grad = True)\n"
     ]
    }
   ],
   "source": [
    "tensor_fusion_by_iteration = {}\n",
    "for i in range(iteration):\n",
    "    n, _, _ = audio_embedded_dict[1][i]['last_hidden_state'].shape\n",
    "    \n",
    "    temp_list = []\n",
    "    for j in range(n):\n",
    "        a_f_tf = audio_mlp_for_tensor_fusion(audio_embedded_dict[1][i]['last_hidden_state'][j])\n",
    "\n",
    "        t_f_tf = text_mlp_for_tensor_fusion(text_embedded_dict[1][i]['last_hidden_state'][j])\n",
    "        temp_tensor = torch.outer(a_f_tf, t_f_tf)\n",
    "        a, t = temp_tensor.shape\n",
    "        temp_tensor = temp_tensor.view(1, a, t)\n",
    "        temp_list.append(temp_tensor)\n",
    "\n",
    "    tensor_fusion_by_iteration[i+1] = torch.tensor(torch.concat(temp_list),requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 수 :  32\n",
      "iteration 내부 텐서 shape :  torch.Size([32, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "print('iteration 수 : ', len(tensor_fusion_by_iteration[1]))\n",
    "print('iteration 내부 텐서 shape : ', tensor_fusion_by_iteration[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp_For_Regression(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Mlp_For_Regression, self).__init__()\n",
    "        \n",
    "        self.fc2 = nn.Linear(32 * 32, 32)\n",
    "        self.fc3 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "                          \n",
    "        x = torch.flatten(x)\n",
    "        x = F.gelu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "mlp_for_regression = Mlp_For_Regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp_For_Classification(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Mlp_For_Classification, self).__init__()\n",
    "        \n",
    "        self.fc2 = nn.Linear(32 * 32, 32)\n",
    "        self.fc3 = nn.Linear(32, 7)\n",
    "\n",
    "    def forward(self, x):\n",
    "                          \n",
    "        x = torch.flatten(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "mlp_for_classification = Mlp_For_Classification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CCC, self).__init__()\n",
    "        self.mean = torch.mean\n",
    "        self.var = torch.var\n",
    "        self.sum = torch.sum\n",
    "        self.sqrt = torch.sqrt\n",
    "        self.std = torch.std\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        mean_gt = self.mean (target, 0)\n",
    "        mean_pred = self.mean (pred, 0)\n",
    "        var_gt = self.var (target, 0)\n",
    "        var_pred = self.var (pred, 0)\n",
    "        v_pred = pred - mean_pred\n",
    "        v_gt = target - mean_gt\n",
    "        cor = self.sum (v_pred * v_gt) / (self.sqrt(self.sum(v_pred ** 2)) * self.sqrt(self.sum(v_gt ** 2)))\n",
    "        sd_gt = self.std(target)\n",
    "        sd_pred = self.std(pred)\n",
    "        numerator = 2 * cor * sd_gt * sd_pred\n",
    "        denominator = var_gt + var_pred + (mean_gt-mean_pred) ** 2\n",
    "        ccc = numerator / denominator\n",
    "        return ccc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 167.00it/s]\n"
     ]
    }
   ],
   "source": [
    "target_dict = make_target_dict(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valence arousal\n",
    "\n",
    "valence = torch.tensor(target_dict[1]['valence'].astype('float32'))\n",
    "valence = valence.view(1, valence.size(0))\n",
    "arousal = torch.tensor(target_dict[1]['arousal'].astype('float32'))\n",
    "arousal = arousal.view(1, arousal.size(0))\n",
    "target = torch.concat([valence, arousal])\n",
    "target = target.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## arousal과 valence\n",
    "\n",
    "x_train = list(tensor_fusion_by_iteration.values())\n",
    "y_train = target\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(mlp_for_regression.parameters(), lr=0.01)\n",
    "ccc = CCC()\n",
    "\n",
    "batch_size, _, _ = tensor_fusion_by_iteration[1].shape\n",
    "num_epochs = 10\n",
    "\n",
    "num_samples = 0\n",
    "for i in range(iteration):\n",
    "    num, _, _ = tensor_fusion_by_iteration[i+1].shape\n",
    "    num_samples += num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 19.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss_Valence: 9.412535546293595\n",
      "Epoch 1, Loss_Arousal: 7.74254533792232\n",
      "CCC Difference : 0.012114864774048328\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 66.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss_Valence: 6.063390145347816\n",
      "Epoch 2, Loss_Arousal: 4.948316414639881\n",
      "CCC Difference : 0.012114851735532284\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 60.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss_Valence: 3.802438705297145\n",
      "Epoch 3, Loss_Arousal: 3.085053010768829\n",
      "CCC Difference : 0.01211480237543583\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 63.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss_Valence: 2.28096360907294\n",
      "Epoch 4, Loss_Arousal: 1.8421253529392254\n",
      "CCC Difference : 0.012114626355469227\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 61.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss_Valence: 1.2964051877570688\n",
      "Epoch 5, Loss_Arousal: 1.0403137412101893\n",
      "CCC Difference : 0.012113993987441063\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 64.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss_Valence: 0.712212885643125\n",
      "Epoch 6, Loss_Arousal: 0.5622005958074159\n",
      "CCC Difference : 0.012111721560359001\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 64.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss_Valence: 0.4085698378095673\n",
      "Epoch 7, Loss_Arousal: 0.31040899256705473\n",
      "CCC Difference : 0.01210432592779398\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 62.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss_Valence: 0.27288672759218613\n",
      "Epoch 8, Loss_Arousal: 0.19581423668711898\n",
      "CCC Difference : 0.012085750699043274\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 65.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss_Valence: 0.21971068131195387\n",
      "Epoch 9, Loss_Arousal: 0.1501296289576595\n",
      "CCC Difference : 0.012055540457367897\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 65.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss_Valence: 0.20051142338579492\n",
      "Epoch 10, Loss_Arousal: 0.13355249391683044\n",
      "CCC Difference : 0.012029086239635944\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## arousal과 valence / loss_2개\n",
    "\n",
    "epoch_loss_v = 0\n",
    "epoch_loss_a = 0\n",
    "ccc_difference = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss_v = 0.0\n",
    "    running_loss_a = 0.0\n",
    "    running_ccc_difference = 0.0\n",
    "    \n",
    "    for i in tqdm(range(iteration)):\n",
    "        \n",
    "        input = x_train[i]\n",
    "        num, _, _ = input.shape\n",
    "        targets = y_train[i * batch_size :i * batch_size + num]\n",
    "        targets.requires_grad = True\n",
    "        \n",
    "        outputs = []        \n",
    "        for j in range(num):\n",
    "            \n",
    "            output = mlp_for_regression(input[j])\n",
    "            output = output.view(1, output.size(0))\n",
    "            outputs.append(output)\n",
    "            \n",
    "        outputs = torch.concat(outputs)\n",
    "        \n",
    "        loss_v = criterion(outputs[:,0], targets[:,0])\n",
    "        loss_a = criterion(outputs[:,1], targets[:,1])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss_v.backward(retain_graph =True)\n",
    "        loss_a.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pred_ccc = ccc(outputs[:,0], outputs[:,1])\n",
    "        target_ccc = ccc(targets[:,0], targets[:,1])\n",
    "        difference = target_ccc - pred_ccc\n",
    "        running_ccc_difference += difference\n",
    "        \n",
    "        ccc_difference += ccc_difference  * num\n",
    "        running_loss_v += loss_v.item() * num\n",
    "        running_loss_a += loss_a.item() * num\n",
    "\n",
    "    epoch_loss_v = running_loss_v / num_samples\n",
    "    epoch_loss_a = running_loss_a / num_samples\n",
    "    ccc_difference = running_ccc_difference / num_samples\n",
    "    print(\"Epoch {}, Loss_Valence: {}\".format(epoch+1, epoch_loss_v))\n",
    "    print(\"Epoch {}, Loss_Arousal: {}\".format(epoch+1, epoch_loss_a))\n",
    "    print('CCC Difference : {}'.format(ccc_difference))\n",
    "    print('-'*30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 95.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.16094960023160915\n",
      "CCC Difference : 0.012060631066560745\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 102.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.15908928792863797\n",
      "CCC Difference : 0.012056855484843254\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 104.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.15799454247453204\n",
      "CCC Difference : 0.012054767459630966\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 106.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.15734340173830175\n",
      "CCC Difference : 0.012053962796926498\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 105.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.15695037886835755\n",
      "CCC Difference : 0.012054027989506721\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 96.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.15670887445521892\n",
      "CCC Difference : 0.012054613791406155\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 103.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.15655745843790736\n",
      "CCC Difference : 0.012055478990077972\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 105.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.15646047248142708\n",
      "CCC Difference : 0.012056452222168446\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 102.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.15639707192155708\n",
      "CCC Difference : 0.012057436630129814\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 101.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.15635480681416306\n",
      "CCC Difference : 0.012058368884027004\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## arousal과 valence / loss 1개\n",
    "\n",
    "epoch_loss = 0\n",
    "ccc_difference = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    running_ccc_difference = 0.0\n",
    "    \n",
    "    for i in tqdm(range(iteration)):\n",
    "        \n",
    "        input = x_train[i]\n",
    "        num, _, _ = input.shape\n",
    "        targets = y_train[i * batch_size :i * batch_size + num]\n",
    "        targets.requires_grad = True\n",
    "        \n",
    "        outputs = []        \n",
    "        for j in range(num):\n",
    "            \n",
    "            output = mlp_for_regression(input[j])\n",
    "            output = output.view(1, output.size(0))\n",
    "            outputs.append(output)\n",
    "            \n",
    "        outputs = torch.concat(outputs)\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        pred_ccc = ccc(outputs[:,0], outputs[:,1])\n",
    "        target_ccc = ccc(targets[:,0], targets[:,1])\n",
    "        difference = target_ccc - pred_ccc\n",
    "        running_ccc_difference += difference\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        ccc_difference += ccc_difference * num\n",
    "        running_loss += loss.item() * num\n",
    "\n",
    "        \n",
    "    epoch_loss = running_loss / num_samples\n",
    "    ccc_difference = running_ccc_difference / num_samples\n",
    "    print(\"Epoch {}, Loss: {}\".format(epoch+1, epoch_loss))\n",
    "    print('CCC Difference : {}'.format(ccc_difference))\n",
    "    print('-'*30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
