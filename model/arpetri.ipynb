{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data2Vec train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep\n",
    "---\n",
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arplab/project/gc/multi_modal/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from datasets import load_dataset, Audio\n",
    "import datasets\n",
    "from transformers import AutoProcessor, Data2VecAudioModel\n",
    "from transformers import AutoTokenizer, Data2VecTextModel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "- `data_path`\n",
    "- `make_audio_datasets`\n",
    "- `make_text_datasets`\n",
    "- `audio_embedding`\n",
    "- `text_embedding`\n",
    "- `make_target_dataframe`\n",
    "- `make_target_dict`\n",
    "\n",
    "- `tensor_fusion`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_path(session_num, modal):\n",
    "# modal : wav, txt 둘 중 하나 입력\n",
    "# 이렇게 불러온 데이터는 시간 순은 아님을 유의\n",
    "    if session_num <= 9:\n",
    "        dir_path = '/home/arplab/project/paradeigma/multi_modal/org_KEMDy20/Session0' + str(session_num)\n",
    "    else:\n",
    "        dir_path = '/home/arplab/project/paradeigma/multi_modal/org_KEMDy20/Session' + str(session_num)\n",
    "    data_path = glob.glob(dir_path + '/*.' + modal)\n",
    "    data_path = sorted(data_path)\n",
    "    return data_path\n",
    "\n",
    "def make_audio_datasets(num_sessions):\n",
    "# num_sessions : from 1 to 40\n",
    "# type(return) = dict{1:audio_dataset,...,40:audio_dataset}\n",
    "    audio_datasets = {}\n",
    "    \n",
    "    for i in tqdm(range(1, num_sessions + 1)):\n",
    "        # audio_datasets[i] = datasets.Dataset.from_dict({'audio': data_path(i, 'wav')})\n",
    "        audio_datasets[i] = datasets.Dataset.from_dict({'audio': data_path(i, 'wav')}).cast_column(\"audio\", Audio())\n",
    "    return audio_datasets\n",
    "\n",
    "def make_text_datasets(num_sessions):\n",
    "# num_sessions : from 1 to 40\n",
    "# type(return) = dict{1:[paths, corpus],...,40:[paths, corpus]}\n",
    "    text_datasets = {}\n",
    "    \n",
    "    for i in tqdm(range(1, num_sessions + 1)):\n",
    "        \n",
    "        paths = []\n",
    "        corpus = []\n",
    "        \n",
    "        for path in data_path(i, 'txt'):\n",
    "            paths.append(path)\n",
    "            with open(path, \"r\") as f:\n",
    "                corpus.append(f.read())\n",
    "        \n",
    "        text_datasets[i] = [paths, corpus]\n",
    "\n",
    "    return text_datasets  \n",
    "\n",
    "def audio_embedding(audio_datasets, batch_size=32):\n",
    "# audio_datasets = {1:audio_dataset, ... , 40:audio_dataset}\n",
    "    batch_size = batch_size\n",
    "    \n",
    "    audio_processed_dict = {}\n",
    "    for session, audio_dataset in audio_datasets.items():\n",
    "        \n",
    "        processed_audio_by_session = []\n",
    "        for i in range(0, audio_dataset.num_rows, batch_size):\n",
    "            \n",
    "            audio_arrayes = []\n",
    "            for j in audio_dataset[i:i+batch_size]['audio']:\n",
    "                audio_arrayes.append(j['array'])\n",
    "        \n",
    "            inputs = processor(audio_arrayes, sampling_rate = sampling_rate,  padding=True, max_length=16000, truncation = True,return_attention_mask=True, return_tensors=\"pt\")\n",
    "            processed_audio_by_session.append(inputs)\n",
    "        \n",
    "        audio_processed_dict[session] = processed_audio_by_session\n",
    "\n",
    "    \n",
    "# audio_processed_dict : {1 : [[batch 당 processing][batch 당 processing][batch 당 processing]],2,3,}    \n",
    "    audio_embedded_dict = {}        \n",
    "    for session, val_dic_list in tqdm(audio_processed_dict.items()):\n",
    "        \n",
    "        val_dic_by_session = []\n",
    "        for val_dic in val_dic_list:\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = audio_d2v(**val_dic)\n",
    "            val_dic_by_session.append(outputs)    \n",
    "            \n",
    "        audio_embedded_dict[session] = val_dic_by_session\n",
    "# audio_embedded_dict = {1:embedded_data_by_session, ..., 40:embedded_data_by_session}    \n",
    "    \n",
    "    return audio_embedded_dict\n",
    "\n",
    "def text_embedding(text_datasets, batch_size=32):\n",
    "# text_dataset = {1:[path, corpus], ..., 40:[path, corpus]}    \n",
    "    \n",
    "    text_embedded_dict = {}\n",
    "    \n",
    "    for session, ( _ , corpus) in tqdm(text_datasets.items()):\n",
    "        \n",
    "        embedded_data_by_batch = []\n",
    "        print(len(corpus))\n",
    "        for i in range(0, len(corpus), batch_size):\n",
    "            \n",
    "            corpus_by_batch = corpus[i:i+batch_size]\n",
    "            inputs = tokenizer(corpus_by_batch, padding= True, max_length = 20, truncation = True, return_tensors=\"pt\")\n",
    "            outputs = text_d2v(**inputs)\n",
    "            embedded_data_by_batch.append(outputs)\n",
    "            \n",
    "        text_embedded_dict[session] = embedded_data_by_batch\n",
    "# text_embedded_dict = {1:embedded_data_by_session, ..., 40:embedded_data_by_session}         \n",
    "    return text_embedded_dict\n",
    "\n",
    "def make_target_dataframe(session_num):\n",
    "# session_num : from 1 to 40\n",
    "\n",
    "    if session_num <= 9:\n",
    "        target_path = '/home/arplab/project/paradeigma/multi_modal/org_KEMDy20/annotation/Sess0' + str(session_num) + '_eval.csv'\n",
    "    \n",
    "    else:\n",
    "        target_path = '/home/arplab/project/paradeigma/multi_modal/org_KEMDy20/annotation/Sess' + str(session_num) + '_eval.csv'\n",
    "\n",
    "    train = pd.read_csv(target_path)\n",
    "    train = train[['Segment ID', 'Total Evaluation',' .1',' .2']]\n",
    "    train.columns = ['segment_id','emotion','valence','arousal']\n",
    "    train = train.drop([0], axis = 0)\n",
    "    train = train.sort_values('segment_id', ascending=True)\n",
    "    train = train.reset_index(drop=True)\n",
    "    \n",
    "    return train\n",
    "\n",
    "def make_target_dict(num_sessions):\n",
    "# num_sessions : from 1 to 40\n",
    "\n",
    "    target_dict = {}\n",
    "    for i in tqdm(range(1, num_sessions + 1)):\n",
    "        target_data = {}\n",
    "        target_dataframe = make_target_dataframe(i)\n",
    "        columns = target_dataframe.columns\n",
    "        \n",
    "        for j in columns:\n",
    "            target_data[j] = target_dataframe[j]\n",
    "\n",
    "        target_dict[i] = target_data\n",
    "        \n",
    "# target_dict = {1:{segment_id:_,emotion:_,valence:_, arousal:_}, ..., 40: {segment_id:_,emotion:_,valence:_, arousal:_}}        \n",
    "    return target_dict\n",
    "            \n",
    "def tensor_fusion(num):\n",
    "\n",
    "    fusion = torch.outer(audio_outputs[num][0,:,0], text_outputs[num][0,:,0])\n",
    "    a,b = fusion.shape\n",
    "    fusion = fusion.view([a, b, -1])\n",
    "    \n",
    "    for i in tqdm(range(1, 768)):\n",
    "        fusion_1 = torch.outer(audio_outputs[num][0,:,i], text_outputs[num][0,:,i])\n",
    "        a,b = fusion_1.shape\n",
    "        fusion_1 = fusion_1.view([a, b, -1])\n",
    "        fusion = torch.concat((fusion, fusion_1), dim=2)\n",
    "        \n",
    "    return fusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/data2vec-audio-base-960h were not used when initializing Data2VecAudioModel: ['lm_head.bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing Data2VecAudioModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Data2VecAudioModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at facebook/data2vec-text-base were not used when initializing Data2VecTextModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing Data2VecTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Data2VecTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Data2VecTextModel were not initialized from the model checkpoint at facebook/data2vec-text-base and are newly initialized: ['data2vec_text.pooler.dense.bias', 'data2vec_text.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"facebook/data2vec-audio-base-960h\")\n",
    "audio_d2v = Data2VecAudioModel.from_pretrained(\"facebook/data2vec-audio-base-960h\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/data2vec-text-base\")\n",
    "text_d2v = Data2VecTextModel.from_pretrained(\"facebook/data2vec-text-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 61.57it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 124.50it/s]\n"
     ]
    }
   ],
   "source": [
    "audio_datasets = make_audio_datasets(1)\n",
    "sampling_rate = audio_datasets[1][0]['audio']['sampling_rate']\n",
    "text_datasets = make_text_datasets(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-29 00:10:51.176565: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-29 00:10:51.876223: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11/include:/usr/local/cuda-11/lib64::/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-03-29 00:10:51.876299: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11/include:/usr/local/cuda-11/lib64::/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-03-29 00:10:51.876306: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "100%|██████████| 1/1 [00:20<00:00, 20.49s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.82s/it]\n"
     ]
    }
   ],
   "source": [
    "audio_embedded_dict = audio_embedding(audio_datasets,32)\n",
    "text_embedded_dict = text_embedding(text_datasets,32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_audio_shape = audio_embedded_dict[1][0]['last_hidden_state'].shape\n",
    "a_b, a_l, a_e = input_audio_shape\n",
    "input_text_shape = text_embedded_dict[1][0]['last_hidden_state'].shape\n",
    "t_b, t_l, t_e = input_text_shape\n",
    "iteration = len(audio_embedded_dict[1])\n",
    "batch_size = len(audio_embedded_dict[1][0]['last_hidden_state'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Audio_Mlp_For_Tensor_Fusion(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Audio_Mlp_For_Tensor_Fusion, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(a_l * a_e, 768)\n",
    "        self.fc2 = nn.Linear(768, 32)\n",
    "\n",
    "    def forward(self, x):\n",
    "                          \n",
    "        x = torch.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "audio_mlp_for_tensor_fusion = Audio_Mlp_For_Tensor_Fusion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text_Mlp_For_Tensor_Fusion(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Text_Mlp_For_Tensor_Fusion, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(t_l * t_e, 768)\n",
    "        self.fc2 = nn.Linear(768, 32)\n",
    "\n",
    "    def forward(self, x):\n",
    "                          \n",
    "        x = torch.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "text_mlp_for_tensor_fusion = Text_Mlp_For_Tensor_Fusion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_fusion_by_iteration = {}\n",
    "for i in range(iteration):\n",
    "    n, _, _ = audio_embedded_dict[1][i]['last_hidden_state'].shape\n",
    "    \n",
    "    temp_list = []\n",
    "    for j in range(n):\n",
    "        a_f_tf = audio_mlp_for_tensor_fusion(audio_embedded_dict[1][i]['last_hidden_state'][j])\n",
    "\n",
    "        t_f_tf = text_mlp_for_tensor_fusion(text_embedded_dict[1][i]['last_hidden_state'][j])\n",
    "        temp_tensor = torch.outer(a_f_tf, t_f_tf)\n",
    "        a, t = temp_tensor.shape\n",
    "        temp_tensor = temp_tensor.view(1, a, t)\n",
    "        temp_list.append(temp_tensor)\n",
    "\n",
    "    tensor_fusion_by_iteration[i+1] = torch.concat(temp_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 수 :  32\n",
      "iteration 내부 텐서 shape :  torch.Size([32, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "print('iteration 수 : ', len(tensor_fusion_by_iteration[1]))\n",
    "print('iteration 내부 텐서 shape : ', tensor_fusion_by_iteration[1].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.fc2 = nn.Linear(32 * 32, 32)\n",
    "        self.fc3 = nn.Linear(32, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "                          \n",
    "        x = torch.flatten(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "neutral = 0\n",
    "happy = 1\n",
    "surprise = 2\n",
    "angry = 3\n",
    "sad = 4\n",
    "disgust = 5\n",
    "fear = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 173.73it/s]\n"
     ]
    }
   ],
   "source": [
    "target_dict = make_target_dict(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = target_dict[1]['emotion'].copy()\n",
    "for i in range(len(target_dict[1]['emotion'])):\n",
    "    if 'happy' in target_dict[1]['emotion'][i]:\n",
    "        target[i] = 1\n",
    "        \n",
    "    elif 'surprise' in target_dict[1]['emotion'][i]:\n",
    "        target[i] = 2\n",
    "        \n",
    "    elif 'angry' in target_dict[1]['emotion'][i]:\n",
    "        target[i] = 3\n",
    "        \n",
    "    elif 'sad' in target_dict[1]['emotion'][i]:\n",
    "        target[i] = 4\n",
    "        \n",
    "    elif 'disgust' in target_dict[1]['emotion'][i]:\n",
    "        target[i] = 5\n",
    "        \n",
    "    elif 'fear' in target_dict[1]['emotion'][i]:\n",
    "        target[i] = 6\n",
    "        \n",
    "    else: \n",
    "        target[i] = 0        \n",
    "        \n",
    "target = target.astype('float32')\n",
    "target = target.values\n",
    "target = torch.tensor(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = list(tensor_fusion_by_iteration.values())\n",
    "y_train = target.long()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "batch_size, _, _ = tensor_fusion_by_iteration[1].shape\n",
    "num_epochs = 10\n",
    "\n",
    "num_samples = 0\n",
    "for i in range(iteration):\n",
    "    num, _, _ = tensor_fusion_by_iteration[i+1].shape\n",
    "    num_samples += num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_loss = 0\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for i in tqdm(range(iteration)):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        input = x_train[i]\n",
    "        input = torch.tensor(input)\n",
    "        num, _, _ = input.shape\n",
    "        targets = y_train[i * batch_size :i * batch_size + num]\n",
    "        \n",
    "        outputs = []\n",
    "        for j in range(num):\n",
    "            output = net(input[j])\n",
    "            output = output.view(1, len(output))\n",
    "            outputs.append(output)\n",
    "            \n",
    "        print(\"{} epoch에서 {} 번째 iteration\".format(epoch_loss + 1, i + 1))\n",
    "        print(outputs)\n",
    "        outputs = torch.concat(outputs) \n",
    "        loss = criterion(outputs, targets)\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * num\n",
    "        \n",
    "    epoch_loss = running_loss / num_samples\n",
    "    print(\"Epoch {}, Loss: {}\".format(epoch+1, epoch_loss))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############################# 이 위로만 보면 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 49, 768])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_embedded_dict[1][0]['last_hidden_state'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_mlp_for_tensor_fusion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_for_tf = audio_embedded_dict[1][0]['last_hidden_state']\n",
    "text_for_tf = text_embedded_dict[1][0]['last_hidden_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([49, 768])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_for_tf[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 768])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_for_tf[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 32])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_for_tensor_fusion(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_for_tensor_fusion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dict = make_target_dict(40)\n",
    "target = target_dict[1]['emotion'][0:4]\n",
    "target[0:3] = 0\n",
    "target[3] = 1\n",
    "# target[4:8] = 0\n",
    "target = target.astype('float32')\n",
    "target = target.values\n",
    "target = torch.tensor(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_embedding_1 = []\n",
    "processor_inputs = []\n",
    "for i in range(0, 8):\n",
    "    processor_inputs.append(audio_datasets[1][i]['audio']['array'])\n",
    "    \n",
    "model_inputs = processor(\n",
    "        processor_inputs, sampling_rate=sampling_rate, \n",
    "        padding='max_length', max_length = 16000, truncation = True, return_attention_mask=True, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = audio_d2v(**model_inputs)\n",
    "audio_embedding_1.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 4.,  4.,  4.,  8.,  6., 12.,  8., 10.,  9.,  7.,  7.,  8.,  8.,\n",
       "         4.,  8.,  8.,  7.,  4.,  7., 10.,  5.,  6.,  4.,  7.,  6.,  6.,\n",
       "         2., 10.,  6.,  4.,  6.,  8.,  3.,  7.,  8.,  3.,  6.,  3.,  3.,\n",
       "         5.,  3.,  0.,  2.,  1.,  3.,  3.,  2.,  1.,  1.,  1.,  2.,  5.,\n",
       "         3.,  2.,  2.,  1.,  2.,  2.,  3.,  1.,  2.,  1.,  0.,  0.,  2.,\n",
       "         0.,  0.,  1.,  1.,  0.,  1.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,\n",
       "         3.,  1.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,\n",
       "         1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.]),\n",
       " array([ 11616.  ,  15020.64,  18425.28,  21829.92,  25234.56,  28639.2 ,\n",
       "         32043.84,  35448.48,  38853.12,  42257.76,  45662.4 ,  49067.04,\n",
       "         52471.68,  55876.32,  59280.96,  62685.6 ,  66090.24,  69494.88,\n",
       "         72899.52,  76304.16,  79708.8 ,  83113.44,  86518.08,  89922.72,\n",
       "         93327.36,  96732.  , 100136.64, 103541.28, 106945.92, 110350.56,\n",
       "        113755.2 , 117159.84, 120564.48, 123969.12, 127373.76, 130778.4 ,\n",
       "        134183.04, 137587.68, 140992.32, 144396.96, 147801.6 , 151206.24,\n",
       "        154610.88, 158015.52, 161420.16, 164824.8 , 168229.44, 171634.08,\n",
       "        175038.72, 178443.36, 181848.  , 185252.64, 188657.28, 192061.92,\n",
       "        195466.56, 198871.2 , 202275.84, 205680.48, 209085.12, 212489.76,\n",
       "        215894.4 , 219299.04, 222703.68, 226108.32, 229512.96, 232917.6 ,\n",
       "        236322.24, 239726.88, 243131.52, 246536.16, 249940.8 , 253345.44,\n",
       "        256750.08, 260154.72, 263559.36, 266964.  , 270368.64, 273773.28,\n",
       "        277177.92, 280582.56, 283987.2 , 287391.84, 290796.48, 294201.12,\n",
       "        297605.76, 301010.4 , 304415.04, 307819.68, 311224.32, 314628.96,\n",
       "        318033.6 , 321438.24, 324842.88, 328247.52, 331652.16, 335056.8 ,\n",
       "        338461.44, 341866.08, 345270.72, 348675.36, 352080.  ]),\n",
       " <BarContainer object of 100 artists>)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = []\n",
    "for i in range(311):\n",
    "    _, l = audio_processed[i]['input_values'].shape\n",
    "    result.append(l)\n",
    "    \n",
    "plt.hist(result, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 988, 768])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_embedding_3[2]['last_hidden_state'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for i in range(311):\n",
    "    _, l, _ = audio_embedding_3[i]['last_hidden_state'].shape\n",
    "    result.append(l)\n",
    "    \n",
    "plt.hist(result, bins=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "audio_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00183105, -0.00146484, -0.00140381, ..., -0.00091553,\n",
       "       -0.00076294, -0.0010376 ], dtype=float32)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_datasets[1]['audio'][0]['array']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_values': [array([-0.07524762, -0.05815899, -0.05531088, ..., -0.03252605,\n",
       "       -0.02540578, -0.03822226], dtype=float32)], 'attention_mask': [array([1, 1, 1, ..., 1, 1, 1], dtype=int32)]}"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor(audio_datasets[1]['audio'][0]['array'], sampling_rate = sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 4.,  4.,  4.,  8.,  7., 11.,  8., 11.,  8., 10.,  5.,  7.,  9.,\n",
       "         3.,  8.,  8.,  7.,  4.,  7., 10.,  5.,  7.,  3.,  8.,  5.,  6.,\n",
       "         4.,  8.,  6.,  4.,  7.,  7.,  4.,  7.,  7.,  3.,  6.,  4.,  2.,\n",
       "         5.,  3.,  0.,  2.,  1.,  3.,  3.,  2.,  1.,  1.,  1.,  2.,  5.,\n",
       "         3.,  2.,  2.,  1.,  2.,  2.,  3.,  1.,  2.,  1.,  0.,  0.,  2.,\n",
       "         0.,  0.,  1.,  1.,  0.,  1.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,\n",
       "         3.,  1.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,\n",
       "         1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.]),\n",
       " array([  36.  ,   46.64,   57.28,   67.92,   78.56,   89.2 ,   99.84,\n",
       "         110.48,  121.12,  131.76,  142.4 ,  153.04,  163.68,  174.32,\n",
       "         184.96,  195.6 ,  206.24,  216.88,  227.52,  238.16,  248.8 ,\n",
       "         259.44,  270.08,  280.72,  291.36,  302.  ,  312.64,  323.28,\n",
       "         333.92,  344.56,  355.2 ,  365.84,  376.48,  387.12,  397.76,\n",
       "         408.4 ,  419.04,  429.68,  440.32,  450.96,  461.6 ,  472.24,\n",
       "         482.88,  493.52,  504.16,  514.8 ,  525.44,  536.08,  546.72,\n",
       "         557.36,  568.  ,  578.64,  589.28,  599.92,  610.56,  621.2 ,\n",
       "         631.84,  642.48,  653.12,  663.76,  674.4 ,  685.04,  695.68,\n",
       "         706.32,  716.96,  727.6 ,  738.24,  748.88,  759.52,  770.16,\n",
       "         780.8 ,  791.44,  802.08,  812.72,  823.36,  834.  ,  844.64,\n",
       "         855.28,  865.92,  876.56,  887.2 ,  897.84,  908.48,  919.12,\n",
       "         929.76,  940.4 ,  951.04,  961.68,  972.32,  982.96,  993.6 ,\n",
       "        1004.24, 1014.88, 1025.52, 1036.16, 1046.8 , 1057.44, 1068.08,\n",
       "        1078.72, 1089.36, 1100.  ]),\n",
       " <BarContainer object of 100 artists>)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = []\n",
    "for i in range(311):\n",
    "    _, l, _ = audio_embedding_3[i]['last_hidden_state'].shape\n",
    "    result.append(l)\n",
    "    \n",
    "plt.hist(result, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = processor(\n",
    "        processor_inputs, padding=True, return_attention_mask=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>311.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>315.77492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>209.48179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>36.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>147.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>273.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>410.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1100.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0\n",
       "count   311.00000\n",
       "mean    315.77492\n",
       "std     209.48179\n",
       "min      36.00000\n",
       "25%     147.00000\n",
       "50%     273.00000\n",
       "75%     410.00000\n",
       "max    1100.00000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "for i in range(311):\n",
    "    a, b, c = audio_outputs[i]['last_hidden_state'].shape\n",
    "    result.append(b)\n",
    "    \n",
    "pd.DataFrame(result).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([79., 69., 59., 46., 27., 18.,  6.,  4.,  2.,  1.]),\n",
       " array([ 15. ,  47.5,  80. , 112.5, 145. , 177.5, 210. , 242.5, 275. ,\n",
       "        307.5, 340. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(sorted(result), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>311.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94.491961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>59.196930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>45.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>81.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>128.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>340.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0\n",
       "count  311.000000\n",
       "mean    94.491961\n",
       "std     59.196930\n",
       "min     15.000000\n",
       "25%     45.500000\n",
       "50%     81.000000\n",
       "75%    128.000000\n",
       "max    340.000000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "for i in range(311):\n",
    "    a, b, c = text_outputs[i]['last_hidden_state'].shape\n",
    "    result.append(b)\n",
    "    \n",
    "pd.DataFrame(result).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 7.,  7., 11.,  7.,  8., 13.,  5.,  5., 10.,  6.,  4.,  6.,  8.,\n",
       "         7.,  2.,  6., 14.,  6.,  9.,  7., 14.,  6.,  5.,  5.,  4.,  4.,\n",
       "         4.,  4., 11.,  2.,  5.,  3.,  7.,  5.,  7.,  4.,  5.,  4.,  3.,\n",
       "         3.,  4.,  3.,  2.,  1.,  3.,  5.,  2.,  3.,  3.,  1.,  2.,  0.,\n",
       "         1.,  0.,  3.,  6.,  3.,  1.,  1.,  1.,  0.,  1.,  3.,  0.,  2.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  2.,  0.,  0.,  0.,  0.,  2.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.]),\n",
       " array([ 15.  ,  18.25,  21.5 ,  24.75,  28.  ,  31.25,  34.5 ,  37.75,\n",
       "         41.  ,  44.25,  47.5 ,  50.75,  54.  ,  57.25,  60.5 ,  63.75,\n",
       "         67.  ,  70.25,  73.5 ,  76.75,  80.  ,  83.25,  86.5 ,  89.75,\n",
       "         93.  ,  96.25,  99.5 , 102.75, 106.  , 109.25, 112.5 , 115.75,\n",
       "        119.  , 122.25, 125.5 , 128.75, 132.  , 135.25, 138.5 , 141.75,\n",
       "        145.  , 148.25, 151.5 , 154.75, 158.  , 161.25, 164.5 , 167.75,\n",
       "        171.  , 174.25, 177.5 , 180.75, 184.  , 187.25, 190.5 , 193.75,\n",
       "        197.  , 200.25, 203.5 , 206.75, 210.  , 213.25, 216.5 , 219.75,\n",
       "        223.  , 226.25, 229.5 , 232.75, 236.  , 239.25, 242.5 , 245.75,\n",
       "        249.  , 252.25, 255.5 , 258.75, 262.  , 265.25, 268.5 , 271.75,\n",
       "        275.  , 278.25, 281.5 , 284.75, 288.  , 291.25, 294.5 , 297.75,\n",
       "        301.  , 304.25, 307.5 , 310.75, 314.  , 317.25, 320.5 , 323.75,\n",
       "        327.  , 330.25, 333.5 , 336.75, 340.  ]),\n",
       " <BarContainer object of 100 artists>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(result, bins = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "    text_embedding_1 = []\n",
    "# for i in range(311):\n",
    "    processed_inputs = tokenizer(text_datasets[1][1][0:4], padding=True, return_attention_mask=True, return_tensors=\"pt\")  \n",
    "    with torch.no_grad():\n",
    "        text_output = text_d2v(**processed_inputs)\n",
    "    text_embedding_1.append(text_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "    text_embedding_2 = []\n",
    "# for i in range(311):\n",
    "    processed_inputs = tokenizer(text_datasets[1][1][4:8], padding=True, return_attention_mask=True, return_tensors=\"pt\")  \n",
    "    with torch.no_grad():\n",
    "        text_output = text_d2v(**processed_inputs)\n",
    "    text_embedding_2.append(text_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "    text_embedding_3 = []\n",
    "# for i in range(311):\n",
    "    processed_inputs = tokenizer(text_datasets[1][1][4:8], padding=True, return_attention_mask=True, return_tensors=\"pt\")  \n",
    "    with torch.no_grad():\n",
    "        text_output = text_d2v(**processed_inputs)\n",
    "    text_embedding_3.append(text_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_embedding = [audio_embedding_1, audio_embedding_2, audio_embedding_3]\n",
    "text_embedding = [text_embedding_1, text_embedding_2, text_embedding_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:18<00:00, 18.84s/it]\n"
     ]
    }
   ],
   "source": [
    "# cpu만 사용하면 8분걸림\n",
    "# cuda를 사용하면 OOM\n",
    "\n",
    "audio_embedded_dict = audio_embedding(audio_datasets,32)\n",
    "text_embedded_dict = text_embedding(text_datasets, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 49, 768])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_embedded_dict[1][0]['last_hidden_state'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. 셀의 코드를 검토하여 오류의 가능한 원인을 식별하세요. 자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'> 여기 </a> 를 클릭하세요. 자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "# cpu만 사용하다가 커널이 죽었음\n",
    "# cuda를 사용하면 OOM\n",
    "\n",
    "text_embedded_dict = text_embedding(text_datasets, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 154.77it/s]\n"
     ]
    }
   ],
   "source": [
    "target_dict = make_target_dict(40)\n",
    "target = target_dict[1]['emotion'][0:4]\n",
    "target[0:3] = 0\n",
    "target[3] = 1\n",
    "# target[4:8] = 0\n",
    "target = target.astype('float32')\n",
    "target = target.values\n",
    "target = torch.tensor(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = txt.len\n",
    "class MLP_2(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MLP_2, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(a * 768, 768)\n",
    "        self.fc2 = nn.Linear(768, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        \n",
    "mlp_2 = MLP_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = txt.len\n",
    "class MLP_3(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MLP_2, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(b * 768, 768)\n",
    "        self.fc2 = nn.Linear(768, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        \n",
    "mlp_2 = MLP_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_1(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MLP_1, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(1235 * 768, 768)\n",
    "        self.fc2 = nn.Linear(768, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        \n",
    "mlp_1 = MLP_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 차원 축소를 무조건 수행해야 할 것. \n",
    "\n",
    "class MLP_2(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MLP_2, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(988 * 247 * 768, 768)\n",
    "        self.fc2 = nn.Linear(768, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        \n",
    "mlp_2 = MLP_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = txt.len\n",
    "class MLP_1(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MLP_1, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(a * 768, 768)\n",
    "        self.fc2 = nn.Linear(768, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        \n",
    "mlp_2 = MLP_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.2201, -0.0666,  0.0060,  ..., -0.0497, -0.1140,  0.1118],\n",
       "           [-0.0825, -0.1271, -0.0981,  ..., -0.0238,  0.1167,  0.0750],\n",
       "           [-0.0827, -0.1283, -0.0996,  ..., -0.0240,  0.1175,  0.0731],\n",
       "           ...,\n",
       "           [-0.0175, -0.0055, -0.0603,  ...,  0.0140, -0.1443,  0.1085],\n",
       "           [-0.0175, -0.0055, -0.0603,  ...,  0.0140, -0.1443,  0.1085],\n",
       "           [-0.0175, -0.0055, -0.0603,  ...,  0.0140, -0.1443,  0.1085]],\n",
       "  \n",
       "          [[ 0.2213, -0.0926,  0.0161,  ..., -0.0221, -0.0915,  0.1040],\n",
       "           [-0.0858, -0.1488, -0.1290,  ..., -0.0294,  0.0905,  0.0782],\n",
       "           [-0.0866, -0.1474, -0.1287,  ..., -0.0289,  0.0938,  0.0791],\n",
       "           ...,\n",
       "           [ 0.0723, -0.0225, -0.0495,  ...,  0.0341, -0.1373,  0.1264],\n",
       "           [ 0.0723, -0.0225, -0.0495,  ...,  0.0341, -0.1373,  0.1264],\n",
       "           [ 0.0723, -0.0225, -0.0495,  ...,  0.0341, -0.1373,  0.1264]],\n",
       "  \n",
       "          [[ 0.1490,  0.0442,  0.0011,  ..., -0.0777, -0.0520,  0.0530],\n",
       "           [ 0.0451,  0.1310, -0.1243,  ..., -0.1696, -0.0587,  0.1216],\n",
       "           [-0.1387,  0.1176, -0.1055,  ..., -0.1350, -0.1373,  0.0800],\n",
       "           ...,\n",
       "           [-0.0301,  0.1807, -0.0913,  ..., -0.1080, -0.0254,  0.0851],\n",
       "           [ 0.0088,  0.1920, -0.0450,  ..., -0.0625,  0.0330,  0.1144],\n",
       "           [ 0.0420,  0.0207, -0.0287,  ..., -0.1400, -0.0137,  0.1640]],\n",
       "  \n",
       "          [[ 0.1999, -0.0700,  0.0083,  ..., -0.0400, -0.0878,  0.0947],\n",
       "           [-0.0811, -0.1350, -0.0984,  ..., -0.0193,  0.1041,  0.0530],\n",
       "           [-0.0978, -0.1667, -0.1205,  ..., -0.0163,  0.1153,  0.0230],\n",
       "           ...,\n",
       "           [-0.1043, -0.0704, -0.0799,  ..., -0.0227,  0.1061,  0.0512],\n",
       "           [-0.1043, -0.0704, -0.0799,  ..., -0.0227,  0.1061,  0.0512],\n",
       "           [-0.1043, -0.0704, -0.0799,  ..., -0.0227,  0.1061,  0.0512]]]), pooler_output=tensor([[ 0.1540,  0.2666,  0.4922,  ..., -0.0024, -0.0418,  0.0778],\n",
       "          [ 0.1506,  0.2731,  0.4825,  ...,  0.0128, -0.0301,  0.0877],\n",
       "          [ 0.2489,  0.2499,  0.3779,  ..., -0.0834, -0.1714,  0.0695],\n",
       "          [ 0.1980,  0.2820,  0.4899,  ...,  0.0022, -0.0549,  0.0632]]), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)],\n",
       " [BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.2069, -0.0914,  0.0268,  ..., -0.0217, -0.1011,  0.0959],\n",
       "           [-0.0960, -0.1339, -0.0956,  ..., -0.0243,  0.1004,  0.0824],\n",
       "           [-0.0969, -0.1342, -0.0958,  ..., -0.0246,  0.1029,  0.0817],\n",
       "           ...,\n",
       "           [-0.1315, -0.0455, -0.0699,  ..., -0.0009,  0.0794, -0.0271],\n",
       "           [-0.1315, -0.0455, -0.0699,  ..., -0.0009,  0.0794, -0.0271],\n",
       "           [-0.1315, -0.0455, -0.0699,  ..., -0.0009,  0.0794, -0.0271]],\n",
       "  \n",
       "          [[ 0.1930, -0.0008,  0.0025,  ..., -0.1118, -0.0759,  0.1418],\n",
       "           [-0.0804, -0.0774, -0.0583,  ..., -0.0292,  0.0867,  0.0921],\n",
       "           [-0.0915, -0.0769, -0.0658,  ..., -0.0164,  0.0774,  0.0796],\n",
       "           ...,\n",
       "           [-0.0990,  0.0018, -0.1043,  ..., -0.0689,  0.0661,  0.1004],\n",
       "           [-0.0990,  0.0018, -0.1043,  ..., -0.0689,  0.0661,  0.1004],\n",
       "           [-0.0990,  0.0018, -0.1043,  ..., -0.0689,  0.0661,  0.1004]],\n",
       "  \n",
       "          [[ 0.1555, -0.0130, -0.0471,  ..., -0.0462, -0.0392,  0.0339],\n",
       "           [-0.0841,  0.1841, -0.0622,  ..., -0.0278,  0.0063,  0.0067],\n",
       "           [-0.1023,  0.1290, -0.0390,  ..., -0.0447,  0.0023,  0.0709],\n",
       "           ...,\n",
       "           [-0.1242,  0.0957, -0.0843,  ..., -0.1112,  0.0358,  0.1312],\n",
       "           [-0.1242,  0.0957, -0.0843,  ..., -0.1112,  0.0358,  0.1312],\n",
       "           [-0.1242,  0.0957, -0.0843,  ..., -0.1112,  0.0358,  0.1312]],\n",
       "  \n",
       "          [[ 0.1385,  0.0238, -0.0428,  ..., -0.0228, -0.0383,  0.0292],\n",
       "           [-0.0237,  0.0916, -0.1573,  ..., -0.1828, -0.0597,  0.1499],\n",
       "           [-0.1159,  0.1607, -0.1232,  ..., -0.0968, -0.0580,  0.1512],\n",
       "           ...,\n",
       "           [ 0.0074,  0.1468, -0.0801,  ..., -0.0152,  0.0470,  0.0509],\n",
       "           [ 0.0246,  0.1051, -0.0773,  ..., -0.0590,  0.0794,  0.0901],\n",
       "           [-0.0119,  0.0034, -0.0428,  ..., -0.1452,  0.0231,  0.1840]]]), pooler_output=tensor([[ 0.1933,  0.2836,  0.4871,  ...,  0.0126, -0.0310,  0.0710],\n",
       "          [ 0.1658,  0.2715,  0.4922,  ..., -0.0417, -0.0989,  0.0618],\n",
       "          [ 0.2673,  0.2953,  0.4478,  ..., -0.0413, -0.0995,  0.0266],\n",
       "          [ 0.2667,  0.2922,  0.4133,  ..., -0.0603, -0.1403,  0.0344]]), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)],\n",
       " [BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.2069, -0.0914,  0.0268,  ..., -0.0217, -0.1011,  0.0959],\n",
       "           [-0.0960, -0.1339, -0.0956,  ..., -0.0243,  0.1004,  0.0824],\n",
       "           [-0.0969, -0.1342, -0.0958,  ..., -0.0246,  0.1029,  0.0817],\n",
       "           ...,\n",
       "           [-0.1315, -0.0455, -0.0699,  ..., -0.0009,  0.0794, -0.0271],\n",
       "           [-0.1315, -0.0455, -0.0699,  ..., -0.0009,  0.0794, -0.0271],\n",
       "           [-0.1315, -0.0455, -0.0699,  ..., -0.0009,  0.0794, -0.0271]],\n",
       "  \n",
       "          [[ 0.1930, -0.0008,  0.0025,  ..., -0.1118, -0.0759,  0.1418],\n",
       "           [-0.0804, -0.0774, -0.0583,  ..., -0.0292,  0.0867,  0.0921],\n",
       "           [-0.0915, -0.0769, -0.0658,  ..., -0.0164,  0.0774,  0.0796],\n",
       "           ...,\n",
       "           [-0.0990,  0.0018, -0.1043,  ..., -0.0689,  0.0661,  0.1004],\n",
       "           [-0.0990,  0.0018, -0.1043,  ..., -0.0689,  0.0661,  0.1004],\n",
       "           [-0.0990,  0.0018, -0.1043,  ..., -0.0689,  0.0661,  0.1004]],\n",
       "  \n",
       "          [[ 0.1555, -0.0130, -0.0471,  ..., -0.0462, -0.0392,  0.0339],\n",
       "           [-0.0841,  0.1841, -0.0622,  ..., -0.0278,  0.0063,  0.0067],\n",
       "           [-0.1023,  0.1290, -0.0390,  ..., -0.0447,  0.0023,  0.0709],\n",
       "           ...,\n",
       "           [-0.1242,  0.0957, -0.0843,  ..., -0.1112,  0.0358,  0.1312],\n",
       "           [-0.1242,  0.0957, -0.0843,  ..., -0.1112,  0.0358,  0.1312],\n",
       "           [-0.1242,  0.0957, -0.0843,  ..., -0.1112,  0.0358,  0.1312]],\n",
       "  \n",
       "          [[ 0.1385,  0.0238, -0.0428,  ..., -0.0228, -0.0383,  0.0292],\n",
       "           [-0.0237,  0.0916, -0.1573,  ..., -0.1828, -0.0597,  0.1499],\n",
       "           [-0.1159,  0.1607, -0.1232,  ..., -0.0968, -0.0580,  0.1512],\n",
       "           ...,\n",
       "           [ 0.0074,  0.1468, -0.0801,  ..., -0.0152,  0.0470,  0.0509],\n",
       "           [ 0.0246,  0.1051, -0.0773,  ..., -0.0590,  0.0794,  0.0901],\n",
       "           [-0.0119,  0.0034, -0.0428,  ..., -0.1452,  0.0231,  0.1840]]]), pooler_output=tensor([[ 0.1933,  0.2836,  0.4871,  ...,  0.0126, -0.0310,  0.0710],\n",
       "          [ 0.1658,  0.2715,  0.4922,  ..., -0.0417, -0.0989,  0.0618],\n",
       "          [ 0.2673,  0.2953,  0.4478,  ..., -0.0413, -0.0995,  0.0266],\n",
       "          [ 0.2667,  0.2922,  0.4133,  ..., -0.0603, -0.1403,  0.0344]]), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)]]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [audio_embedding, text_embedding]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Wav2Vec2BaseModelOutput(last_hidden_state=tensor([[[-1.1453e+00, -1.3806e-01, -6.6733e-02,  ...,  6.6787e-03,\n",
       "           -7.2099e-02,  2.8462e-01],\n",
       "          [-1.1453e+00, -1.3806e-01, -6.6732e-02,  ...,  6.6784e-03,\n",
       "           -7.2098e-02,  2.8462e-01],\n",
       "          [-1.1453e+00, -1.3806e-01, -6.6733e-02,  ...,  6.6792e-03,\n",
       "           -7.2098e-02,  2.8462e-01],\n",
       "          ...,\n",
       "          [-9.0208e-01, -6.1873e-02, -4.0272e-02,  ..., -1.4839e-01,\n",
       "           -1.7549e-01,  1.7050e-01],\n",
       "          [-9.0822e-01, -5.7904e-02, -4.5461e-02,  ..., -1.5357e-01,\n",
       "           -1.8189e-01,  1.7206e-01],\n",
       "          [-9.1004e-01, -5.6092e-02, -4.8181e-02,  ..., -1.5627e-01,\n",
       "           -1.8337e-01,  1.7411e-01]],\n",
       " \n",
       "         [[-9.5902e-01, -6.7264e-02, -2.1643e-01,  ...,  9.4446e-02,\n",
       "           -7.7794e-04,  4.8156e-01],\n",
       "          [-9.5902e-01, -6.7264e-02, -2.1643e-01,  ...,  9.4447e-02,\n",
       "           -7.7803e-04,  4.8156e-01],\n",
       "          [-9.5902e-01, -6.7264e-02, -2.1643e-01,  ...,  9.4446e-02,\n",
       "           -7.7759e-04,  4.8156e-01],\n",
       "          ...,\n",
       "          [-1.0012e+00,  4.9996e-02, -1.3064e-01,  ..., -1.2703e-01,\n",
       "           -8.2842e-02,  1.5022e-01],\n",
       "          [-1.0109e+00,  5.1292e-02, -1.3564e-01,  ..., -1.2783e-01,\n",
       "           -9.2527e-02,  1.4789e-01],\n",
       "          [-1.0029e+00,  5.1213e-02, -1.3539e-01,  ..., -1.3114e-01,\n",
       "           -9.4374e-02,  1.5265e-01]],\n",
       " \n",
       "         [[-6.3203e-01,  5.8064e-02, -4.6260e-01,  ...,  1.8155e-01,\n",
       "           -1.5392e-01,  4.4419e-01],\n",
       "          [-6.3203e-01,  5.8064e-02, -4.6260e-01,  ...,  1.8155e-01,\n",
       "           -1.5392e-01,  4.4419e-01],\n",
       "          [-6.3203e-01,  5.8064e-02, -4.6260e-01,  ...,  1.8155e-01,\n",
       "           -1.5392e-01,  4.4419e-01],\n",
       "          ...,\n",
       "          [-9.9084e-01, -6.8332e-02, -4.6180e-02,  ..., -7.5207e-02,\n",
       "           -1.4895e-01,  1.9261e-01],\n",
       "          [-9.9089e-01, -6.7374e-02, -4.7013e-02,  ..., -7.5602e-02,\n",
       "           -1.4943e-01,  1.9264e-01],\n",
       "          [-9.9204e-01, -6.7517e-02, -4.7611e-02,  ..., -7.6439e-02,\n",
       "           -1.5051e-01,  1.9204e-01]],\n",
       " \n",
       "         [[-9.5966e-01,  8.3105e-03, -3.2980e-01,  ...,  8.2517e-02,\n",
       "           -1.2838e-03,  2.9957e-01],\n",
       "          [-9.5966e-01,  8.3099e-03, -3.2980e-01,  ...,  8.2517e-02,\n",
       "           -1.2841e-03,  2.9957e-01],\n",
       "          [-9.5966e-01,  8.3094e-03, -3.2980e-01,  ...,  8.2517e-02,\n",
       "           -1.2841e-03,  2.9957e-01],\n",
       "          ...,\n",
       "          [-9.1937e-01, -1.4559e-01, -1.0681e-01,  ..., -4.3610e-02,\n",
       "           -9.8243e-02,  2.2111e-01],\n",
       "          [-9.1728e-01, -1.4615e-01, -1.0637e-01,  ..., -4.3936e-02,\n",
       "           -1.0096e-01,  2.2127e-01],\n",
       "          [-9.1788e-01, -1.4855e-01, -1.1012e-01,  ..., -4.2501e-02,\n",
       "           -1.0045e-01,  2.2177e-01]]]), extract_features=tensor([[[-0.0150,  0.0193, -0.0324,  ...,  0.0079,  0.0037,  0.1372],\n",
       "          [-0.0075,  0.0794, -0.0222,  ..., -0.0229,  0.0599,  0.0383],\n",
       "          [ 0.0390,  0.0452, -0.0463,  ...,  0.0379,  0.0597,  0.1438],\n",
       "          ...,\n",
       "          [-0.0204, -0.0718, -0.0484,  ..., -0.0099, -0.0317,  0.0508],\n",
       "          [-0.0204, -0.0718, -0.0484,  ..., -0.0099, -0.0317,  0.0508],\n",
       "          [-0.0204, -0.0718, -0.0484,  ..., -0.0099, -0.0317,  0.0508]],\n",
       " \n",
       "         [[-0.0063, -0.0017, -0.0410,  ...,  0.0107,  0.0233,  0.1168],\n",
       "          [ 0.0078,  0.0402, -0.0408,  ..., -0.0027,  0.0110,  0.1877],\n",
       "          [-0.0279,  0.0433, -0.0413,  ..., -0.0087,  0.0713,  0.1769],\n",
       "          ...,\n",
       "          [-0.0204, -0.0718, -0.0484,  ..., -0.0099, -0.0317,  0.0508],\n",
       "          [-0.0204, -0.0718, -0.0484,  ..., -0.0099, -0.0317,  0.0508],\n",
       "          [-0.0204, -0.0718, -0.0484,  ..., -0.0099, -0.0317,  0.0508]],\n",
       " \n",
       "         [[-0.0282,  0.0591, -0.0437,  ..., -0.0078,  0.0761,  0.1159],\n",
       "          [-0.0328,  0.0526, -0.0414,  ..., -0.0211,  0.0420,  0.1000],\n",
       "          [-0.0466,  0.0442, -0.0404,  ..., -0.0221,  0.1174,  0.2382],\n",
       "          ...,\n",
       "          [-0.0310,  0.0054, -0.0415,  ..., -0.0073, -0.0198,  0.0947],\n",
       "          [-0.0243,  0.0619, -0.0517,  ..., -0.0171,  0.0623,  0.1965],\n",
       "          [-0.0190, -0.0010, -0.0364,  ...,  0.0052,  0.1083,  0.0590]],\n",
       " \n",
       "         [[-0.0195,  0.0016, -0.0401,  ..., -0.0127,  0.0190,  0.0447],\n",
       "          [-0.0126, -0.0161, -0.0317,  ..., -0.0066,  0.0395,  0.0767],\n",
       "          [ 0.0045, -0.0061, -0.0332,  ...,  0.0091,  0.0311,  0.0611],\n",
       "          ...,\n",
       "          [-0.0204, -0.0718, -0.0484,  ..., -0.0099, -0.0317,  0.0508],\n",
       "          [-0.0204, -0.0718, -0.0484,  ..., -0.0099, -0.0317,  0.0508],\n",
       "          [-0.0204, -0.0718, -0.0484,  ..., -0.0099, -0.0317,  0.0508]]]), hidden_states=None, attentions=None)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 500, 768])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_embedding[1][0]['last_hidden_state'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Dynamic_Mlp.__init__() missing 1 required positional argument: 'input'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m         x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(x)\n\u001b[1;32m     14\u001b[0m         \u001b[39mreturn\u001b[39;00m x\n\u001b[0;32m---> 16\u001b[0m dynamic_mlp \u001b[39m=\u001b[39m Dynamic_Mlp()\n\u001b[1;32m     17\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     18\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mSGD(Dynamic_Mlp\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Dynamic_Mlp.__init__() missing 1 required positional argument: 'input'"
     ]
    }
   ],
   "source": [
    "class Dynamic_Mlp(nn.Module):\n",
    "    \n",
    "    def __init__(self, input, target):\n",
    "        super(Dynamic_Mlp, self).__init__()\n",
    "        self.input = input\n",
    "        self.audio_input = self.input[0]\n",
    "        self.text_input = self.input[1]\n",
    "        self.iteration = len(self.audio_input)\n",
    "        self.target = target\n",
    "        \n",
    "\n",
    "        b, w, d = self.input.shape\n",
    "        self.fc1 = nn.Linear(w * d, 768)\n",
    "        self.rangex = []\n",
    "        self.rangex.append((w, d))\n",
    "        self.fcx = []\n",
    "        \n",
    "        self.fc2 = nn.Linear(768, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        for i in range(self.iteration):\n",
    "            \n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        \n",
    "dynamic_mlp = Dynamic_Mlp()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(Dynamic_Mlp.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 3\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_loss = 0\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    inputs = audio_embedding[i]\n",
    "    targets = target.long()[i * 4 : (i + 1) * 4]\n",
    "    \n",
    "    (b, w, d) = inputs.shape\n",
    "    \n",
    "    running_loss = 0.0\n",
    "\n",
    "    outputs = Dynamic_Mlp(inputs)\n",
    "        \n",
    "    loss = criterion(outputs, targets)\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "    running_loss += loss.item() * batch_size\n",
    "        \n",
    "    epoch_loss = running_loss / num_samples\n",
    "    print(\"Epoch {}, Loss: {}\".format(epoch+1, epoch_loss))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (4, 1235 , 768)\n",
    "x_train = torch.rand(shape)\n",
    "y_train = target\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(mlp_1.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 2\n",
    "batch_size = 1\n",
    "num_samples = x_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1453e+00, -1.3806e-01, -6.6733e-02,  ...,  6.6787e-03,\n",
       "          -7.2099e-02,  2.8462e-01],\n",
       "         [-1.1453e+00, -1.3806e-01, -6.6732e-02,  ...,  6.6784e-03,\n",
       "          -7.2098e-02,  2.8462e-01],\n",
       "         [-1.1453e+00, -1.3806e-01, -6.6733e-02,  ...,  6.6792e-03,\n",
       "          -7.2098e-02,  2.8462e-01],\n",
       "         ...,\n",
       "         [-9.0208e-01, -6.1873e-02, -4.0272e-02,  ..., -1.4839e-01,\n",
       "          -1.7549e-01,  1.7050e-01],\n",
       "         [-9.0822e-01, -5.7904e-02, -4.5461e-02,  ..., -1.5357e-01,\n",
       "          -1.8189e-01,  1.7206e-01],\n",
       "         [-9.1004e-01, -5.6092e-02, -4.8181e-02,  ..., -1.5627e-01,\n",
       "          -1.8337e-01,  1.7411e-01]],\n",
       "\n",
       "        [[-9.5902e-01, -6.7264e-02, -2.1643e-01,  ...,  9.4446e-02,\n",
       "          -7.7794e-04,  4.8156e-01],\n",
       "         [-9.5902e-01, -6.7264e-02, -2.1643e-01,  ...,  9.4447e-02,\n",
       "          -7.7803e-04,  4.8156e-01],\n",
       "         [-9.5902e-01, -6.7264e-02, -2.1643e-01,  ...,  9.4446e-02,\n",
       "          -7.7759e-04,  4.8156e-01],\n",
       "         ...,\n",
       "         [-1.0012e+00,  4.9996e-02, -1.3064e-01,  ..., -1.2703e-01,\n",
       "          -8.2842e-02,  1.5022e-01],\n",
       "         [-1.0109e+00,  5.1292e-02, -1.3564e-01,  ..., -1.2783e-01,\n",
       "          -9.2527e-02,  1.4789e-01],\n",
       "         [-1.0029e+00,  5.1213e-02, -1.3539e-01,  ..., -1.3114e-01,\n",
       "          -9.4374e-02,  1.5265e-01]],\n",
       "\n",
       "        [[-6.3203e-01,  5.8064e-02, -4.6260e-01,  ...,  1.8155e-01,\n",
       "          -1.5392e-01,  4.4419e-01],\n",
       "         [-6.3203e-01,  5.8064e-02, -4.6260e-01,  ...,  1.8155e-01,\n",
       "          -1.5392e-01,  4.4419e-01],\n",
       "         [-6.3203e-01,  5.8064e-02, -4.6260e-01,  ...,  1.8155e-01,\n",
       "          -1.5392e-01,  4.4419e-01],\n",
       "         ...,\n",
       "         [-9.9084e-01, -6.8332e-02, -4.6180e-02,  ..., -7.5207e-02,\n",
       "          -1.4895e-01,  1.9261e-01],\n",
       "         [-9.9089e-01, -6.7374e-02, -4.7013e-02,  ..., -7.5602e-02,\n",
       "          -1.4943e-01,  1.9264e-01],\n",
       "         [-9.9204e-01, -6.7517e-02, -4.7611e-02,  ..., -7.6439e-02,\n",
       "          -1.5051e-01,  1.9204e-01]],\n",
       "\n",
       "        [[-9.5966e-01,  8.3105e-03, -3.2980e-01,  ...,  8.2517e-02,\n",
       "          -1.2838e-03,  2.9957e-01],\n",
       "         [-9.5966e-01,  8.3099e-03, -3.2980e-01,  ...,  8.2517e-02,\n",
       "          -1.2841e-03,  2.9957e-01],\n",
       "         [-9.5966e-01,  8.3094e-03, -3.2980e-01,  ...,  8.2517e-02,\n",
       "          -1.2841e-03,  2.9957e-01],\n",
       "         ...,\n",
       "         [-9.1937e-01, -1.4559e-01, -1.0681e-01,  ..., -4.3610e-02,\n",
       "          -9.8243e-02,  2.2111e-01],\n",
       "         [-9.1728e-01, -1.4615e-01, -1.0637e-01,  ..., -4.3936e-02,\n",
       "          -1.0096e-01,  2.2127e-01],\n",
       "         [-9.1788e-01, -1.4855e-01, -1.1012e-01,  ..., -4.2501e-02,\n",
       "          -1.0045e-01,  2.2177e-01]]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0][0][0]['last_hidden_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1453e+00, -1.3806e-01, -6.6733e-02,  ...,  6.6787e-03,\n",
       "          -7.2099e-02,  2.8462e-01],\n",
       "         [-1.1453e+00, -1.3806e-01, -6.6732e-02,  ...,  6.6784e-03,\n",
       "          -7.2098e-02,  2.8462e-01],\n",
       "         [-1.1453e+00, -1.3806e-01, -6.6733e-02,  ...,  6.6792e-03,\n",
       "          -7.2098e-02,  2.8462e-01],\n",
       "         ...,\n",
       "         [-9.3663e-01, -5.5293e-02, -6.8097e-02,  ..., -1.5364e-01,\n",
       "          -1.4114e-01,  3.0419e-01],\n",
       "         [-9.3495e-01, -5.3199e-02, -6.8471e-02,  ..., -1.5613e-01,\n",
       "          -1.4451e-01,  3.0472e-01],\n",
       "         [-9.3474e-01, -5.6793e-02, -6.9754e-02,  ..., -1.5615e-01,\n",
       "          -1.4653e-01,  3.0076e-01]],\n",
       "\n",
       "        [[-9.5902e-01, -6.7264e-02, -2.1643e-01,  ...,  9.4446e-02,\n",
       "          -7.7794e-04,  4.8156e-01],\n",
       "         [-9.5902e-01, -6.7264e-02, -2.1643e-01,  ...,  9.4447e-02,\n",
       "          -7.7803e-04,  4.8156e-01],\n",
       "         [-9.5902e-01, -6.7264e-02, -2.1643e-01,  ...,  9.4446e-02,\n",
       "          -7.7759e-04,  4.8156e-01],\n",
       "         ...,\n",
       "         [-6.7408e-01, -1.5964e-01, -5.4855e-02,  ..., -1.3499e-01,\n",
       "          -1.4991e-01,  2.7362e-01],\n",
       "         [-6.6732e-01, -1.6485e-01, -4.5102e-02,  ..., -1.3650e-01,\n",
       "          -1.5365e-01,  2.6874e-01],\n",
       "         [-6.7186e-01, -1.6027e-01, -5.3782e-02,  ..., -1.3490e-01,\n",
       "          -1.5317e-01,  2.7249e-01]],\n",
       "\n",
       "        [[-6.3203e-01,  5.8064e-02, -4.6260e-01,  ...,  1.8155e-01,\n",
       "          -1.5392e-01,  4.4419e-01],\n",
       "         [-6.3203e-01,  5.8064e-02, -4.6260e-01,  ...,  1.8155e-01,\n",
       "          -1.5392e-01,  4.4419e-01],\n",
       "         [-6.3203e-01,  5.8064e-02, -4.6260e-01,  ...,  1.8155e-01,\n",
       "          -1.5392e-01,  4.4419e-01],\n",
       "         ...,\n",
       "         [-6.9692e-01, -1.5726e-01, -4.0159e-02,  ..., -1.3434e-01,\n",
       "          -8.7844e-02,  2.7122e-01],\n",
       "         [-6.9623e-01, -1.5768e-01, -4.3292e-02,  ..., -1.3562e-01,\n",
       "          -8.9875e-02,  2.7151e-01],\n",
       "         [-6.9810e-01, -1.5583e-01, -4.5407e-02,  ..., -1.3879e-01,\n",
       "          -9.6007e-02,  2.6739e-01]],\n",
       "\n",
       "        [[-9.5966e-01,  8.3105e-03, -3.2980e-01,  ...,  8.2517e-02,\n",
       "          -1.2838e-03,  2.9957e-01],\n",
       "         [-9.5966e-01,  8.3099e-03, -3.2980e-01,  ...,  8.2517e-02,\n",
       "          -1.2841e-03,  2.9957e-01],\n",
       "         [-9.5966e-01,  8.3094e-03, -3.2980e-01,  ...,  8.2517e-02,\n",
       "          -1.2841e-03,  2.9957e-01],\n",
       "         ...,\n",
       "         [-1.0920e+00, -1.2600e-01,  5.6552e-02,  ..., -7.1258e-02,\n",
       "          -1.4981e-01,  2.8537e-02],\n",
       "         [-1.0923e+00, -1.2641e-01,  5.7477e-02,  ..., -7.1126e-02,\n",
       "          -1.4941e-01,  2.8387e-02],\n",
       "         [-1.0916e+00, -1.2651e-01,  5.6120e-02,  ..., -7.0701e-02,\n",
       "          -1.4733e-01,  2.9437e-02]]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.concat((a[0][0][0]['last_hidden_state'], a[0][1][0]['last_hidden_state']), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 1])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_758714/3510190268.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_train = torch.tensor(torch.concat((a[0][0][0]['last_hidden_state'], a[0][1][0]['last_hidden_state']), dim=1))\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.tensor(torch.concat((a[0][0][0]['last_hidden_state'], a[0][1][0]['last_hidden_state']), dim=1))\n",
    "y_train = target.long()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 2\n",
    "batch_size = 1\n",
    "num_samples = x_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ')' (3127040863.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[122], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    x_train = torch.tensor(a[0][0], text_outputs['last_hidden_state']), dim=1))\u001b[0m\n\u001b[0m                                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unmatched ')'\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.tensor(a[0][0], text_outputs['last_hidden_state']), dim=1\n",
    "y_train = target\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 2\n",
    "batch_size = 1\n",
    "num_samples = x_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x1142784 and 120x84)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[127], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m inputs \u001b[39m=\u001b[39m x_train[i:i\u001b[39m+\u001b[39mbatch_size]\n\u001b[1;32m      8\u001b[0m targets \u001b[39m=\u001b[39m y_train[i:i\u001b[39m+\u001b[39mbatch_size]\n\u001b[0;32m---> 10\u001b[0m outputs \u001b[39m=\u001b[39m net(inputs)\n\u001b[1;32m     12\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     14\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/project/paradeigma/multi_modal/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[126], line 12\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     11\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc2(x))\n\u001b[1;32m     13\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc3(x)\n\u001b[1;32m     14\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/project/paradeigma/multi_modal/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/project/paradeigma/multi_modal/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x1142784 and 120x84)"
     ]
    }
   ],
   "source": [
    "epoch_loss = 0\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        print(i)\n",
    "        inputs = x_train[i:i+batch_size]\n",
    "        targets = y_train[i:i+batch_size]\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * batch_size\n",
    "        \n",
    "    epoch_loss = running_loss / num_samples\n",
    "    print(\"Epoch {}, Loss: {}\".format(epoch+1, epoch_loss))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37632"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "49 * 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "16 * 62976 / 37632 * 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.125643518555141e+19"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m y_train \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, (\u001b[39m1\u001b[39m,\u001b[39m10\u001b[39m))\n\u001b[1;32m      4\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mMSELoss()\n\u001b[0;32m----> 5\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mSGD(net\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n\u001b[1;32m      7\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n\u001b[1;32m      8\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "x_train = torch.randint(0, 1, (1,120))\n",
    "y_train = torch.randint(0, 1, (1,10))\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 5\n",
    "batch_size = 1\n",
    "num_samples = x_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "                          \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m net(x_train)\n",
      "File \u001b[0;32m~/project/paradeigma/multi_modal/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[66], line 11\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 11\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc2(x))\n\u001b[1;32m     12\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc3(x))\n\u001b[1;32m     13\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/project/paradeigma/multi_modal/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/project/paradeigma/multi_modal/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype"
     ]
    }
   ],
   "source": [
    "net(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.8839\n",
      "Epoch 2 Loss: 0.8758\n",
      "Epoch 3 Loss: 0.8678\n"
     ]
    }
   ],
   "source": [
    "# 모델 정의\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 데이터셋 생성\n",
    "x_train = torch.randn((30, 10))\n",
    "y_train = torch.randn((30, 5))\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "input_dim = x_train.shape[1]\n",
    "hidden_dim = 20\n",
    "output_dim = y_train.shape[1]\n",
    "lr = 0.01\n",
    "num_epochs = 3\n",
    "batch_size = 5\n",
    "\n",
    "# 모델 초기화\n",
    "model = SimpleModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# 손실함수 정의\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 옵티마이저 정의\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# 훈련\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        # 배치 데이터 로드\n",
    "        inputs = x_train[i:i+batch_size]\n",
    "        targets = y_train[i:i+batch_size]\n",
    "        \n",
    "        # 순전파\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # 역전파\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 손실값 누적\n",
    "        running_loss += loss.item() * batch_size\n",
    "    \n",
    "    # 에폭별 손실값 출력\n",
    "    print('Epoch {} Loss: {:.4f}'.format(epoch+1, running_loss/num_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1453e+00, -1.3806e-01, -6.6733e-02,  ...,  6.6787e-03,\n",
       "          -7.2099e-02,  2.8462e-01],\n",
       "         [-1.1453e+00, -1.3806e-01, -6.6732e-02,  ...,  6.6784e-03,\n",
       "          -7.2098e-02,  2.8462e-01],\n",
       "         [-1.1453e+00, -1.3806e-01, -6.6733e-02,  ...,  6.6792e-03,\n",
       "          -7.2098e-02,  2.8462e-01],\n",
       "         ...,\n",
       "         [-1.7483e-02, -5.4983e-03, -6.0314e-02,  ...,  1.3965e-02,\n",
       "          -1.4430e-01,  1.0852e-01],\n",
       "         [-1.7483e-02, -5.4983e-03, -6.0314e-02,  ...,  1.3965e-02,\n",
       "          -1.4430e-01,  1.0852e-01],\n",
       "         [-1.7483e-02, -5.4984e-03, -6.0314e-02,  ...,  1.3965e-02,\n",
       "          -1.4430e-01,  1.0852e-01]],\n",
       "\n",
       "        [[-9.5902e-01, -6.7264e-02, -2.1643e-01,  ...,  9.4446e-02,\n",
       "          -7.7794e-04,  4.8156e-01],\n",
       "         [-9.5902e-01, -6.7264e-02, -2.1643e-01,  ...,  9.4447e-02,\n",
       "          -7.7803e-04,  4.8156e-01],\n",
       "         [-9.5902e-01, -6.7264e-02, -2.1643e-01,  ...,  9.4446e-02,\n",
       "          -7.7759e-04,  4.8156e-01],\n",
       "         ...,\n",
       "         [ 7.2260e-02, -2.2458e-02, -4.9515e-02,  ...,  3.4100e-02,\n",
       "          -1.3729e-01,  1.2640e-01],\n",
       "         [ 7.2260e-02, -2.2457e-02, -4.9515e-02,  ...,  3.4100e-02,\n",
       "          -1.3729e-01,  1.2640e-01],\n",
       "         [ 7.2260e-02, -2.2458e-02, -4.9515e-02,  ...,  3.4100e-02,\n",
       "          -1.3729e-01,  1.2640e-01]],\n",
       "\n",
       "        [[-6.3203e-01,  5.8064e-02, -4.6260e-01,  ...,  1.8155e-01,\n",
       "          -1.5392e-01,  4.4419e-01],\n",
       "         [-6.3203e-01,  5.8064e-02, -4.6260e-01,  ...,  1.8155e-01,\n",
       "          -1.5392e-01,  4.4419e-01],\n",
       "         [-6.3203e-01,  5.8064e-02, -4.6260e-01,  ...,  1.8155e-01,\n",
       "          -1.5392e-01,  4.4419e-01],\n",
       "         ...,\n",
       "         [-3.0071e-02,  1.8067e-01, -9.1326e-02,  ..., -1.0799e-01,\n",
       "          -2.5392e-02,  8.5111e-02],\n",
       "         [ 8.8385e-03,  1.9198e-01, -4.5030e-02,  ..., -6.2464e-02,\n",
       "           3.2973e-02,  1.1441e-01],\n",
       "         [ 4.1969e-02,  2.0687e-02, -2.8678e-02,  ..., -1.4005e-01,\n",
       "          -1.3668e-02,  1.6395e-01]],\n",
       "\n",
       "        [[-9.5966e-01,  8.3105e-03, -3.2980e-01,  ...,  8.2517e-02,\n",
       "          -1.2838e-03,  2.9957e-01],\n",
       "         [-9.5966e-01,  8.3099e-03, -3.2980e-01,  ...,  8.2517e-02,\n",
       "          -1.2841e-03,  2.9957e-01],\n",
       "         [-9.5966e-01,  8.3094e-03, -3.2980e-01,  ...,  8.2517e-02,\n",
       "          -1.2841e-03,  2.9957e-01],\n",
       "         ...,\n",
       "         [-1.0426e-01, -7.0433e-02, -7.9854e-02,  ..., -2.2678e-02,\n",
       "           1.0606e-01,  5.1207e-02],\n",
       "         [-1.0426e-01, -7.0433e-02, -7.9854e-02,  ..., -2.2678e-02,\n",
       "           1.0606e-01,  5.1207e-02],\n",
       "         [-1.0426e-01, -7.0433e-02, -7.9854e-02,  ..., -2.2678e-02,\n",
       "           1.0606e-01,  5.1207e-02]]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7589, -0.2451,  0.3043, -1.1257, -0.7017, -0.5993,  1.4000,  0.9494,\n",
       "         -0.0170, -0.0424],\n",
       "        [-0.8227,  0.0786, -0.7696,  0.7970,  2.1975,  0.3471,  0.3051, -1.5030,\n",
       "          1.1239,  0.3276],\n",
       "        [ 1.1657, -0.7739,  1.4156, -1.7199,  1.0934,  0.5445, -0.7570, -1.4942,\n",
       "         -1.0362,  0.5432],\n",
       "        [-0.2782, -1.2190,  1.0431,  0.0808, -0.1665,  0.7406, -1.2880,  0.7423,\n",
       "          0.8216,  1.6441],\n",
       "        [ 0.3207,  0.8375,  0.5336,  1.1942,  0.5790,  0.2433, -1.0244, -0.9470,\n",
       "         -0.0452, -0.8878],\n",
       "        [ 0.8417,  0.2664,  0.5194,  0.7736, -1.3530, -1.0820,  0.6434, -0.9518,\n",
       "         -0.4851, -0.5505],\n",
       "        [ 0.1473,  2.3938, -0.8641,  0.2462,  0.0651,  0.2549, -0.9982, -0.8814,\n",
       "         -0.0098,  1.5151],\n",
       "        [ 0.5930, -0.8168, -0.8172, -0.1530,  0.0569,  0.6663, -0.3458, -1.2687,\n",
       "         -0.3133, -0.4569],\n",
       "        [-0.9944, -0.7339, -0.1201, -0.6393,  0.7754, -1.3064, -0.4372, -0.0418,\n",
       "         -0.4973, -0.1493],\n",
       "        [-0.4678, -0.9364, -0.4461,  0.3912, -1.4863,  1.3704, -1.1609, -0.8786,\n",
       "         -0.6707, -0.8626],\n",
       "        [ 0.3724,  0.5360, -0.4087, -0.0558, -0.0507,  0.4615, -0.5196, -1.8035,\n",
       "         -0.0698,  0.8305],\n",
       "        [ 2.4892, -0.2987,  1.9883, -0.2630,  0.7044, -0.3708, -1.8317, -0.7665,\n",
       "         -0.7574, -1.4376],\n",
       "        [-0.0712,  1.1290, -0.3818,  1.1290, -2.0891, -0.8861,  0.3131,  1.5800,\n",
       "          0.6626, -1.6276],\n",
       "        [ 1.4915,  0.4752, -0.3868, -0.5807,  1.0075, -0.2444,  0.7096,  0.4142,\n",
       "          0.5415,  1.3997],\n",
       "        [ 0.0081,  1.9106,  0.0344,  0.5899,  0.1351,  0.0696, -1.5236, -0.1285,\n",
       "         -0.7277,  0.4702],\n",
       "        [-1.2165,  0.2231, -0.1695, -0.6135,  2.1242,  0.4438, -0.4490, -1.8190,\n",
       "          0.7870, -0.1732],\n",
       "        [-0.1803, -1.0523, -0.1428,  1.4736, -0.9674, -2.0183, -0.2337, -0.3959,\n",
       "         -1.1572,  0.6013],\n",
       "        [ 0.3881, -0.1020,  1.2568, -1.2618, -1.6121, -2.5220,  0.6814, -0.4900,\n",
       "         -0.5767,  0.1199],\n",
       "        [ 0.0362, -0.6409,  0.2485, -0.7908, -0.4876, -0.2191,  0.9571,  0.2600,\n",
       "         -0.8580, -0.6262],\n",
       "        [ 0.6827, -0.4280,  0.0962, -1.2233, -1.1323,  0.6629, -0.8648,  0.0961,\n",
       "          1.4262,  1.8084],\n",
       "        [ 0.4237,  0.6470,  0.2201,  1.5339,  0.2819, -0.4238, -0.5199,  0.6771,\n",
       "          0.6909, -0.7824],\n",
       "        [-0.7740, -0.1863, -1.9577, -0.9537, -0.5329,  1.7583,  0.9565,  0.8781,\n",
       "          2.4277, -1.3034],\n",
       "        [-1.2643,  1.0173,  0.9665,  1.0494,  0.0647, -0.8131,  0.0911, -0.6157,\n",
       "          0.1404,  0.4604],\n",
       "        [-0.8960,  0.0768, -0.9132,  1.9220,  1.1196, -1.1806, -0.5477, -1.1084,\n",
       "          0.4830,  0.8057],\n",
       "        [-1.4615, -1.1441, -0.5810,  1.2424,  1.5238,  0.7062, -0.8846,  0.2243,\n",
       "          0.0253,  0.7271],\n",
       "        [ 0.4301, -0.1797,  1.1007, -1.4457, -1.0218,  0.1792, -0.5069, -0.1566,\n",
       "         -0.5936,  1.9436],\n",
       "        [-1.3451, -1.4461,  0.6019, -0.2457,  0.3002, -0.2187,  0.1682,  0.6909,\n",
       "         -1.4757,  0.5196],\n",
       "        [ 0.4629, -0.3864,  1.4337,  1.4364,  0.3738, -1.1027,  1.2089,  0.4500,\n",
       "          0.2582, -1.0971],\n",
       "        [-0.0295,  1.0087, -0.9352, -0.7352,  0.4715,  0.5272, -0.4059,  0.1124,\n",
       "          0.2835,  0.8462],\n",
       "        [-0.2852,  0.8242,  0.8234,  0.1393,  0.3209, -0.3744, -1.7069, -0.8258,\n",
       "         -0.9010,  0.0488]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
