{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arplab/project/paradeigma/multi_modal/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoProcessor, Data2VecAudioModel\n",
    "from transformers import AutoTokenizer, Data2VecTextModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset, Audio\n",
    "import datasets\n",
    "import glob\n",
    "import collections\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_path(session_num, modal):\n",
    "# modal : wav, txt 둘 중 하나 입력\n",
    "# 이렇게 불러온 데이터는 시간 순은 아님을 유의\n",
    "    if session_num <= 9:\n",
    "        dir_path = '/home/arplab/project/paradeigma/multi_modal/org_KEMDy20/Session0' + str(session_num)\n",
    "    else:\n",
    "        dir_path = '/home/arplab/project/paradeigma/multi_modal/org_KEMDy20/Session' + str(session_num)\n",
    "    data_path = glob.glob(dir_path + '/*.' + modal)\n",
    "    data_path = sorted(data_path)\n",
    "    return data_path\n",
    "\n",
    "def make_audio_datasets(num_sessions):\n",
    "# num_sessions : from 1 to 40\n",
    "# type(return) = dict{1:audio_dataset,...,40:audio_dataset}\n",
    "    audio_datasets = {}\n",
    "    \n",
    "    for i in tqdm(range(1, num_sessions + 1)):\n",
    "        # audio_datasets[i] = datasets.Dataset.from_dict({'audio': data_path(i, 'wav')})\n",
    "        audio_datasets[i] = datasets.Dataset.from_dict({'audio': data_path(i, 'wav')}).cast_column(\"audio\", Audio())\n",
    "    return audio_datasets\n",
    "\n",
    "def make_text_datasets(num_sessions):\n",
    "# num_sessions : from 1 to 40\n",
    "# type(return) = dict{1:[paths, corpus],...,40:[paths, corpus]}\n",
    "    text_datasets = {}\n",
    "    \n",
    "    for i in tqdm(range(1, num_sessions + 1)):\n",
    "        \n",
    "        paths = []\n",
    "        corpus = []\n",
    "        \n",
    "        for path in data_path(i, 'txt'):\n",
    "            paths.append(path)\n",
    "            with open(path, \"r\") as f:\n",
    "                corpus.append(f.read())\n",
    "        \n",
    "        text_datasets[i] = [paths, corpus]\n",
    "\n",
    "    return text_datasets  \n",
    "\n",
    "def audio_embedding(audio_datasets, batch_size=32):\n",
    "# audio_datasets = {1:audio_dataset, ... , 40:audio_dataset}\n",
    "    batch_size = batch_size\n",
    "    \n",
    "    audio_processed_dict = {}\n",
    "    for session, audio_dataset in audio_datasets.items():\n",
    "        \n",
    "        processed_audio_by_session = []\n",
    "        for i in range(0, audio_dataset.num_rows, batch_size):\n",
    "            \n",
    "            audio_arrayes = []\n",
    "            for j in audio_dataset[i:i+batch_size]['audio']:\n",
    "                audio_arrayes.append(j['array'])\n",
    "            \n",
    "            inputs = processor(audio_arrayes, sampling_rate=sampling_rate, padding=True, max_length=400, return_attention_mask=True, return_tensors=\"pt\")\n",
    "            processed_audio_by_session.append(inputs)\n",
    "        \n",
    "        audio_processed_dict[session] = processed_audio_by_session\n",
    "\n",
    "    \n",
    "# audio_processed_dict : {1 : [[batch 당 processing][batch 당 processing][batch 당 processing]],2,3,}    \n",
    "    audio_embedded_dict = {}        \n",
    "    for session, val_dic_list in tqdm(audio_processed_dict.items()):\n",
    "        \n",
    "        val_dic_by_session = []\n",
    "        for val_dic in val_dic_list:\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = audio_d2v(**val_dic)\n",
    "            val_dic_by_session.append(outputs)    \n",
    "            \n",
    "        audio_embedded_dict[session] = val_dic_by_session\n",
    "# audio_embedded_dict = {1:embedded_data_by_session, ..., 40:embedded_data_by_session}    \n",
    "    \n",
    "    return audio_embedded_dict\n",
    "\n",
    "def text_embedding(text_datasets, batch_size=32):\n",
    "# text_dataset = {1:[path, corpus], ..., 40:[path, corpus]}    \n",
    "    \n",
    "    text_embedded_dict = {}\n",
    "    \n",
    "    for session, ( _ , corpus) in tqdm(text_datasets.items()):\n",
    "        \n",
    "        embedded_data_by_batch = []\n",
    "        print(len(corpus))\n",
    "        for i in range(0, len(corpus), batch_size):\n",
    "            \n",
    "            corpus_by_batch = corpus[i:i+batch_size]\n",
    "            inputs = tokenizer(corpus_by_batch, padding= True, max_length = 35, return_tensors=\"pt\")\n",
    "            outputs = text_d2v(**inputs)\n",
    "            embedded_data_by_batch.append(outputs)\n",
    "            \n",
    "        text_embedded_dict[session] = embedded_data_by_batch\n",
    "# text_embedded_dict = {1:embedded_data_by_session, ..., 40:embedded_data_by_session}         \n",
    "    return text_embedded_dict\n",
    "\n",
    "def make_target_dataframe(session_num):\n",
    "# session_num : from 1 to 40\n",
    "\n",
    "    if session_num <= 9:\n",
    "        target_path = '/home/arplab/project/paradeigma/multi_modal/org_KEMDy20/annotation/Sess0' + str(session_num) + '_eval.csv'\n",
    "    \n",
    "    else:\n",
    "        target_path = '/home/arplab/project/paradeigma/multi_modal/org_KEMDy20/annotation/Sess' + str(session_num) + '_eval.csv'\n",
    "\n",
    "    train = pd.read_csv(target_path)\n",
    "    train = train[['Segment ID', 'Total Evaluation',' .1',' .2']]\n",
    "    train.columns = ['segment_id','emotion','valence','arousal']\n",
    "    train = train.drop([0], axis = 0)\n",
    "    train = train.sort_values('segment_id', ascending=True)\n",
    "    train = train.reset_index(drop=True)\n",
    "    \n",
    "    return train\n",
    "\n",
    "def make_target_dict(num_sessions):\n",
    "# num_sessions : from 1 to 40\n",
    "\n",
    "    target_dict = {}\n",
    "    for i in tqdm(range(1, num_sessions + 1)):\n",
    "        target_data = {}\n",
    "        target_dataframe = make_target_dataframe(i)\n",
    "        columns = target_dataframe.columns\n",
    "        \n",
    "        for j in columns:\n",
    "            target_data[j] = target_dataframe[j]\n",
    "\n",
    "        target_dict[i] = target_data\n",
    "        \n",
    "# target_dict = {1:{segment_id:_,emotion:_,valence:_, arousal:_}, ..., 40: {segment_id:_,emotion:_,valence:_, arousal:_}}        \n",
    "    return target_dict\n",
    "            \n",
    "def tensor_fusion(num):\n",
    "\n",
    "    fusion = torch.outer(audio_outputs[num][0,:,0], text_outputs[num][0,:,0])\n",
    "    a,b = fusion.shape\n",
    "    fusion = fusion.view([a, b, -1])\n",
    "    \n",
    "    for i in tqdm(range(1, 768)):\n",
    "        fusion_1 = torch.outer(audio_outputs[num][0,:,i], text_outputs[num][0,:,i])\n",
    "        a,b = fusion_1.shape\n",
    "        fusion_1 = fusion_1.view([a, b, -1])\n",
    "        fusion = torch.concat((fusion, fusion_1), dim=2)\n",
    "        \n",
    "    return fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/data2vec-audio-base-960h were not used when initializing Data2VecAudioModel: ['lm_head.bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing Data2VecAudioModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Data2VecAudioModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at facebook/data2vec-text-base were not used when initializing Data2VecTextModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing Data2VecTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Data2VecTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Data2VecTextModel were not initialized from the model checkpoint at facebook/data2vec-text-base and are newly initialized: ['data2vec_text.pooler.dense.weight', 'data2vec_text.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"facebook/data2vec-audio-base-960h\")\n",
    "audio_d2v = Data2VecAudioModel.from_pretrained(\"facebook/data2vec-audio-base-960h\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/data2vec-text-base\")\n",
    "text_d2v = Data2VecTextModel.from_pretrained(\"facebook/data2vec-text-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "['n/ 아 친구들도? l/\\n', 'l/ 나는 생일?\\n', '생일날이면은 b/ 내가 고기를 되게 좋아하니까 엄마도 그걸 아니까 미역국도 같이 해주시는데 꼭 고기를 같이 구워주시거나 아니면 여행을 가서 고기를 구워 먹는다거나 꼭 고깃집에 가서 어 해먹어, 꼭.\\n', 'b/ 선물이라 이 보통 돈으로 주시지. l/\\n', 'l/ 용돈으로 주시고.\\n', '옛날에는 n/ b/ 내가 원하는 게 뭔지 일단 물어보시곤 했던 거 같애.\\n', 'c/ 옷이라든가 갖고 싶은 거 있나라든가 n/ 근데 이제 b/ 나이를 먹을수록 현금으로 챙겨주시더라구.l/\\n', '응 나도 되게 b/ 여행 가는 걸 좋아하긴 하는데 가족 다 같이 가는 것보다 되게 엄마랑 여행 가는 게 많았던 거 같아. \\n', '어 단둘이.\\n', '그 되게 아빠나 오빠가 시간이 안 나거나 꼭 b/ 어디 약속이 있다거나 그래가지고 엄마가 주말마다 좀 같이 둘이 어디 데려다가 줬어, 그냥.\\n', '여행 여행을.\\n', 'b/ 최근 최근에?\\n', 'b/ 굳이 가자면 나는 부산에 가면 꼭 엄마가 생각나.\\n', '어 부산을 좀 가기도 많이 갔는데 부산에 엄마가 좀 n/ 그 친척들이 u/ 가지고.\\n', '아 n/ 되게 되게 처음이라 엉망진창이었던 거 같긴 한데.\\n', '나는 어 어머니, 엄마가 이제 생일 때마다 이제 미역국도 많이 해서 미역국을 되게 좋아하거든.\\n', '전에도 말했지만 그래서 미역국이랑 볶음김치 막 해주셔서 맨날 그렇게 차려주시는데 어 옛날에는 이제 집에서 밥 먹는 거 보다 그 캔모아 빙수 있는데 있잖아.\\n', 'c/ 어 응 거기를 되게 엄 집 앞에 그 조그만한 가게로 있었는데 거기를 엄청 생일 때마다 매번 같이 데려갔었거든.\\n', '거기서 엄마랑 둘이 생일 파티 그냥 간단하게 하는 경우도 있었고 b/ 보 음 보통 그렇게 아니면 친구들 이제 거기 빙수 가게로 불러서 같이 빙수 먹으면서 케익도 불고 막 이랬었었어.\\n', '응응 그래서 생일 때 그렇게 했던 경험이 있는데 그때가 이제 초등학교 때였는데 이제 중학교 때쯤 가게가 문을 닫았거든, 집 앞이었는데.\\n', '그래서 이제 막 지역 돌아다니면서 캔모아 빙수 있는 가게 저번에 광주 갔을 때도 있었 갔었잖아.\\n', 'c/ 어 그래서 그런 가게들을 갈 때면 맨날 엄마 생각이 되게 자주 났어.\\n', '그래서 일부러 보이면 일부러 가기도 해.\\n', 'c/ 거기 그때 먹었던 빙수 맛이 그대로인데 되게 맛있었거든.\\n', '그래서 일부러 좀 자주 일부러 찾아서 가는 경향도 좀 있는 거 같애, 옛날 추억도 있고.\\n', 'u/ 거기 가면 꼭 먹는 게 떡볶이 막 구멍 나 있는 거랑 그 빙수랑 딱 같이 두 개는 무조건 시켜서 먹는 거 같애.\\n', '그래서 되게 그거 말고도 뭐 선물 같은 거는 u/ 어머니가 막 해주셔?\\n', '해주셨었어?\\n', '어 용돈으로?\\n', 'c/ 나도 옛날에는 막 사고 싶었던 물건들 막 주셨는데 지금 보통 용돈으로 주시는 거 같애, 거의 고등학생 이후로는.\\n', '그래가지고 좀 b/ 뭔가 그 생각도 들어.\\n', 'c/ 뭔가 나를 기념해 주는 선물을 받으면 또 어머니 생각이 많이 날 거 같기도 해.\\n', 'c/ 옛날에 옛날에 막 그 뭐지?\\n', 'c/ 요번에 나 어머니 생신 때 그 편지 써줬거든, 책으로 된 거 그걸로 드렸었거든.\\n', '그래서 그거 쓰면서도 되게 어머니 생각이 되게 많이 났던 거 같애.\\n', '응, 그거 말고도 뭐 여행 가는 거나 우리 집은 되게 여행을 많이 갔었거든.\\n', 'c/ 다른 친 주변 친구들에 비해서.\\n', '그래서 나는 그게 당연한 건 줄 알았는데 그게 다 사진으로 기록이 되고 남으니까 되게 좋은 거 같더라.\\n', 'c/ 사진을 막 사진첩 열면 계속 어 엄마나 아버지나 형 생각도 계속 나는 거 같애.\\n', '이렇게 떨어져 있다 보니까.\\n', '아 단둘이 그러면?\\n', 'c/ 단 둘이 여행 가는 거야?\\n', '그럼 어디 가면 생각나는 그런 게 있어?\\n', 'c/ 어 최근에 어디를 가면?\\n', '아 부산을 가면?\\n', '나도 좀 그 울산 쪽이거든, 부산 옆에 이제 그러다 보니까 그쪽도 자주 가는 거 같애.\\n', 'c/ 그래서 아버지는 되게 생선 엄청 좋아하시고 u/ 어머니랑 가 어머니가 자주 고기 생선 구워서 주시거든.\\n', '그래갖고 그것도 막 생각 나는 거 같애.\\n', 'u/ 집 밥 먹을 때가 제일 많이 좋은 거 같은데 요새 이제 익산에서 지내다 보니까 u/ 맨날 우리 나 배달 시켜 먹고 밖에서 먹고 맨날 이러잖아.\\n', 'c/ 그래서 b/ 되게 많이 생각 나는 거 같애.\\n', 'c/ 그래서 내 생일날 가영이가 이제 미역국이랑 막 해줬을 때 되게 좋았었어.\\n', '생각이 많이 났어.\\n', '그래서 뭔가 다음번에 같이 만들면 더 맛있게 만들 수 있지 않을까라는 생각이 들었었어.\\n', '응 그래서 한 번 나중에 가영이 생일 때 내년에 도전을 해봐야겠다. l/\\n', '그 꼰대에 특징을 갖고 있다거나 막 그런 거? l/\\n', 'b/ 근데 나는 약간 그니까 어떤 사람한테는 꼰대로 느껴질 수 있긴 한데 솔직히 이게 다 들어 보면은 되게 좋은 말이고 다 필요한 말이거든?\\n', '근데 그게 좀 듣기 싫을 수가 있으니까 그게 꼰대처럼 느껴지는 거지.\\n', '나는 괜찮았다고 생각했어, 그게.\\n', '그니까 u/ 그니까 오빠를 모르던 모르는 사람이 오빠가 막 나때는 이러이러 했는데 너는 그렇게 하는 게 좋지 않을까 하고 얘기를 하면은.\\n', '그니까 이 사람 꼰대인가라는 생각이 들 수 있긴 한데 나는 오빠를 알고 있으니까 u/ 좋은 의도로 말을 하는 거나 하는 걸 알고 있으니까 꼰대라고는 생각은 안 들었어.\\n', '위에 사람한테? l/ \\n', '근데 나는 그렇게 사람을 많이 만나보는 경험이 없어가지고 특히 위에 사람을 만나는 경우 거의 없었거든?\\n', '그니까 회식 자리를 가도 그런 별로 대화를 많이 나눈 편은 아니였어서 그런 경우는 없었는데 b/ 근데 위에 사람한테 물 좀 달라고 한다는 거는 좀 정중하게는 얘기한 적 있었던 거 같애.\\n', '어 그거는 좀 문제가 있다. l/\\n', 'l/ 내가 갈 수 있는데 굳이 윗사람한테 너가 갔다 와라. l/\\n', '약간 나는 그거면은 문제가 있다고 생각해.\\n', 'c/ 그니까 조언이나 그런 게 아니라 내 자랑만 하고 뭔가 나를 좀 띄워주면서 얘기를 하고 되게 좀 좋은 의도로 말을 하지도 않으면서 좀 후배들을 부리려고 하는 사람?\\n', '그런 건 문제가 있다고 생각해.\\n', '응, 어 나는 그 꼰대라는 개념을 좀 대학교에 처음 들어왔을 때 어 처음 처음 느꼈던 거 같애.\\n', 'c/ 그 b/ 막 대학교 처음 들어 와서 이제 막 학생회 활동하고 과 동아리 활동하고 이렇게 했는데 학생회가 주로 술을 되게 자주 먹었었거든.\\n', '응 u/ 그러다 보니까 이제 술자리에서 어 막 진짜 나때는 말이야라는 말을 진짜 많이 들었었어, 그때 당시에. b/\\n', '그래갖고 어 이런 이런 자리에서는 이렇게 하는 거야라고 뭔가 조언을 해주시는데 그때 그때 당시에는 응 그냥 그게 처음 먹는 어떻게 보면 첫 접하는 사회에서의 술자리다 보니까 위 내 윗분들이랑 부모님이랑 먹는 거 말고.\\n', '그래서 되게 그런 거에 대한 개념이 없었는데 어 나중에 이제 1학년이 끝나고 나서 좀 아 저 사람들이 되게 꼰대구나 되게 이런 거를 많이 느 그때 이제 알고 나니까 이제 알겠더라고. b/\\n', '그래서 나는 어 나는 저러지 말아야지라는 생각을 많이 했었어, 그때 당시.\\n', '그러고 이제 군대를 갔다 오니까 어 오 오히려 나도 나도 조금 꼰대가 돼 있는 거 같더라고, 방금 그 영상을 보미 보니까. b/\\n', '응 u/ 혹시 가영이가 나를 봤을 때는 그런 그런 거 같은 걸 못 느꼈어?\\n', '어 b/ 뭔가 그게 되게 조금의 차이, 생각의 차인 거 같애.\\n', 'c/ 이게 잔 부모님으로 치면 뭔가 잔소리냐 아니면 그 조언이냐 이 차인 거 같애.\\n', '그냥 되게 그거 느낌이 좀 셌었어.\\n', '그래서 군대를 갔다 오니까 이제 대부분에 사람들이 무얼 꼰대라고 하는지 좀 알게 됐어.\\n', 'c/ 그게 되게 군대 갔다 오면 되게 그게 심해지더라구.\\n', '보통 이제 위에 사람이 아래 층 아래 있는 애들을 시키잖아, 군대에선.\\n', '그래서 그 경향이 되게 심하게 보이더라구.\\n', 'c/ 쟤는 군대 안 갔다 와서 아직 모르는구나 이런 친구들이 정말 많아.\\n', '어 특히 남자 u/ 남자 애들 보면.\\n', '어 b/ 그래서 아까 같은 경우 그 대화 보면은 술자리에 가면 뭐 음식을 같이 먹으러 갔는데 어 위에 사람한테 어 저 물 좀 갖다주시겠어요 이 말이 나는 되게 조금 어 저게 말이 되는 건가라는 생각이 잠깐 들었거든.\\n', 'c/ 되게 어어 군대에서는 그게 되게 절대 안 되는 행동이니까 b/ 보통 이제 남자애들이 군대 갔다오면 그 생각을 많이 하지 않을까 싶어.\\n', 'b/ 뭐 되게 b/ 응 l/ 가영이는 그런 경험 있었어?\\n', 'c/ 꼰대를 하는 사람 있었어?\\n', '응 어제 이제 되게 내가 너무 멀어서 혹시 바로 앞에 있는 물을 좀 주실 수 있나요 뭐 이렇게 하는 건 되는데 되게 저기 멀리 있는데 내 나 내가 갈수도 있는데 혹시 저기 물 좀 갖다 주실 수 있나요 이거는 나는 말이 안 된다고 생각해.\\n', '어어어 맞아.\\n', 'c/ 어 u/ 식탁에 있는데 이제 앞에 있는 그 물을 달라고 하는 거는 되게 괜찮다고 생각을 하거든?\\n', 'c/ 그건 그 사람 앞에 있는 거니까. b/\\n', '근데 되게 저 멀리 있는 물 갖다 주세요 이거는 b/ l/ 어 아 맞아 맞아.\\n', '뭔가 뭔가 너가 조금 더 가까우니까 갔다 와라 뭔가 이런 느낌이 들면은 오히려 위에 사람이 안 좋게 보지 않을까라는 생각이 들어.\\n', 'c/ 뭔가 바로 앞에 그 사람 앞에 물건이 있다면 좀 다 말이 다르겠지만.\\n', 'b/ 그래서 되게 꼰대라는 기준이 되게 응 다른 거 같애.\\n', 'c/ 가연이 생각은 어때?\\n', '어 b/ u/ 나는 이제 군대 갔다 오다 보니까 진짜 다 군대 때문인 거 같애.\\n', 'c/ 뭔가 아래 사람을 일단 시키는 게 뭔가 당연하니까 거기는 나와서도 조금 그런 게 있지 않나 싶었어.\\n', 'b/ 아니 근데 l/ 근데 그러면 굳이 룸메를 했어야 해? l/\\n', 'b/ 너무 잘 맞아서 대화가 없는 건가?\\n', '되게 오래 살았는데?\\n', '어 좀 비슷하게 라고는 말하기 좀 그런데 약간 버스를 타다가 넘어 그니까 되게 급 정차를 해 하는 바람에 버스가 그래서 막.\\n', '발이 미끄러져서 모르는 사람 다리에 앉은 경험이 한 번 있는데 그런 경험도 있고 그 뭐지 내가 어릴 때 한 초등학교 때 그때 되게 인사를 잘하는 아이였거든?\\n', '그래서 되게 모르는 사람이나 지나 가는 되게 어른들 한.\\n', 'u/ 같은 아파트 내에 있는 어른들이나 그런 사람들한텐 다 인사하고 다녔어.\\n', '근데 너무 열심히 해 해가지고 내가 그거 땜에 친구를 사귄 경험도 있었는데 n/ 인제 내가 그 원대 병원에 갔었는데 내가 너무 인사를 열심히 하다가 l/ 모르는 모르는 여자 의사 선생님한테 인사를 하고 갔는데 그 의사 선생님이 갑자기 나를 보더니.\\n', '혹시 여기 무슨 검사 받았 적 있었냐고 나한테 검사받았던 환자시냐고 묻는 거야.\\n', 'c/ 그래서 어린 어렸을 때 난 너무 당황해가지고 어 n/ 아니에요 잘 u/ 잘 잘못 인사했어요 하고 l/ 이제 응 u/ 이제 그 그 이후로 모르는 사람한테는 인사를 하지 않게 됐어. l/ \\n', 'l/ 그래서 되게 인사를 열심히 하다가 그 이후로 안 하게 됐어. l/\\n', 'b/ 그니까 아는 사람이면 인사를 하거나 같은 자주 보는 그런 지 사람이면 인사를 하는데 정말 처음 보는 사람이다 l/ 그런 그런 경운 안 하게 되더라구.\\n', 'l/ b/ 확실히 u/ 시대 별로 유행하는 그런 게임도 많이 다른 거 같기는 한 거 같애.\\n', 'b/ 요즘은 뭐가 유행하드라 게임이?\\n', '어어 맞아 그거 그거는 하면 좋을 거 같아.\\n', '좋아하는 프로그램?\\n', '나는 예능 채널은 잘 많이 안 보거든.\\n', 'c/ 근데 가끔 옛날에는 런닝맨 많이 보긴했었어, 근데.\\n', '어 나는 그 그 아까 얘기 들으면서 모르는 사람이랑 있던 얘기를 친 친구 통해서 들었던 적이 있었거든, 아까 영상 보면서 딱 그 생각이 나더라고.\\n', '그니까 우리가 스무살 때 이제 대학교 입학하면 우리가 입학하면 전국에서 다 오잖아, 우리 학교로.\\n', '어 근데 이제 거기서 친구가 처음에 이제 서울에서 이제 익산으로 내려오는 거였어.\\n', 'c/ 기차를 타고.\\n', 'c/ 근데 그때가 이제 처음 방을 구하러 내려온 날이었거든.\\n', '근데 처음에 이제 기차를 서울에서 타고 오는데 친해진 옆자리 사람도 같 원광대 오는 사람인 거야.\\n', 'c/ 바로 옆에 앉 u/ .\\n', 'c/ 이제 그게 익산 가까이 와서 좀 알게 됐는데 그 사람도 방을 보러 왔 온다고 하더라고.\\n', '그래서 같이 어 그러면 같이 방을 볼래요 하면서 같이 방을 보다가 그 결국은 둘이 같이 룸메를 해서 하숙집에 들어갔어.\\n', '그래갖고 근데 지 난 그 둘이 되게 친해졌을 거라고 생각을 했거든?\\n', 'c/ 근데 진짜 방만 공유하고 서로 아무 대화도 안 한다는 거야. l/\\n', '심지어 같은 원룸에서 같이 지내는데.\\n', '그냥 이불을 깔고 같이 지내는데 아무 그거를 안 얘기를 거의 안 한다고 하더라고, 어 왔어 정도? l/ \\n', '진짜 이름 이 만 그거 하고 거의 밖에서 놀지도 않고 술 먹거나 이런 것도 아니고 그냥 서로 이제 얼굴 이름만 아는 인사만 하는 룸메더라고.\\n', 'c/ 그래서 나는 어 그 이야기가 되게 웃겼어.\\n', 'l/ 근데 이제 둘이 되게 통한다고 하더라고, 그 생활하는 거에 있어서는.\\n', 'l/ 그래서 되게 청소할 거는 자기들이 알아서 딱딱 나눠서 하니까 별로 신경 터치할 것도 없고 그냥 생활하는데 문제없어서 계속 그렇게 1년 1년 가까이 어 1년 살았더라고.\\n', '응 그래서 원래 반년만 계약하는 건데 그냥 연장해가지고 했었다고 들었어.\\n', '어 가영이는 비슷하게 막 모르는 사람 이런 거 있었어?\\n', '그럼 다 인사하고 다녀?\\n', 'c/ 지나 가면서?\\n', 'l/ 아 당황해가지고?\\n', '아 b/ 어 되게 그렇긴 하겠다, 처음에는 예의 바르게 생각할 수도 있는데 b/ 아는 사람이겠구나 생각할 수도 있을 거 같애.\\n', '되게 b/ 아 그것도 색다르네, 이야기. l/ \\n', '아 b/ 되게 나는 근데 아까 그 영상 보면서 그 무한도전도 되게 옛날에 지금은 해체 되고 안 하는 프로그램이잖아.\\n', '그래서 그래서 되게 옛날에 웃긴 거 봤던 막 아까 패밀리가 떴다도 보고 막 이런 거 웃겼던 얘기가 많이 기억 났어.\\n', 'c/ 그리고 요즘은 런닝맨 같은 거 많이 나오잖아.\\n', 'c/ 그래서 거기에서 나오는 게임 같은 거를 그 막 MT때 어떻게 하면 할 수 있을까 되게 고민을 많이 해보고 있어. b/\\n', '응 b/ 그래서 다음에 MT를 뭐하면 좀 고민, 고민도 되기는 해.\\n', 'l/ 게임 같은 건.\\n', '어 어 b/ 되게 그거 막 이마에 붙이고 막 이러는 거 있잖아.\\n', 'c/ 어 어 어 어 어 그것도 어 그것도 되게 재밌는데 어려울 어렵더라, 저번에 한 번 해봤었거든 친구들이랑.\\n', '어 어 되게 어렵더라, 맞 아무도 못 맞추더라고. l/\\n', '그래 갖고 되게 웃겼었어.\\n', '어 그리고 뭐 좋아하는 프로그램 같은 거 있어?\\n', '성 평등에 대해서?\\n', 'b/ 글쎄 일단은 나는 그 말이 살짝 좀 이해가 안 됐어, 약간.\\n', '굳이 남녀 비율을 반반으로 맞출 필요가 있나라는 거에서 나는 솔직히 맞출 필요가 없다고 생각을 하거든?\\n', '본인 재량에 따라서 다르다고 생각을 해가지고 응, 그냥 이 이거는 경쟁이지 남녀에 싸움이 아니라고 생각을 하거든.\\n', '응 나는 살짝 약간 의견이 다랐 달랐던 게 뭐냐면 방금 얘기에서 간호사 직 같은 경우에는 n/ 난 좀 생각이 다른 게 그거는 남녀 비율이 좀 반반이 되어도 괜찮다고 생각은 해.\\n', 'c/ 왜냐면 n/ 어떤 요즘 그 간호사 직 같은 경우에는 여성 비율이 상당히 높은 편이잖아.\\n', '남자가 가끔 있다고는 하는데 근데 사실상 간호사분들이 일을 하다가 진상인 환자분들도 많이 계시다고 하는데 그 안에서 되게 남자가 간호사가 좀 있으면은 좀 든든하다고 얘기를 하더라고.\\n', '어 나도 그 말에 굉장히 동의하는 바입니다.\\n', 'c/ 근데 개인 사정으로 그냥 일을 관두는 거에서 발생하는 승진 문제이지 응 굳이 회사 안에서 발생하는 갈등이라고 해야하는지는 모르겠어.\\n', 'b/ 글쎄 근데 나는 약간 어릴 때는 굉장히 그게 싫었던 거 같기는 해.\\n', 'c/ 성차별이라고 봐야 될지 모르겠는데 저기 되게 힘쓰는 일은 남자가 보통 많이 하다 보니까 나는 근데 내가 할 수 있다고 생각을 하는데 굳이 남자들한테 시키더라고.\\n', '그래서 좀 옛날에는 그게 조금 싫었던 건 있었었어.\\n', '그것도 맞기는 해. l/\\n', '근데 그거는 굳이 약간 남녀 그런 문제가 아니라 누가 도우려고 하든 간에 내가 도움을 받는 걸 싫어해서 그런 거야.\\n', '근데 나는 오 오빠가 말했듯이 살짝 경찰이나 소방관이나 그런 직에서는 좀 남자가 더 많이 있는 게 낫다고 생각은 해.\\n', 'b/ 어 가영이는 방금 영상 보면서 혹시 어떻 어떻게 생각했어?\\n', 'c/ 성 평등에 대해서.\\n', 'b/ 근데 이제 나 같은 경우는 뭔가 어 가영이 말에 동감을 하는데 뭔가 남자가 잘하는 일이 있고 여자가 잘하는 일이 나는 따로 따 나눠 조금 있 나눠져 있다고 생각을 하거든.\\n', 'c/ 물론 같이 공부를 했을 때 u/ 똑같은 사람들도 있겠지만.\\n', 'c/ 근데 이제 보면은 뭐 교사나 이제 유치원 교사나 뭐 이런 u/ 되 되게 간호사나 이런 건 여 여자가 많이 하시는 그 일이잖아.\\n', 'c/ 그래서 그쪽에 비율이 낮다고 많다고 들었는데 근데 이제 거기서 여기서 얘기하는 건 이제 정부 공무원직이나 뭐 아니면 그 뭐 직장에서의 이제 높은 임원들 얘기를 하는 거잖아. b/\\n', 'c/ 근데 이제 거기 u/ 이제 나도 경쟁이라고 생각을 하거든. b/\\n', '이제 그거에 대해서는 어린 나이에도 대표를 다시는 여성분들도 계시는데 굳이 그거를 비율을 맞춰야 되느냐에 대해서 조금 그런 거 같애.\\n', 'c/ 본인에 역량에 맞춰서 올라가는 거지 않을까?\\n', '난 그렇게 생각을 하거든.\\n', '응, 맞아 그래서 되게 지금 남자 간호사의 채용률이 엄청 높다고는 들었었어, 일부러 더 구하고 있는 추세라고.\\n', '그래서 되게 그쪽으로 가면 취업은 당연히 된다라고 얘기를 하더라고, 보통적으로. b/\\n', '뭔가 나는 그래서 사람마다 이제 하는 직업군이 그 잘하는 일이 다르듯이 그렇게 되게 본인이 노력하면서 하는 게 맞다고 생각하는데 아까 얘기 들었던 말 중에 30대에서 그만두는 여성들이 많다라는 말이 보통 이제 그런 문제는 출산이나 뭐 이런 거 때문에 중간에 관두는 경우가 많은 거잖아.\\n', '근데 그거를 그거를 이제 성 평등에 어긋난다라고 얘기하는 게 조금 나는 어 조금 반대하는 의견이었어.\\n', '응 b/ 그래서 내가 아 근데 하긴 내가 경찰청에서 일했을 때 되게 간부에 있으신 분들이 여자분은 거의 못 뵀던 거 같기는 하거든.\\n', 'b/ 그래서 어느 정도 인원수는 필요하지만 이게 이제 능력에 따라서 올라가야 되지, 일부러 할당제로 이게 딱 맞춰서 인원을 들어가야 된다 이거는 조금 아닌 거 같애.\\n', 'c/ 아까 말했던 거처럼 능력이 안 되는데 내가 올라갈 수 있고 뭔가 낙하산이 되는 느낌이 될 수도 있잖아.\\n', '그래서 그게 되게 안 좋지 않을까라는 생각이 나는 오히려 반대로 어 아까 그분들의 말에 동의가 들었었어.\\n', '되게 내가 그 경험을 하면서. b/\\n', '응 그래서 혹시 그 성 평등에 대한 경험 음 성차별 이런 경험이 혹시 있었어, 가영이는?\\n', '응 내가 할 수 있는 근데 가영이는 그게 n/ 그것도 있지 않아?\\n', 'c/ 내가 u/ 내가 가영이 거를 뭔가 해주려고 하면은 어 내가 할 수 있다고 하는.\\n', '뭔가 그런 거 자주 있지 않아?\\n', '응 b/ u/ 내가 응 내가 뭔가 가방을 들어주려고 하거나 뭐 이럴 때 되게 싫어하는 거 같더라고.\\n', 'b/ 그래서 음 굳이 내가 해줄 수 있는데 싫어하니까 조금 난감했던 경우도 있던 거 같애.\\n', '응 b/ 그래서 되게 b/ 어 막 성차별하니까 되게 b/ 어 또 어디서 느꼈었지?\\n', '어 b/ 근데 확실히 아까 그 반대한 u/ 성차 성 평등 쪽에 되게 하나 그나마 공감이 갔던 거는 어 이제 할 수 뭐라고 해야 되지?\\n', '그 사람 여 여성들이 할 수 있는 역할들이 분명 있잖아.\\n', 'c/ 경찰 안에서도 이제 여자들을 다루려면 여자 공무원이 있어야 u/ 경찰 직원이 있어야 되고 그니까 그런 거에 역할이 있어서의 비율이 늘어나는 거는 맞겠지만 역 어 일부러 이제 할당제로 이제 열 연령 그거를 늘려야 되는 건 난 이해를 못 하겠어.\\n', 'b/ 저번에도 얘기했던 거 같기는 한데 약간 b/ 외계어를 하고 하면서 자가지고 좀 무서운 큰 오빠 잠꼬대라든가 l/ 아니면은 b/ 나 빼고 가족들이 전부 다 이제 코골이가 좀 심한 편인 거 같애.\\n', '나도 피곤하면 코골이를 하긴 하는데 근데 아빠가 유독 진짜 안방에서 문을 닫고 자도 진짜 그 문 너머로 크게 들릴 만큼 막 우르 킁하면서 l/ 엄청 크게 들리거든.\\n', '어 그 진짜로 l/ 그래서 가끔 잠을 깬 적 있었어, 한 번씩.\\n', '코골이 때문에? b/\\n', '딱히 없었는데.\\n', '어 그런 얘기도 있었어.\\n', 'b/ 요즘엔 핸드폰이 자꾸 그 뭐지 빅스비였나? l/\\n', '그거 자꾸 켜져서 짜증 나. l/\\n', 'b/ 아니 이유도 없이 그냥 막 켜져, 내가 뭐라 한 것도 아닌데.\\n', 'c/ 말도 말도 안 했는데 그냥 막 떠.\\n', 'l/ 근데 좀 신나기는 한 거 같애, 그런 노래 나오면 갑자기.\\n', 'b/ 스피커로?\\n', 'b/ 나 같은 경우에는 그냥 재생 목록 트는 것만 u/ 다여서 그런 적은 없는데 b/ 근데 스피커를 좀 여러 종류를 좀 써보면서 비교를 해보고 싶은데 l/ 뭐가 좋은질 모르겠더라고.\\n', '어 개발사마다 막 구글 쪽도 있고 카카오도 있고 네이버도 있고 막 엄청 다양하다 보니까 b/ 뭐가 좋은 u/ 뭐가 좋은 거 같애 오빠?\\n', 'l/ 갑자기 네 사랑합니다. l/\\n', '응, b/ 응 가영이는 잠꼬대 땜에 히 힘들거나 웃겼던 이야기 있어?\\n', '어 막 탱크 온 거처럼? l/\\n', '어 b/ 나는 되게 코골이 때문에 어 맞아 나는 훈련소에 처음 가면 되게 바로 옆에서 이렇게 남자 애들이 이렇게 붙어가지고 잔단 말야, 이 딱 이 거리에서.\\n', '진짜 그러다 보니까 옆에서 코 골면은 옆에 사람이 진짜 잘 못 자.\\n', '근데 나는 되게 잠을 엄청 잘 자는 타입이거든.\\n', 'c/ 그냥 눈 감으면 바로 잘 수도 있는 타입이어서 b/ 그 눈 이제 감고 이제 자면은 코골이를 내가 하는데 옆 사람이 이제 되게 민감한 애였던 거야.\\n', 'c/ 그래갖고 소리 때문에 되게 나 때문에 못 잤다고 막 너 좀 코 좀 그만 골라고 했던 적이 있었었어.\\n', '그래서 나는 그렇게 코골이가 심한 거를 군대 가서 처음 알았어.\\n', 'c/ 아 심하다기보다는 코골이를 한다는 거 자체를.\\n', '어 그때 이제 누구랑 옆에서 재 자고 막 이러는 거를 처음 하니까 옆에 잘 사람 자는 사람 적이 없으니까 부모님도 얘기해 준 적 없었고.\\n', 'b/ 어 그래서 처음 알게 됐었어, 군대 가서.\\n', '그래갖고 그거 고치는 거 막 알아보고 막 했는데 b/ 되게 나 말고도 더 고생한 사람이 많더라고. l/\\n', 'c/ 그 가영이는 뭐 주변에서 막 이렇게 보고 가영이가 그거 했던 적은 웃겼던 건 적은 없었어?\\n', 'c/ 코골이 땜에.\\n', 'c/ 아니면 어 어.\\n', '어 b/ 그거랑 또 뭐 있었지?\\n', 'c/ 막 AI 스피커도 있잖아. \\n', 'c/ 그것 그것 땜에 당 그거 땜에 당황스러웠던 적은 없었어?\\n', 'c/ AI 스피커 때문에?\\n', '응, 나는 예전에 이제 u/ 그 그 나 군대 전역하고 나서 나왔는데 집에서 그거를 애들이랑 집에서 밥을 먹었어.\\n', 'c/ 밥을 먹으면서 어 클로바 부르면서 무슨 무슨 노래 켜줘 했는데 되게 생뚱 맞은 노래가 나오는 거야.\\n', 'c/ 엄청 옛날 막 u/ 노래처럼 막 그런 거 나오거나 막 어 트로트 나오거나 아니면은 막 진짜 듣지도 못한 노래가 막 나오거나 막 이런 적 있었어서 엄청 당황했었어.\\n', 'c/ u/ 한 번도 들어보지 못한 노랜데 l/ 애들이 어 이 노래 좋다면서 막 듣는데 너무 웃긴 거야. l/\\n', '응, 아 그래서 되게 당황했던 적이 있었었어.\\n', '그래갖고 그 노래 일부러 막 찾으려는 노래를 다시 말해줘도 그 노래를 또 트는 거야. l/\\n', '그래갖고 결국은 막 다른 노래 찾아서 들었던 적도 있었었어.\\n', '어 가영이는 뭐 그런 적은 없었어, 뭐 검색하거나 이랬던 적은 없어?\\n', '아 그 개발사마다 이제 만든데마다?\\n', '네이버도 있고.\\n', '응 b/ 근데 보통 사용하면은 막 주로 물어보는 게 날씨 물어보고 노래 찾는 노래 들려줘 뭐 u/ 날씨가 나는 보통 대부분인 거 같애.\\n', '어 오늘 날씨 알려 그리고 막 되게 가끔 생뚱 맞는 말 할 때 있어.\\n', 'c/ 내가 혼잣말하거나 전화를 막 하고 있는데 집에서 자기 혼자 네 부르셨나요 막 이러거나 l/ 되게 띠링 소리 나다가 자기 혼자 갑자기 노래를 트는 경우가 있어.\\n', 'c/ 내가 그냥 그 일상적인 전화로 대화하거나 하는데.\\n', '그래갖고 되게 특히 친구랑 전화하는데 웃겼던 적도 있었었어.\\n', '어 막 한참 웃으면서 얘기하고 있는데 갑자기 막 노래 노래 막 팝송 나오고 l/ 어.\\n', '어어 l/ b/ 되게 BGM처럼 막 깔아줬던 웃겼던 적도 있었었어.\\n', 'b/ u/ 그것 말고 또 뭐가 있었을까?\\n', '되게 웃겼던 적은 막 인식을 진짜 잘못해서 하는 경우가 대부분인 거 같애.\\n', 'c/ 그 영상에서 봤다시피.\\n', '우리 집은 되게 자유로워 가지고 솔직히 아빠가 이게 어때라고 하면 본인 생 그니까 가족들 생각이랑 안 맞으면 그냥 자기 주관대로 가거든, 다들.\\n', '그래서 좀 자유로워서 아빠가 b/ 가부장적인 면이 없지 않아 있다고는 생각을 하는데 그래도 우리가 그냥 안 듣다 보니까 그런 게 없 없는 듯이 느껴지는 거 같애.\\n', '가부장적이다라는 거?\\n', '응 일단은 아빠가 하는 말에 전부다 따라야 하는 그런 환경?\\n', '그니까 b/ 그런 경험이 b/ 음 솔직히 거의 없는 거 같은데 약간 명절에 시골에 내려 가면은 약간 큰 아빠?\\n', '큰아빠 말대로 뭔가 그 시골 내에서 일이 돌아가는 거 같기는 했어.\\n', '근데 되게 그런 걸 보면서 되게 엄마도 명절 때 가서 큰아빠 말에 따르고 하니까 되게 스트레스를 받다 보니까 엄마가 그 뒤로는 안 가드라고.\\n', '통금 시간?\\n', '나도 그건 이해가 안 될 거 같애, 자기는 되는데 남은 안 된다는 게.\\n', '확실하게?\\n', '지금도 그래?\\n', '아 그럴 수도 있겠구나.\\n', '너무 늦게 들어오면 그럼 좀 잔소리를 하시기는 하는데 사실 중학교 때까지는 통금이 있기는 했었어.\\n', 'c/ 한 여섯시까지는 들어가야 된다라는 게 있었는데 그거 그 규칙이 불문율이라고 했어야 되나 근데 그거를 오빠가 깨버렸거든. l/\\n', '친아빠들이 그걸 깨버려서 나도 그냥 그때까지 안 들어 와도 되는 구나라는 걸 그때 알아서. l/\\n', '응 나는 나는 근데 재깍재깍 연락을 미리 하고 하고 다녔어가지고 근데 지금은 조금 말을 많이 안 듣기는 하는데. l/\\n', '근데 아빠도 사실 내가 아빠를 닮아서 좀 알겠는데 약간 마음에도 없는 소리가 나올 때가 있어.\\n', '되게 가부장적인 아버지라고 생각하면 b/ 우리 아버지도 좀 그런 거 있는 거 같애.\\n', '음 되게 b/ 어 되게 본인 생각이 엄청 확고하신 편이시거든.\\n', '그래서 아버지에 말은 거의 무조건 따라야 되는 뭔가 집안에 그런 게 있었었어.\\n', '그래서 u/ b/ 거의 아버지가 하는 말이 거의 절대적인 느낌이었고 그런 걸 많이 느꼈었던 거 같애.\\n', 'c/ 가영이 집은 그런 거 있었었어?\\n', 'b/ 되게 b/ 가부장적이라는 기준을 조금 내가 되게 애매한 거 같애.\\n', 'c/ 가영이는 가부장적이다라고 하면 어떤 걸 생각해 혹시?\\n', '어어어 기준을 어떤 걸로 세웠어?\\n', '어 그런 경험이 있었어?\\n', '안 가실 수도 있을 거 같애, 만약에 되게 스트레스를 많이 받으면.\\n', 'b/ u/ b/ 나는 보통 아버지가 되게 옛날에 이해가 안 갔던 것 중에 하나가 통금 시간이었어.\\n', '어 근데 이제 통금 시간을 내가 군대 갔다 와서도 계속 있었거든?\\n', '요 올해 들어서 좀 없었고 작년까지만 해도 있었단 말이야.\\n', '통금 시간이 거의 12시를 무조건 거의 넘으면 안 됐었어.\\n', 'c/ 근데 이게 내 형도 그랬고 엄마도 그래야 됐어.\\n', '그래갖고 그래갖고 그니까 아 아빠도 그런데 아빠는 근데 술 먹으면 되고 이게 되게 그게 좀 그랬어.\\n', 'c/ 아빠는 술 먹으면 늦는 게 좀 당연하신 거고 우리가 술 먹고 늦게 들어오는 거는 좀 안 되는 그런 느낌이었어.\\n', 'c/ 그니까 되게 아빠가 이제 열두시만 되면은 전화가 오거든.\\n', 'c/ 왜 안 들어오냐고. b/\\n', '그러면 이제 나나 형은 어 친구들한테 이제 가봐야 아빠가 연락 와서 열두시 돼서 가봐야 u/ 거의 막 u/ 그래갖고 내가 나랑 형이 친구들 사이에서 별명이 연데렐라였어.\\n', '그래갖고 전화 오면 u/ 되게 그게 되게 힘들었었어, 생각보다.\\n', 'c/ 이제 나는 전화 오면 어 진욱이 당연히 가야 돼 이 소리가 되게 나는 듣기가 좀 그랬었거든. \\n', 'c/ 근데 어머니도 되게 불편해하셨었어.\\n', '되게 어머니가 회식하거나 이러면 어 회식 가요하고 연락이 좀 안 되시는 타입이시긴 하거든.\\n', 'c/ 이 돌아오실 때까지.\\n', '그래갖고 아빠가 되게 걱정하시는 건 아는데 걱정하고 조금 본인 고집이라면 고집인데 b/ 조금 나는 그게 이해가 안 됐었어, 아버지가.\\n', '응 b/ 근데 그 기준이 되게 연락을 아빠는 자주하고 늦는다고 미리 얘기를 했으니까 된다고 하는 건데 이제 그 기준도 한 20분 30분이 넘어가면 또 연락이 와.\\n', '연락이 와서 어디쯤이냐 어 몇시에 정확하게 올 거냐 그러면 이제 이제 그 시간까진 연락을 안 하시긴 하거든.\\n', 'c/ 근데 내가 만약에 열두시에 연락을 해서 어 저 한시간 정도 더 놀다 갈게요 했는데 그 한시까지 안오잖아?\\n', 'c/ 그르면 뿔이나 계셔, 엄청.\\n', '엄청 화를 내셔.\\n', '그래갖고 그 기준을 얘기하는데 어 너무 막 늦게 세시에 갈 거 같아요 이러면 절대 절대 안 돼.\\n', '어 뭔가 그런 어어어.\\n', '그런 게 딱 정해져 있으셔서 되게 늦으면 많이 혼났던 기억이 있어.\\n', '지금 요새는 조금 덜해진 거 같애.\\n', 'c/ 내가 익산에서 있다 보니까 그런 것도 있지만 b/ 어 어머니가 그거에 대해서 되게 많이 얘기를 해신 거 같더라고, 올해 들어서.\\n', '어, 그니까 어머니도 되게 외부 일이 되게 많으신데 밤에 막 회식하거나 일이 많으시거든 u/ .\\n', '어 가영이는 통금 같은 건 없었어?\\n', 'c/ 그런 면에서 되게 아버지가 뭐라 하시거나?\\n', 'l/ 아 b/ 그럼 그거 땜에 되게 그 화내시거나 그런 적은 없으셨어, 아버지가?\\n', 'l/ 그 이해 안 되는 다른 행동을 한 적은 있었어?\\n', 'c/ 아빠가 되게 화를 b/ 내셨던 적은?\\n', 'c/ 가영이가 행동을 해서.\\n']\n",
      "['n/ 아 친구들도? l/\\n', 'l/ 나는 생일?\\n', '생일날이면은 b/ 내가 고기를 되게 좋아하니까 엄마도 그걸 아니까 미역국도 같이 해주시는데 꼭 고기를 같이 구워주시거나 아니면 여행을 가서 고기를 구워 먹는다거나 꼭 고깃집에 가서 어 해먹어, 꼭.\\n', 'b/ 선물이라 이 보통 돈으로 주시지. l/\\n', 'l/ 용돈으로 주시고.\\n', '옛날에는 n/ b/ 내가 원하는 게 뭔지 일단 물어보시곤 했던 거 같애.\\n', 'c/ 옷이라든가 갖고 싶은 거 있나라든가 n/ 근데 이제 b/ 나이를 먹을수록 현금으로 챙겨주시더라구.l/\\n', '응 나도 되게 b/ 여행 가는 걸 좋아하긴 하는데 가족 다 같이 가는 것보다 되게 엄마랑 여행 가는 게 많았던 거 같아. \\n', '어 단둘이.\\n', '그 되게 아빠나 오빠가 시간이 안 나거나 꼭 b/ 어디 약속이 있다거나 그래가지고 엄마가 주말마다 좀 같이 둘이 어디 데려다가 줬어, 그냥.\\n', '여행 여행을.\\n', 'b/ 최근 최근에?\\n', 'b/ 굳이 가자면 나는 부산에 가면 꼭 엄마가 생각나.\\n', '어 부산을 좀 가기도 많이 갔는데 부산에 엄마가 좀 n/ 그 친척들이 u/ 가지고.\\n', '아 n/ 되게 되게 처음이라 엉망진창이었던 거 같긴 한데.\\n', '나는 어 어머니, 엄마가 이제 생일 때마다 이제 미역국도 많이 해서 미역국을 되게 좋아하거든.\\n', '전에도 말했지만 그래서 미역국이랑 볶음김치 막 해주셔서 맨날 그렇게 차려주시는데 어 옛날에는 이제 집에서 밥 먹는 거 보다 그 캔모아 빙수 있는데 있잖아.\\n', 'c/ 어 응 거기를 되게 엄 집 앞에 그 조그만한 가게로 있었는데 거기를 엄청 생일 때마다 매번 같이 데려갔었거든.\\n', '거기서 엄마랑 둘이 생일 파티 그냥 간단하게 하는 경우도 있었고 b/ 보 음 보통 그렇게 아니면 친구들 이제 거기 빙수 가게로 불러서 같이 빙수 먹으면서 케익도 불고 막 이랬었었어.\\n', '응응 그래서 생일 때 그렇게 했던 경험이 있는데 그때가 이제 초등학교 때였는데 이제 중학교 때쯤 가게가 문을 닫았거든, 집 앞이었는데.\\n', '그래서 이제 막 지역 돌아다니면서 캔모아 빙수 있는 가게 저번에 광주 갔을 때도 있었 갔었잖아.\\n', 'c/ 어 그래서 그런 가게들을 갈 때면 맨날 엄마 생각이 되게 자주 났어.\\n', '그래서 일부러 보이면 일부러 가기도 해.\\n', 'c/ 거기 그때 먹었던 빙수 맛이 그대로인데 되게 맛있었거든.\\n', '그래서 일부러 좀 자주 일부러 찾아서 가는 경향도 좀 있는 거 같애, 옛날 추억도 있고.\\n', 'u/ 거기 가면 꼭 먹는 게 떡볶이 막 구멍 나 있는 거랑 그 빙수랑 딱 같이 두 개는 무조건 시켜서 먹는 거 같애.\\n', '그래서 되게 그거 말고도 뭐 선물 같은 거는 u/ 어머니가 막 해주셔?\\n', '해주셨었어?\\n', '어 용돈으로?\\n', 'c/ 나도 옛날에는 막 사고 싶었던 물건들 막 주셨는데 지금 보통 용돈으로 주시는 거 같애, 거의 고등학생 이후로는.\\n', '그래가지고 좀 b/ 뭔가 그 생각도 들어.\\n', 'c/ 뭔가 나를 기념해 주는 선물을 받으면 또 어머니 생각이 많이 날 거 같기도 해.\\n'] 32\n"
     ]
    }
   ],
   "source": [
    "for session, (_, corpus) in text_datasets.items():\n",
    "    print(session)\n",
    "    print(corpus)\n",
    "    print(corpus[0:32], len(corpus[0:32]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 119.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 104.88it/s]\n"
     ]
    }
   ],
   "source": [
    "audio_datasets = make_audio_datasets(1)\n",
    "sampling_rate = audio_datasets[1][0]['audio']['sampling_rate']\n",
    "text_datasets = make_text_datasets(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch 4짜리\n",
    "\n",
    "inputs = []\n",
    "for i in range(4):\n",
    "    inputs.append(audio_datasets[1][i]['audio']['array'])\n",
    "    \n",
    "processed_inputs = processor(inputs, sampling_rate=sampling_rate, padding=True, max_length=400, return_attention_mask=True, return_tensors=\"pt\")\n",
    "audio_outputs = audio_d2v(**processed_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arplab/project/paradeigma/multi_modal/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2364: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# batch 4짜리\n",
    "\n",
    "inputs = []\n",
    "for i in range(4):\n",
    "    inputs.append(text_datasets[1][1][i])\n",
    "    \n",
    "processed_inputs = tokenizer(inputs, padding=True, max_length=20, return_attention_mask=True, return_tensors=\"pt\")\n",
    "text_outputs = text_d2v(**processed_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_datasets[1]['audio']['array']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [08:22<00:00, 502.70s/it]\n"
     ]
    }
   ],
   "source": [
    "# cpu만 사용하면 8분걸림\n",
    "# cuda를 사용하면 OOM\n",
    "\n",
    "audio_embedded_dict = audio_embedding(audio_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]/home/arplab/project/paradeigma/multi_modal/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2364: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. 셀의 코드를 검토하여 오류의 가능한 원인을 식별하세요. 자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'> 여기 </a> 를 클릭하세요. 자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "# cpu만 사용하다가 커널이 죽었음\n",
    "# cuda를 사용하면 OOM\n",
    "\n",
    "text_embedded_dict = text_embedding(text_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 93.94it/s]\n"
     ]
    }
   ],
   "source": [
    "target_dict = make_target_dict(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = target_dict[1]['emotion'][0:4]\n",
    "target[0:3] = 0\n",
    "target[3] = 1\n",
    "target = target.astype('float32')\n",
    "target = target.values\n",
    "target = torch.tensor(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_1(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MLP_1, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(1235 * 768, 768)\n",
    "        self.fc2 = nn.Linear(768, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        \n",
    "mlp_1 = MLP_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:75] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 575753158656 bytes. Error code 12 (Cannot allocate memory)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m         x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(x)\n\u001b[1;32m     13\u001b[0m         \u001b[39mreturn\u001b[39;00m x\n\u001b[0;32m---> 15\u001b[0m mlp_2 \u001b[39m=\u001b[39m MLP_2()\n",
      "Cell \u001b[0;32mIn[64], line 6\u001b[0m, in \u001b[0;36mMLP_2.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m      4\u001b[0m     \u001b[39msuper\u001b[39m(MLP_2, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m----> 6\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mLinear(\u001b[39m988\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39m247\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39m768\u001b[39;49m, \u001b[39m768\u001b[39;49m)\n\u001b[1;32m      7\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(\u001b[39m768\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/project/paradeigma/multi_modal/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:96\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_features \u001b[39m=\u001b[39m in_features\n\u001b[1;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_features \u001b[39m=\u001b[39m out_features\n\u001b[0;32m---> 96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39;49mempty((out_features, in_features), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfactory_kwargs))\n\u001b[1;32m     97\u001b[0m \u001b[39mif\u001b[39;00m bias:\n\u001b[1;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty(out_features, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:75] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 575753158656 bytes. Error code 12 (Cannot allocate memory)"
     ]
    }
   ],
   "source": [
    "# 차원 축소를 무조건 수행해야 할 것. \n",
    "\n",
    "class MLP_2(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MLP_2, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(988 * 247 * 768, 768)\n",
    "        self.fc2 = nn.Linear(768, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        \n",
    "mlp_2 = MLP_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (4, 1235 , 768)\n",
    "x_train = torch.rand(shape)\n",
    "y_train = target\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(mlp_1.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 2\n",
    "batch_size = 1\n",
    "num_samples = x_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.tensor(torch.cat((audio_outputs['last_hidden_state'], text_outputs['last_hidden_state']), dim=1))\n",
    "y_train = target\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(mlp_1.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 2\n",
    "batch_size = 1\n",
    "num_samples = x_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = a_1\n",
    "y_train = target[0]\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(mlp_1.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 2\n",
    "batch_size = 1\n",
    "num_samples = x_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "Epoch 1, Loss: 1931420318564352.0\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "Epoch 2, Loss: 1643181204570112.0\n"
     ]
    }
   ],
   "source": [
    "epoch_loss = 0\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        print(i)\n",
    "        inputs = x_train[i:i+batch_size]\n",
    "        targets = y_train[i:i+batch_size]\n",
    "        \n",
    "        outputs = mlp_1(inputs)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * batch_size\n",
    "        \n",
    "    epoch_loss = running_loss / num_samples\n",
    "    print(\"Epoch {}, Loss: {}\".format(epoch+1, epoch_loss))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m y_train \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, (\u001b[39m1\u001b[39m,\u001b[39m10\u001b[39m))\n\u001b[1;32m      4\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mMSELoss()\n\u001b[0;32m----> 5\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mSGD(net\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n\u001b[1;32m      7\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n\u001b[1;32m      8\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "x_train = torch.randint(0, 1, (1,120))\n",
    "y_train = torch.randint(0, 1, (1,10))\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 5\n",
    "batch_size = 1\n",
    "num_samples = x_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m net(x_train)\n",
      "File \u001b[0;32m~/project/paradeigma/multi_modal/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[66], line 11\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 11\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc2(x))\n\u001b[1;32m     12\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc3(x))\n\u001b[1;32m     13\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/project/paradeigma/multi_modal/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/project/paradeigma/multi_modal/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype"
     ]
    }
   ],
   "source": [
    "net(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.8839\n",
      "Epoch 2 Loss: 0.8758\n",
      "Epoch 3 Loss: 0.8678\n"
     ]
    }
   ],
   "source": [
    "# 모델 정의\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 데이터셋 생성\n",
    "x_train = torch.randn((30, 10))\n",
    "y_train = torch.randn((30, 5))\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "input_dim = x_train.shape[1]\n",
    "hidden_dim = 20\n",
    "output_dim = y_train.shape[1]\n",
    "lr = 0.01\n",
    "num_epochs = 3\n",
    "batch_size = 5\n",
    "\n",
    "# 모델 초기화\n",
    "model = SimpleModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# 손실함수 정의\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 옵티마이저 정의\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# 훈련\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        # 배치 데이터 로드\n",
    "        inputs = x_train[i:i+batch_size]\n",
    "        targets = y_train[i:i+batch_size]\n",
    "        \n",
    "        # 순전파\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # 역전파\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 손실값 누적\n",
    "        running_loss += loss.item() * batch_size\n",
    "    \n",
    "    # 에폭별 손실값 출력\n",
    "    print('Epoch {} Loss: {:.4f}'.format(epoch+1, running_loss/num_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1453e+00, -1.3806e-01, -6.6733e-02,  ...,  6.6787e-03,\n",
       "          -7.2099e-02,  2.8462e-01],\n",
       "         [-1.1453e+00, -1.3806e-01, -6.6732e-02,  ...,  6.6784e-03,\n",
       "          -7.2098e-02,  2.8462e-01],\n",
       "         [-1.1453e+00, -1.3806e-01, -6.6733e-02,  ...,  6.6792e-03,\n",
       "          -7.2098e-02,  2.8462e-01],\n",
       "         ...,\n",
       "         [-1.7483e-02, -5.4983e-03, -6.0314e-02,  ...,  1.3965e-02,\n",
       "          -1.4430e-01,  1.0852e-01],\n",
       "         [-1.7483e-02, -5.4983e-03, -6.0314e-02,  ...,  1.3965e-02,\n",
       "          -1.4430e-01,  1.0852e-01],\n",
       "         [-1.7483e-02, -5.4984e-03, -6.0314e-02,  ...,  1.3965e-02,\n",
       "          -1.4430e-01,  1.0852e-01]],\n",
       "\n",
       "        [[-9.5902e-01, -6.7264e-02, -2.1643e-01,  ...,  9.4446e-02,\n",
       "          -7.7794e-04,  4.8156e-01],\n",
       "         [-9.5902e-01, -6.7264e-02, -2.1643e-01,  ...,  9.4447e-02,\n",
       "          -7.7803e-04,  4.8156e-01],\n",
       "         [-9.5902e-01, -6.7264e-02, -2.1643e-01,  ...,  9.4446e-02,\n",
       "          -7.7759e-04,  4.8156e-01],\n",
       "         ...,\n",
       "         [ 7.2260e-02, -2.2458e-02, -4.9515e-02,  ...,  3.4100e-02,\n",
       "          -1.3729e-01,  1.2640e-01],\n",
       "         [ 7.2260e-02, -2.2457e-02, -4.9515e-02,  ...,  3.4100e-02,\n",
       "          -1.3729e-01,  1.2640e-01],\n",
       "         [ 7.2260e-02, -2.2458e-02, -4.9515e-02,  ...,  3.4100e-02,\n",
       "          -1.3729e-01,  1.2640e-01]],\n",
       "\n",
       "        [[-6.3203e-01,  5.8064e-02, -4.6260e-01,  ...,  1.8155e-01,\n",
       "          -1.5392e-01,  4.4419e-01],\n",
       "         [-6.3203e-01,  5.8064e-02, -4.6260e-01,  ...,  1.8155e-01,\n",
       "          -1.5392e-01,  4.4419e-01],\n",
       "         [-6.3203e-01,  5.8064e-02, -4.6260e-01,  ...,  1.8155e-01,\n",
       "          -1.5392e-01,  4.4419e-01],\n",
       "         ...,\n",
       "         [-3.0071e-02,  1.8067e-01, -9.1326e-02,  ..., -1.0799e-01,\n",
       "          -2.5392e-02,  8.5111e-02],\n",
       "         [ 8.8385e-03,  1.9198e-01, -4.5030e-02,  ..., -6.2464e-02,\n",
       "           3.2973e-02,  1.1441e-01],\n",
       "         [ 4.1969e-02,  2.0687e-02, -2.8678e-02,  ..., -1.4005e-01,\n",
       "          -1.3668e-02,  1.6395e-01]],\n",
       "\n",
       "        [[-9.5966e-01,  8.3105e-03, -3.2980e-01,  ...,  8.2517e-02,\n",
       "          -1.2838e-03,  2.9957e-01],\n",
       "         [-9.5966e-01,  8.3099e-03, -3.2980e-01,  ...,  8.2517e-02,\n",
       "          -1.2841e-03,  2.9957e-01],\n",
       "         [-9.5966e-01,  8.3094e-03, -3.2980e-01,  ...,  8.2517e-02,\n",
       "          -1.2841e-03,  2.9957e-01],\n",
       "         ...,\n",
       "         [-1.0426e-01, -7.0433e-02, -7.9854e-02,  ..., -2.2678e-02,\n",
       "           1.0606e-01,  5.1207e-02],\n",
       "         [-1.0426e-01, -7.0433e-02, -7.9854e-02,  ..., -2.2678e-02,\n",
       "           1.0606e-01,  5.1207e-02],\n",
       "         [-1.0426e-01, -7.0433e-02, -7.9854e-02,  ..., -2.2678e-02,\n",
       "           1.0606e-01,  5.1207e-02]]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7589, -0.2451,  0.3043, -1.1257, -0.7017, -0.5993,  1.4000,  0.9494,\n",
       "         -0.0170, -0.0424],\n",
       "        [-0.8227,  0.0786, -0.7696,  0.7970,  2.1975,  0.3471,  0.3051, -1.5030,\n",
       "          1.1239,  0.3276],\n",
       "        [ 1.1657, -0.7739,  1.4156, -1.7199,  1.0934,  0.5445, -0.7570, -1.4942,\n",
       "         -1.0362,  0.5432],\n",
       "        [-0.2782, -1.2190,  1.0431,  0.0808, -0.1665,  0.7406, -1.2880,  0.7423,\n",
       "          0.8216,  1.6441],\n",
       "        [ 0.3207,  0.8375,  0.5336,  1.1942,  0.5790,  0.2433, -1.0244, -0.9470,\n",
       "         -0.0452, -0.8878],\n",
       "        [ 0.8417,  0.2664,  0.5194,  0.7736, -1.3530, -1.0820,  0.6434, -0.9518,\n",
       "         -0.4851, -0.5505],\n",
       "        [ 0.1473,  2.3938, -0.8641,  0.2462,  0.0651,  0.2549, -0.9982, -0.8814,\n",
       "         -0.0098,  1.5151],\n",
       "        [ 0.5930, -0.8168, -0.8172, -0.1530,  0.0569,  0.6663, -0.3458, -1.2687,\n",
       "         -0.3133, -0.4569],\n",
       "        [-0.9944, -0.7339, -0.1201, -0.6393,  0.7754, -1.3064, -0.4372, -0.0418,\n",
       "         -0.4973, -0.1493],\n",
       "        [-0.4678, -0.9364, -0.4461,  0.3912, -1.4863,  1.3704, -1.1609, -0.8786,\n",
       "         -0.6707, -0.8626],\n",
       "        [ 0.3724,  0.5360, -0.4087, -0.0558, -0.0507,  0.4615, -0.5196, -1.8035,\n",
       "         -0.0698,  0.8305],\n",
       "        [ 2.4892, -0.2987,  1.9883, -0.2630,  0.7044, -0.3708, -1.8317, -0.7665,\n",
       "         -0.7574, -1.4376],\n",
       "        [-0.0712,  1.1290, -0.3818,  1.1290, -2.0891, -0.8861,  0.3131,  1.5800,\n",
       "          0.6626, -1.6276],\n",
       "        [ 1.4915,  0.4752, -0.3868, -0.5807,  1.0075, -0.2444,  0.7096,  0.4142,\n",
       "          0.5415,  1.3997],\n",
       "        [ 0.0081,  1.9106,  0.0344,  0.5899,  0.1351,  0.0696, -1.5236, -0.1285,\n",
       "         -0.7277,  0.4702],\n",
       "        [-1.2165,  0.2231, -0.1695, -0.6135,  2.1242,  0.4438, -0.4490, -1.8190,\n",
       "          0.7870, -0.1732],\n",
       "        [-0.1803, -1.0523, -0.1428,  1.4736, -0.9674, -2.0183, -0.2337, -0.3959,\n",
       "         -1.1572,  0.6013],\n",
       "        [ 0.3881, -0.1020,  1.2568, -1.2618, -1.6121, -2.5220,  0.6814, -0.4900,\n",
       "         -0.5767,  0.1199],\n",
       "        [ 0.0362, -0.6409,  0.2485, -0.7908, -0.4876, -0.2191,  0.9571,  0.2600,\n",
       "         -0.8580, -0.6262],\n",
       "        [ 0.6827, -0.4280,  0.0962, -1.2233, -1.1323,  0.6629, -0.8648,  0.0961,\n",
       "          1.4262,  1.8084],\n",
       "        [ 0.4237,  0.6470,  0.2201,  1.5339,  0.2819, -0.4238, -0.5199,  0.6771,\n",
       "          0.6909, -0.7824],\n",
       "        [-0.7740, -0.1863, -1.9577, -0.9537, -0.5329,  1.7583,  0.9565,  0.8781,\n",
       "          2.4277, -1.3034],\n",
       "        [-1.2643,  1.0173,  0.9665,  1.0494,  0.0647, -0.8131,  0.0911, -0.6157,\n",
       "          0.1404,  0.4604],\n",
       "        [-0.8960,  0.0768, -0.9132,  1.9220,  1.1196, -1.1806, -0.5477, -1.1084,\n",
       "          0.4830,  0.8057],\n",
       "        [-1.4615, -1.1441, -0.5810,  1.2424,  1.5238,  0.7062, -0.8846,  0.2243,\n",
       "          0.0253,  0.7271],\n",
       "        [ 0.4301, -0.1797,  1.1007, -1.4457, -1.0218,  0.1792, -0.5069, -0.1566,\n",
       "         -0.5936,  1.9436],\n",
       "        [-1.3451, -1.4461,  0.6019, -0.2457,  0.3002, -0.2187,  0.1682,  0.6909,\n",
       "         -1.4757,  0.5196],\n",
       "        [ 0.4629, -0.3864,  1.4337,  1.4364,  0.3738, -1.1027,  1.2089,  0.4500,\n",
       "          0.2582, -1.0971],\n",
       "        [-0.0295,  1.0087, -0.9352, -0.7352,  0.4715,  0.5272, -0.4059,  0.1124,\n",
       "          0.2835,  0.8462],\n",
       "        [-0.2852,  0.8242,  0.8234,  0.1393,  0.3209, -0.3744, -1.7069, -0.8258,\n",
       "         -0.9010,  0.0488]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
