{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# working model for tensorfusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['file_names', 'text_embeddings', 'wav_embeddings', 'Emotion', 'Arousal', 'Valence'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data load\n",
    "with open('./data/lou_dataset_1_3.pkl', 'rb') as f:\n",
    "    raw_dataset = pickle.load(f)\n",
    "raw_dataset['Session01'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'neutral': 0,\n",
       "  'happy': 1,\n",
       "  'happy;neutral': 2,\n",
       "  'surprise;neutral': 3,\n",
       "  'happy;surprise': 4,\n",
       "  'angry;neutral': 5},\n",
       " {0: 'neutral',\n",
       "  1: 'happy',\n",
       "  2: 'happy;neutral',\n",
       "  3: 'surprise;neutral',\n",
       "  4: 'happy;surprise',\n",
       "  5: 'angry;neutral'})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoding Emotion\n",
    "encode_dict = {b:i for i, b in enumerate(raw_dataset['Session01']['Emotion'].unique())}\n",
    "decode_dict = {i:b for i, b in enumerate(raw_dataset['Session01']['Emotion'].unique())}\n",
    "encode_dict, decode_dict    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset['Session01']['Emotion'] = raw_dataset['Session01']['Emotion'].map(encode_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch dataset 만들기\n",
    "- 참고: https://tutorials.pytorch.kr/beginner/basics/data_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EtriDataset(Dataset):\n",
    "    def __init__(self, file_names, text_embeddings, wav_embeddings, Emotion, Arousal, Valence):\n",
    "        self.file_names = file_names\n",
    "        self.text_embeddings = text_embeddings\n",
    "        self.wav_embeddings = wav_embeddings\n",
    "        self.label_emotion = Emotion\n",
    "        self.label_arousal = Arousal\n",
    "        self.label_valence = Valence\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text_embeddings = self.text_embeddings[idx]\n",
    "        wav_embeddings = self.wav_embeddings[idx]\n",
    "        label_emotion = self.label_emotion[idx]\n",
    "        label_arousal = self.label_arousal[idx]\n",
    "        label_valence = self.label_valence[idx]\n",
    "        return text_embeddings, wav_embeddings, label_emotion, label_arousal, label_valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data load 및 나누기: https://076923.github.io/posts/Python-pytorch-11/\n",
    "\n",
    "dataset = EtriDataset(raw_dataset['Session01']['file_names'],\n",
    "                      raw_dataset['Session01']['text_embeddings'],\n",
    "                      raw_dataset['Session01']['wav_embeddings'],\n",
    "                      raw_dataset['Session01']['Emotion'],\n",
    "                      raw_dataset['Session01']['Arousal'],\n",
    "                      raw_dataset['Session01']['Valence'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217 48 46\n",
      "Training Data Size : 217\n",
      "Validation Data Size : 46\n",
      "Testing Data Size : 48\n"
     ]
    }
   ],
   "source": [
    "dataset_size = len(dataset)\n",
    "train_size = int(dataset_size * 0.7)\n",
    "validation_size = int(dataset_size * 0.15)\n",
    "test_size = dataset_size - train_size - validation_size\n",
    "\n",
    "train_dataset, validation_dataset, test_dataset = random_split(dataset, [train_size, validation_size, test_size])\n",
    "\n",
    "print(train_size, test_size, validation_size)\n",
    "print(f\"Training Data Size : {len(train_dataset)}\")\n",
    "print(f\"Validation Data Size : {len(validation_dataset)}\")\n",
    "print(f\"Testing Data Size : {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 49, 768])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data size\n",
    "raw_dataset['Session01']['wav_embeddings'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, drop_last=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=4, shuffle=True, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=True, drop_last=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NetWork 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNetwork_pre(nn.Module):\n",
    "    def __init__(self, input_length, input_width):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_length*input_width, 768)\n",
    "        self.gelu1 = nn.GELU()\n",
    "        self.bn1 = nn.BatchNorm1d(768)\n",
    "        self.fc2 = nn.Linear(768, 512)\n",
    "        self.gelu2 = nn.GELU()\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.fc3 = nn.Linear(512, 32)\n",
    "        self.gelu3 = nn.GELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.gelu1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.gelu2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.fc3(x)\n",
    "        output = self.gelu3(x)\n",
    "        return output\n",
    "\n",
    "class MLPNetwork_final(nn.Module):\n",
    "    def __init__(self, input_length, input_width):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_length*input_width, 256)\n",
    "        self.gelu1 = nn.GELU()\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.gelu2 = nn.GELU()\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.fc3 = nn.Linear(64, 6)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.gelu1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.gelu2(x)\n",
    "        x = self.bn2(x)\n",
    "        output = self.fc3(x)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFusionMixer(\n",
      "  (ModelA): MLPNetwork_pre(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc1): Linear(in_features=62976, out_features=768, bias=True)\n",
      "    (gelu1): GELU(approximate='none')\n",
      "    (bn1): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (fc2): Linear(in_features=768, out_features=512, bias=True)\n",
      "    (gelu2): GELU(approximate='none')\n",
      "    (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (fc3): Linear(in_features=512, out_features=32, bias=True)\n",
      "    (gelu3): GELU(approximate='none')\n",
      "  )\n",
      "  (ModelB): MLPNetwork_pre(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc1): Linear(in_features=37632, out_features=768, bias=True)\n",
      "    (gelu1): GELU(approximate='none')\n",
      "    (bn1): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (fc2): Linear(in_features=768, out_features=512, bias=True)\n",
      "    (gelu2): GELU(approximate='none')\n",
      "    (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (fc3): Linear(in_features=512, out_features=32, bias=True)\n",
      "    (gelu3): GELU(approximate='none')\n",
      "  )\n",
      "  (Model_mlp_final): MLPNetwork_final(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc1): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (gelu1): GELU(approximate='none')\n",
      "    (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (gelu2): GELU(approximate='none')\n",
      "    (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (fc3): Linear(in_features=64, out_features=6, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class TensorFusionMixer(nn.Module):\n",
    "    def __init__(self, ModelA, ModelB):\n",
    "        super().__init__()\n",
    "        self.ModelA = ModelA\n",
    "        self.ModelB = ModelB\n",
    "        self.Model_mlp_final = MLPNetwork_final(32,32).to(device)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def tensor_fusion(self, batch_arr1, batch_arr2):\n",
    "        fusion_matrix_lst = []\n",
    "        for i, (arr1, arr2) in enumerate(zip(batch_arr1, batch_arr2)):\n",
    "            outer_matrix = torch.outer(arr1, arr2)\n",
    "            l, w = outer_matrix.shape\n",
    "            outer_matrix = outer_matrix.view(1, l, w)\n",
    "            fusion_matrix_lst.append(outer_matrix)\n",
    "        fusion_matrix = torch.concat(fusion_matrix_lst)\n",
    "        # print(fusion_matrix.shape)\n",
    "        return fusion_matrix\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.ModelA(x1)\n",
    "        x2 = self.ModelB(x2)\n",
    "        fusion_matrix = self.tensor_fusion(x1, x2) \n",
    "        x = self.Model_mlp_final(fusion_matrix)\n",
    "        output = self.softmax(x)\n",
    "        return output     \n",
    "\n",
    "txt_input_length, txt_input_width = raw_dataset['Session01']['text_embeddings'][0].shape\n",
    "_, wav_input_length, wav_input_width = raw_dataset['Session01']['wav_embeddings'][0].shape\n",
    "\n",
    "# tf_mixer에 들어갈 wav mlp, txt mlp 선언\n",
    "model_mlp_txt = MLPNetwork_pre(txt_input_length,txt_input_width).to(device)\n",
    "model_mlp_wav = MLPNetwork_pre(wav_input_length,wav_input_width).to(device)\n",
    "\n",
    "# 최종 모델 선언\n",
    "model_tf_mixer = TensorFusionMixer(ModelA = model_mlp_txt, ModelB = model_mlp_wav).to(device)\n",
    "\n",
    "print(model_tf_mixer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습을 위한 train, test method 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X_txt, X_wav, y, _, _) in enumerate(dataloader): # data 순서: file_names, text_embeddings, wav_embeddings, label_emotion, label_arousal, label_valence\n",
    "        \n",
    "        # 예측 오류 계산\n",
    "        X_txt, X_wav, y = X_txt.to(device), X_wav.to(device),y.type(torch.LongTensor).to(device)\n",
    "        pred = model(X_txt, X_wav)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # 역전파\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X_txt)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn, mode = 'test'):\n",
    "       \n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch, (X_txt, X_wav, y, _, _) in enumerate(dataloader): # text_embeddings, wav_embeddings, label_emotion, label_arousal, label_valence\n",
    "            # 예측 오류 계산\n",
    "            X_txt, X_wav, y = X_txt.to(device), X_wav.to(device),y.type(torch.LongTensor).to(device)\n",
    "            pred = model(X_txt, X_wav)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    if mode == 'test':\n",
    "        print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    elif mode == 'val':\n",
    "        print(f\"Validation Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.805170  [    0/  217]\n",
      "Validation Error: \n",
      " Accuracy: 0.0%, Avg loss: 1.784804 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.784523  [    0/  217]\n",
      "Validation Error: \n",
      " Accuracy: 0.0%, Avg loss: 1.784104 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.785294  [    0/  217]\n",
      "Validation Error: \n",
      " Accuracy: 0.0%, Avg loss: 1.783390 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.780635  [    0/  217]\n",
      "Validation Error: \n",
      " Accuracy: 0.0%, Avg loss: 1.782096 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.788549  [    0/  217]\n",
      "Validation Error: \n",
      " Accuracy: 0.0%, Avg loss: 1.781471 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.781594  [    0/  217]\n",
      "Validation Error: \n",
      " Accuracy: 0.0%, Avg loss: 1.781223 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.779850  [    0/  217]\n",
      "Validation Error: \n",
      " Accuracy: 0.0%, Avg loss: 1.779849 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.780122  [    0/  217]\n",
      "Validation Error: \n",
      " Accuracy: 0.0%, Avg loss: 1.779759 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.779699  [    0/  217]\n",
      "Validation Error: \n",
      " Accuracy: 0.0%, Avg loss: 1.779028 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.780524  [    0/  217]\n",
      "Validation Error: \n",
      " Accuracy: 0.0%, Avg loss: 1.778284 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.779449  [    0/  217]\n",
      "Validation Error: \n",
      " Accuracy: 0.0%, Avg loss: 1.777541 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 1.777096  [    0/  217]\n",
      "Validation Error: \n",
      " Accuracy: 0.0%, Avg loss: 1.776786 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 1.776332  [    0/  217]\n",
      "Validation Error: \n",
      " Accuracy: 0.0%, Avg loss: 1.775377 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 1.775527  [    0/  217]\n",
      "Validation Error: \n",
      " Accuracy: 0.0%, Avg loss: 1.775267 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 1.779401  [    0/  217]\n",
      "Validation Error: \n",
      " Accuracy: 0.0%, Avg loss: 1.774499 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 1.775914  [    0/  217]\n",
      "Validation Error: \n",
      " Accuracy: 0.0%, Avg loss: 1.773730 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 1.771288  [    0/  217]\n",
      "Validation Error: \n",
      " Accuracy: 0.0%, Avg loss: 1.772949 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 1.772608  [    0/  217]\n",
      "Validation Error: \n",
      " Accuracy: 0.0%, Avg loss: 1.772166 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 1.773711  [    0/  217]\n",
      "Validation Error: \n",
      " Accuracy: 0.0%, Avg loss: 1.771387 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 1.768681  [    0/  217]\n",
      "Validation Error: \n",
      " Accuracy: 0.0%, Avg loss: 1.770591 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 1.770448  [    0/  217]\n",
      "Validation Error: \n",
      " Accuracy: 0.0%, Avg loss: 1.769789 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 1.768716  [    0/  217]\n",
      "Validation Error: \n",
      " Accuracy: 0.0%, Avg loss: 1.768988 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 1.766012  [    0/  217]\n",
      "Validation Error: \n",
      " Accuracy: 0.0%, Avg loss: 1.767300 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 1.765188  [    0/  217]\n",
      "Validation Error: \n",
      " Accuracy: 0.0%, Avg loss: 1.767357 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 1.766768  [    0/  217]\n",
      "Validation Error: \n",
      " Accuracy: 0.0%, Avg loss: 1.766535 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 1.768438  [    0/  217]\n",
      "Validation Error: \n",
      " Accuracy: 0.0%, Avg loss: 1.765713 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 1.762861  [    0/  217]\n",
      "Validation Error: \n",
      " Accuracy: 52.2%, Avg loss: 1.764880 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 1.764237  [    0/  217]\n",
      "Validation Error: \n",
      " Accuracy: 87.0%, Avg loss: 1.762024 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 1.763392  [    0/  217]\n",
      "Validation Error: \n",
      " Accuracy: 82.6%, Avg loss: 1.763188 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 1.765582  [    0/  217]\n",
      "Validation Error: \n",
      " Accuracy: 82.6%, Avg loss: 1.762336 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Set the Training Parameters\n",
    "lr = 1e-3\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.SGD(model_tf_mixer.parameters(), lr=lr)\n",
    "\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model_tf_mixer, loss_fn, optimizer)\n",
    "    test(validation_dataloader, model_tf_mixer, loss_fn, mode = 'val')\n",
    "print(\"Done!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 1.764071 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(test_dataloader, model_tf_mixer, loss_fn, mode = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.4036, -0.8254,  0.3374,  ..., -0.4919,  0.1812,  0.9015],\n",
      "         [-1.5124, -1.0119,  1.0906,  ..., -0.4881,  1.2984,  0.6860],\n",
      "         [-2.3484, -1.5205,  0.8371,  ...,  0.4671,  0.7323,  1.5519],\n",
      "         ...,\n",
      "         [-1.3646, -0.5572,  0.7691,  ..., -0.4963,  0.5333,  0.3470],\n",
      "         [-1.3499, -0.6444,  0.7400,  ..., -0.7245,  0.5905,  0.6070],\n",
      "         [-1.2281, -0.1519,  0.4446,  ..., -0.4954,  0.4255,  0.4170]],\n",
      "\n",
      "        [[ 0.0683, -1.3784,  2.1355,  ..., -0.5110,  0.3518,  0.3829],\n",
      "         [-0.1755, -0.6976,  0.9760,  ..., -0.2128,  0.3653,  0.1003],\n",
      "         [-0.7046, -0.6705,  1.0717,  ..., -0.2769,  0.1023,  0.6022],\n",
      "         ...,\n",
      "         [ 0.4858, -0.2909,  1.9416,  ..., -0.2216,  1.1318,  0.3733],\n",
      "         [ 0.2900,  0.4255,  2.0321,  ..., -0.2868,  1.1614,  0.6492],\n",
      "         [ 0.3786, -0.3927,  1.8511,  ..., -0.0986,  1.4052,  0.5346]],\n",
      "\n",
      "        [[-0.3103, -0.6272,  0.8433,  ..., -1.4785, -0.8344,  0.9614],\n",
      "         [ 0.8770, -0.1079,  1.3329,  ..., -0.9728, -0.1311,  0.9477],\n",
      "         [-0.6596,  0.6025,  0.6909,  ..., -0.6994, -0.6783,  0.2883],\n",
      "         ...,\n",
      "         [-0.1531, -0.4445,  0.8832,  ..., -1.2681, -0.4320,  0.6223],\n",
      "         [ 0.0439, -0.3491,  0.9932,  ..., -1.3271, -0.6304,  0.8233],\n",
      "         [-0.3046, -0.3099,  0.5561,  ..., -1.1846, -0.8495,  0.9153]],\n",
      "\n",
      "        [[-1.3579, -1.1992,  0.2801,  ..., -0.9527, -1.1639,  0.1072],\n",
      "         [-0.7321, -0.6102,  0.3306,  ..., -0.4549, -0.1371, -0.3711],\n",
      "         [ 0.0809, -0.3876,  0.3391,  ..., -0.8311, -0.1830, -0.5198],\n",
      "         ...,\n",
      "         [-0.8445, -0.8329,  0.2488,  ..., -0.2926, -0.6861, -0.5564],\n",
      "         [-0.8180, -0.6914,  0.1875,  ..., -0.6384, -0.5889, -0.3547],\n",
      "         [-0.7981, -0.6765,  0.1707,  ..., -0.6056, -0.5819, -0.3552]]],\n",
      "       grad_fn=<StackBackward0>) tensor([[[[-8.7779e-01,  1.3302e-02, -1.5510e-01,  ..., -9.5597e-02,\n",
      "            1.6019e-01,  2.5579e-01],\n",
      "          [-8.7779e-01,  1.3302e-02, -1.5510e-01,  ..., -9.5597e-02,\n",
      "            1.6019e-01,  2.5579e-01],\n",
      "          [-8.7779e-01,  1.3302e-02, -1.5510e-01,  ..., -9.5597e-02,\n",
      "            1.6019e-01,  2.5579e-01],\n",
      "          ...,\n",
      "          [-1.0063e+00, -6.7105e-02, -1.3793e-04,  ..., -3.4229e-02,\n",
      "           -2.0048e-01,  1.2336e-01],\n",
      "          [-1.0030e+00, -6.1798e-02,  5.5315e-04,  ..., -3.5489e-02,\n",
      "           -1.9778e-01,  1.2566e-01],\n",
      "          [-1.0109e+00, -6.3790e-02,  2.6582e-03,  ..., -3.4538e-02,\n",
      "           -1.8806e-01,  1.2790e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.0934e+00,  6.6412e-02, -2.6888e-01,  ...,  3.5760e-03,\n",
      "            4.9129e-02,  1.4878e-01],\n",
      "          [-1.0934e+00,  6.6413e-02, -2.6888e-01,  ...,  3.5754e-03,\n",
      "            4.9130e-02,  1.4879e-01],\n",
      "          [-1.0934e+00,  6.6414e-02, -2.6888e-01,  ...,  3.5756e-03,\n",
      "            4.9129e-02,  1.4879e-01],\n",
      "          ...,\n",
      "          [-1.2464e+00,  3.1005e-02,  2.1497e-02,  ...,  1.2451e-02,\n",
      "           -8.7042e-02,  1.7165e-01],\n",
      "          [-1.3014e+00,  5.2071e-02,  1.1016e-02,  ...,  1.3455e-02,\n",
      "           -1.0638e-01,  1.8087e-01],\n",
      "          [-1.3406e+00,  1.1455e-01, -3.9351e-02,  ...,  1.1105e-01,\n",
      "           -3.4758e-02,  2.3560e-01]]],\n",
      "\n",
      "\n",
      "        [[[-9.4248e-01, -7.0639e-02, -1.1199e-01,  ...,  1.1003e-01,\n",
      "            1.3788e-01,  9.2041e-02],\n",
      "          [-9.4248e-01, -7.0638e-02, -1.1199e-01,  ...,  1.1003e-01,\n",
      "            1.3788e-01,  9.2040e-02],\n",
      "          [-9.4248e-01, -7.0639e-02, -1.1199e-01,  ...,  1.1003e-01,\n",
      "            1.3788e-01,  9.2041e-02],\n",
      "          ...,\n",
      "          [-1.1060e+00,  1.3887e-01,  1.7783e-02,  ..., -4.0635e-02,\n",
      "           -1.3903e-01,  8.9521e-02],\n",
      "          [-1.0990e+00,  1.4164e-01,  4.4568e-03,  ..., -3.9847e-02,\n",
      "           -1.3169e-01,  7.5860e-02],\n",
      "          [-1.1677e+00,  1.8927e-01, -3.1762e-02,  ..., -3.2194e-02,\n",
      "           -1.1438e-01,  1.1343e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.2379e+00,  1.1629e-01, -9.5201e-02,  ...,  6.6466e-02,\n",
      "           -7.3224e-02,  1.5859e-01],\n",
      "          [-1.2379e+00,  1.1629e-01, -9.5200e-02,  ...,  6.6466e-02,\n",
      "           -7.3224e-02,  1.5859e-01],\n",
      "          [-1.2379e+00,  1.1629e-01, -9.5200e-02,  ...,  6.6466e-02,\n",
      "           -7.3223e-02,  1.5859e-01],\n",
      "          ...,\n",
      "          [-1.2084e+00, -5.1152e-03,  6.4890e-02,  ..., -1.3554e-01,\n",
      "           -2.0753e-01,  1.5521e-02],\n",
      "          [-1.2147e+00, -2.0045e-02,  6.0296e-02,  ..., -1.3876e-01,\n",
      "           -2.1976e-01,  8.8187e-03],\n",
      "          [-1.2149e+00, -3.3070e-02,  7.7625e-02,  ..., -1.3920e-01,\n",
      "           -2.4826e-01,  1.1955e-02]]]]) tensor([0, 0, 0, 0]) tensor([3.2000, 3.3000, 2.9000, 2.6000], dtype=torch.float64) tensor([2.4000, 2.7000, 2.9000, 3.0000], dtype=torch.float64)\n",
      "tensor([[[ 0.9276, -0.5664,  0.0711,  ...,  0.8116, -0.0209,  1.6671],\n",
      "         [ 1.2042, -0.4295, -0.0245,  ...,  0.4921,  0.0766,  0.3588],\n",
      "         [ 0.6714, -1.7973,  0.0826,  ...,  0.6640,  0.2447,  1.0171],\n",
      "         ...,\n",
      "         [ 0.7897, -0.1937, -0.0792,  ...,  0.5242,  0.7437,  1.1811],\n",
      "         [ 0.8657, -0.1787, -0.1961,  ...,  0.7888,  0.5981,  1.3246],\n",
      "         [ 0.6816, -0.9449,  0.2135,  ...,  0.3748,  0.4481,  0.7523]],\n",
      "\n",
      "        [[-0.8192, -0.5192,  0.6772,  ..., -0.9680,  0.2846,  1.0216],\n",
      "         [-0.8512, -0.4042,  0.7272,  ..., -1.0851, -0.5025,  0.3903],\n",
      "         [-1.6228, -0.3467,  1.5863,  ..., -0.3701,  0.3190,  0.8019],\n",
      "         ...,\n",
      "         [ 0.0358, -0.6472,  0.5381,  ..., -0.9594,  1.3403,  0.7104],\n",
      "         [-0.5612, -0.7199,  1.0616,  ..., -0.8596,  0.6389,  0.5880],\n",
      "         [-0.5722, -0.6822,  1.0506,  ..., -0.8399,  0.6573,  0.5701]],\n",
      "\n",
      "        [[ 0.7631, -1.7098,  0.8843,  ..., -0.8626,  1.2173,  0.2789],\n",
      "         [-0.2809, -0.7024,  0.0812,  ..., -0.4565,  1.9619,  0.4007],\n",
      "         [ 0.8335, -1.1770,  0.7012,  ..., -0.8390,  0.2444,  0.4770],\n",
      "         ...,\n",
      "         [ 0.8859, -1.4438,  0.7948,  ..., -0.7881,  1.0254,  0.0685],\n",
      "         [ 1.1333, -1.5978,  0.4747,  ..., -1.1287,  1.2189,  0.0109],\n",
      "         [ 1.1026, -1.6520,  0.3720,  ..., -1.1223,  0.9476, -0.1327]],\n",
      "\n",
      "        [[-0.4043, -1.1501,  0.3254,  ..., -0.1509, -0.1007,  0.7250],\n",
      "         [-1.4487, -0.9909,  0.5255,  ..., -0.3561,  0.6473,  0.4580],\n",
      "         [-0.4681,  0.3139,  0.3604,  ..., -0.1712,  0.0133,  0.3682],\n",
      "         ...,\n",
      "         [-0.0713, -2.0387,  0.2619,  ..., -0.5450,  0.3401,  0.2782],\n",
      "         [-0.3944, -1.5510,  0.3661,  ..., -0.1270, -0.0994,  0.3372],\n",
      "         [-0.4660, -1.3860,  0.3729,  ..., -0.0587, -0.1102,  0.3930]]],\n",
      "       grad_fn=<StackBackward0>) tensor([[[[-0.7041, -0.0846, -0.1444,  ...,  0.1355,  0.2257,  0.0827],\n",
      "          [-0.7041, -0.0846, -0.1444,  ...,  0.1355,  0.2257,  0.0827],\n",
      "          [-0.7041, -0.0846, -0.1444,  ...,  0.1355,  0.2257,  0.0827],\n",
      "          ...,\n",
      "          [-1.0590,  0.1758,  0.0481,  ..., -0.0707, -0.1787,  0.0754],\n",
      "          [-1.0575,  0.1745,  0.0494,  ..., -0.0703, -0.1786,  0.0750],\n",
      "          [-1.0570,  0.1759,  0.0491,  ..., -0.0705, -0.1762,  0.0751]]],\n",
      "\n",
      "\n",
      "        [[[-0.6363, -0.0621, -0.0562,  ...,  0.1645,  0.1488,  0.0590],\n",
      "          [-0.6363, -0.0621, -0.0562,  ...,  0.1645,  0.1488,  0.0590],\n",
      "          [-0.6363, -0.0621, -0.0562,  ...,  0.1645,  0.1488,  0.0590],\n",
      "          ...,\n",
      "          [-0.9920,  0.1409,  0.0267,  ..., -0.0613, -0.0972,  0.1257],\n",
      "          [-0.9585,  0.1190,  0.0240,  ..., -0.0635, -0.0993,  0.1077],\n",
      "          [-0.9534,  0.1134,  0.0204,  ..., -0.0633, -0.0953,  0.1015]]],\n",
      "\n",
      "\n",
      "        [[[-0.5694, -0.0774, -0.1107,  ...,  0.1701,  0.1445,  0.2163],\n",
      "          [-0.5694, -0.0774, -0.1107,  ...,  0.1701,  0.1445,  0.2163],\n",
      "          [-0.5694, -0.0774, -0.1107,  ...,  0.1701,  0.1445,  0.2163],\n",
      "          ...,\n",
      "          [-1.0328, -0.0276,  0.0240,  ..., -0.0370, -0.0945,  0.1781],\n",
      "          [-0.9707, -0.0089,  0.0189,  ..., -0.0403, -0.1218,  0.1383],\n",
      "          [-0.9558, -0.0017,  0.0160,  ..., -0.0402, -0.1204,  0.1289]]],\n",
      "\n",
      "\n",
      "        [[[-1.2852,  0.4983, -0.5124,  ..., -0.0815,  0.0176,  0.4459],\n",
      "          [-1.2852,  0.4983, -0.5124,  ..., -0.0815,  0.0176,  0.4459],\n",
      "          [-1.2852,  0.4983, -0.5124,  ..., -0.0815,  0.0176,  0.4459],\n",
      "          ...,\n",
      "          [-1.2921,  0.1473,  0.0884,  ..., -0.1152, -0.3318,  0.0721],\n",
      "          [-1.2929,  0.1460,  0.0885,  ..., -0.1147, -0.3315,  0.0720],\n",
      "          [-1.2968,  0.1465,  0.0877,  ..., -0.1135, -0.3327,  0.0745]]]]) tensor([0, 0, 0, 0]) tensor([3.0000, 2.9000, 3.5000, 2.8000], dtype=torch.float64) tensor([3.1000, 3.0000, 3.4000, 2.9000], dtype=torch.float64)\n",
      "tensor([[[ 0.9656, -0.9728,  1.4968,  ...,  0.2865,  0.5169,  0.0622],\n",
      "         [-0.3387, -1.1444,  1.3311,  ...,  0.8418,  0.8496,  1.1780],\n",
      "         [ 0.8569,  0.1668,  1.3433,  ...,  0.2247,  0.8470,  0.1955],\n",
      "         ...,\n",
      "         [ 0.9580, -0.7464,  1.7766,  ...,  0.1782,  0.6200, -0.0326],\n",
      "         [ 1.0281, -0.7398,  1.7303,  ...,  0.1839,  0.7049,  0.0333],\n",
      "         [ 0.7482, -0.3368,  1.8553,  ...,  0.0519,  0.8815, -0.0524]],\n",
      "\n",
      "        [[ 0.8506,  0.1758,  0.7906,  ...,  1.1225, -0.2691,  1.4486],\n",
      "         [ 0.8207,  0.4437,  0.7544,  ...,  0.4765,  1.0153,  1.6392],\n",
      "         [ 0.5743,  0.3716,  1.2173,  ...,  0.1047,  0.5905,  0.4929],\n",
      "         ...,\n",
      "         [ 1.0965,  0.0275,  1.1618,  ...,  0.6991,  0.2058,  0.7776],\n",
      "         [ 0.8766,  0.3017,  1.0534,  ...,  0.7750,  0.0394,  0.9266],\n",
      "         [ 1.0310,  0.2517,  0.5699,  ...,  0.9274, -0.1503,  1.1918]],\n",
      "\n",
      "        [[ 0.2764,  0.0210,  0.6151,  ..., -1.3155,  0.5228, -0.3075],\n",
      "         [ 0.2189, -0.5836,  1.0734,  ..., -0.3528,  1.3342, -0.5837],\n",
      "         [-0.8682, -1.3586,  0.2598,  ...,  0.1326,  0.3679,  1.1093],\n",
      "         ...,\n",
      "         [ 0.7799, -0.1270,  1.0303,  ..., -0.8368,  0.5074, -0.5549],\n",
      "         [ 0.9647, -0.8254,  1.1410,  ..., -0.7952,  0.8903, -0.8223],\n",
      "         [ 0.7512,  0.0540,  1.0589,  ..., -0.9194,  0.4931, -0.5380]],\n",
      "\n",
      "        [[-0.4392,  0.2658,  0.3925,  ...,  0.9807,  1.5450,  1.4830],\n",
      "         [-0.5285,  0.5159,  0.2705,  ...,  0.8495,  2.5343,  1.2020],\n",
      "         [-0.1090,  1.0650, -0.0227,  ...,  1.1643,  1.9918,  1.7296],\n",
      "         ...,\n",
      "         [-0.4756, -0.1917,  0.4931,  ...,  0.4284,  1.8216,  0.8075],\n",
      "         [-0.0467,  0.1436,  0.5951,  ...,  0.3614,  1.7975,  1.2442],\n",
      "         [-0.0540,  0.3014,  0.5351,  ...,  0.3082,  1.8561,  1.2360]]],\n",
      "       grad_fn=<StackBackward0>) tensor([[[[-1.0732,  0.1509, -0.2243,  ...,  0.0546,  0.1399,  0.1390],\n",
      "          [-1.0732,  0.1509, -0.2243,  ...,  0.0546,  0.1399,  0.1390],\n",
      "          [-1.0732,  0.1509, -0.2243,  ...,  0.0546,  0.1399,  0.1390],\n",
      "          ...,\n",
      "          [-1.1205,  0.0540,  0.0642,  ..., -0.0440, -0.1511,  0.0588],\n",
      "          [-1.0910,  0.0370,  0.0465,  ..., -0.0541, -0.1445,  0.0395],\n",
      "          [-1.0811,  0.0294,  0.0382,  ..., -0.0536, -0.1333,  0.0308]]],\n",
      "\n",
      "\n",
      "        [[[-0.7495, -0.1175, -0.1182,  ...,  0.1234,  0.2070,  0.2150],\n",
      "          [-0.7495, -0.1175, -0.1182,  ...,  0.1234,  0.2070,  0.2150],\n",
      "          [-0.7495, -0.1175, -0.1182,  ...,  0.1234,  0.2070,  0.2150],\n",
      "          ...,\n",
      "          [-1.0706, -0.0614,  0.1425,  ...,  0.0365, -0.1438,  0.1789],\n",
      "          [-1.0427, -0.0212,  0.1423,  ...,  0.0107, -0.1554,  0.1752],\n",
      "          [-1.0103,  0.0103,  0.1216,  ..., -0.0276, -0.1343,  0.1553]]],\n",
      "\n",
      "\n",
      "        [[[-0.4930, -0.1220, -0.0410,  ...,  0.2097,  0.2193,  0.1010],\n",
      "          [-0.4930, -0.1220, -0.0410,  ...,  0.2097,  0.2193,  0.1010],\n",
      "          [-0.4930, -0.1220, -0.0410,  ...,  0.2097,  0.2193,  0.1010],\n",
      "          ...,\n",
      "          [-0.8329,  0.0084,  0.0173,  ..., -0.0071, -0.0666,  0.0773],\n",
      "          [-0.8269,  0.0029,  0.0206,  ..., -0.0099, -0.0630,  0.0769],\n",
      "          [-0.8117, -0.0108,  0.0176,  ..., -0.0019, -0.0581,  0.0620]]],\n",
      "\n",
      "\n",
      "        [[[-0.9414,  0.2476, -0.2508,  ...,  0.0304,  0.0428,  0.1132],\n",
      "          [-0.9414,  0.2476, -0.2508,  ...,  0.0304,  0.0428,  0.1132],\n",
      "          [-0.9414,  0.2476, -0.2508,  ...,  0.0304,  0.0428,  0.1132],\n",
      "          ...,\n",
      "          [-1.0727, -0.0138,  0.0488,  ..., -0.0175, -0.1387,  0.0804],\n",
      "          [-1.0654, -0.0227,  0.0452,  ..., -0.0277, -0.1587,  0.0647],\n",
      "          [-1.0757, -0.0114,  0.0420,  ..., -0.0306, -0.1680,  0.0750]]]]) tensor([1, 0, 4, 0]) tensor([3.4000, 3.2000, 4.3000, 3.1000], dtype=torch.float64) tensor([3.5000, 3.6000, 4.2000, 3.3000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    " for batch, (a,b,c,d,e) in list(enumerate(test_dataloader))[:3]:\n",
    "     print(a,b,c,d,e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2029, 0.1564, 0.1491, 0.1530, 0.1398, 0.1988],\n",
      "        [0.2029, 0.1564, 0.1493, 0.1531, 0.1397, 0.1987],\n",
      "        [0.2031, 0.1564, 0.1492, 0.1530, 0.1398, 0.1986],\n",
      "        [0.2029, 0.1564, 0.1493, 0.1531, 0.1398, 0.1987]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n"
     ]
    }
   ],
   "source": [
    "probs = model_tf_mixer(a.to(device), b.to(device))\n",
    "print(probs)\n",
    "for i in torch.argmax(probs, dim=1):\n",
    "    print(decode_dict[int(i)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bdc1fd12ca460d5768d71e9df3d9063ef832ce64a62e55a1a523c8c99752868e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
